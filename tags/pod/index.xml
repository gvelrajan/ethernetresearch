<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Pod on Ethernet Research</title><link>https://gvelrajan.github.io/ethernetresearch/tags/pod/</link><description>Recent content in Pod on Ethernet Research</description><generator>Hugo -- gohugo.io</generator><copyright>Copyright &amp;copy; 2021 - Ethernet Research</copyright><lastBuildDate>Thu, 29 Oct 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://gvelrajan.github.io/ethernetresearch/tags/pod/index.xml" rel="self" type="application/rss+xml"/><item><title>Docker Error No Space Left On Device</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/docker-error-no-space-left-on-device/</link><pubDate>Fri, 25 Jun 2021 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/docker-error-no-space-left-on-device/</guid><description>&lt;p>Recently, one of my cloud native applications running as a Docker containers failed to function correctly all of a sudden. It turned out that the problem was due to lack of disk space. Docket daemon in specific is the one that complained about the lack of disk space in the host machine. Here is the Docker error that complained about no space left on device:&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">docker: Error response from daemon: no space left on device.&lt;/code>&lt;/pre>&lt;p>In this article I&amp;rsquo;ll discuss how I peformed RCA (Root Cause Analysis) for this problem and what solutions I used to completely address the issue once and for all.&lt;/p>&lt;h2 id="problem-scenario-in-detail">Problem scenario in detail&lt;/h2>&lt;p>My application has several microservices, each running as a Docker container in a VM(Host). The front end microservice was the one that failed to function correctly. Upon investigation I found out that the API requests made by the front end microservice was responded by the backend&amp;rsquo;s API gateway with empty responses.&lt;/p>&lt;p>Upon further investigating the backend microservice, it appeared that all SQL queries to the postgres DB was failing.&lt;/p>&lt;p>I tried restarting the postgres DB docker container but the problem still persisted with a new error.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">docker: Error response from daemon: mkdir /var/lib/docker/overlay2/5051a8704320241cb6e3b92d5911b0769f2c93fbe02a99d33558c95861a64784-init: no space left on device.&lt;/code>&lt;/pre>&lt;p>I tried the &lt;code>docker system prune&lt;/code> command to delete any unwanted images, stale containers and stale networks that are currently not used in the production system. This will free up some disk space for the running docker containers to use. But this solution solved my problem for a day or two and the problem re-appeared.&lt;/p>&lt;p>So this time, I decided I&amp;rsquo;ll do a complete deep dive to understand the real root cause of the problem - no space left on disk - and resolve it.&lt;/p>&lt;h2 id="root-cause-analysis">Root Cause Analysis&lt;/h2>&lt;p>I initially looked at the logs from each of the microservices chain to understand where the source of the problem is. In my case, the backend microservice failed to connect with the DB microservice to perform SQL queries. This was evident from the backend microservice logs.&lt;/p>&lt;p>Now I know that the docker daemon is complaining about creating a new file or write to an already existing file in its directory. Usually docker filesystem is under /var/lib/docker in Linux.&lt;/p>&lt;h3 id="check-where-the-varlibdocker-is-mounted">Check where the /var/lib/docker is mounted&lt;/h3>&lt;p>The disk free (df) utility command comes in handy to find out the disk where the &lt;code>/var/lib/docker&lt;/code>folder is mounted on; it tells which disk is full and by how much.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$ df -h /var/lib/dockerFilesystem Size Used Avail Use% Mounted on/dev/root 49G 42G 6.4G 87% /&lt;/code>&lt;/pre>&lt;p>It says, &lt;code>/var/lib/docker&lt;/code> is mounted on &lt;code>/dev/root&lt;/code> filesystem. Moreover, it also says that &lt;code>/dev/root&lt;/code> filesystem is almost 87% full. This is a clear indication that the &lt;code>/dev/root&lt;/code> could have went out of space in the recent past and that could have prevented the docker daemon from writing any new data to a file on disk or create a new file in disk.&lt;/p>&lt;h3 id="check-the-free-inodes-available">Check the free inodes available&lt;/h3>&lt;p>Check if the docker error is occurring due to lack of inodes availability.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$ df -hi /var/lib/dockerFilesystem Inodes IUsed IFree IUse% Mounted on/dev/root 6.2M 226K 6.0M 4% /&lt;/code>&lt;/pre>&lt;p>It says, &lt;code>/dev/root&lt;/code> filesystem uses only 4% or 226K inodes out of the available inodes 100% or 6.2M inodes. So clearly, inode availability is not the cause of the problem here. Free inodes are available in plenty for the docker daemon to create any new file under &lt;code>/var/lib/docker&lt;/code>&lt;/p>&lt;h3 id="check-the-disk-usage-of-the-docker-directory">Check the disk usage of the docker directory&lt;/h3>&lt;p>The &lt;code>df&lt;/code> (disk free) command used above tells us about the disk free and disk usage information at the file system level only. It doesn&amp;rsquo;t tell which directory (or application) using the &lt;code>/dev/root&lt;/code> disk filesystem is consuming the most data on the disk. For that we need to use the &lt;code>du&lt;/code> (disk usage) command.&lt;/p>&lt;p>In most cases, the application complaining the error [&amp;ldquo;Error: no space left on device&amp;rdquo;], is the one that consumes a lot of space on disk. So first let&amp;rsquo;s check the disk usage of &lt;code>/var/lib/docker&lt;/code> directory in the &lt;code>/dev/root&lt;/code> filesystem.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$ sudo du -h /var/lib/docker4.0K/var/lib/docker/tmp4.0K/var/lib/docker/overlay2/472e0a836ebf11b07eb16761815326dff8861abfaf3d99de31c1efe2c7414bf3/work4.0K/var/lib/docker/overlay2/472e0a836ebf11b07eb16761815326dff8861abfaf3d99de31c1efe2c7414bf3/diff/var/local/webserver8.0K/var/lib/docker/overlay2/472e0a836ebf11b07eb16761815326dff8861abfaf3d99de31c1efe2c7414bf3/diff/var/local12K/var/lib/docker/overlay2/472e0a836ebf11b07eb16761815326dff8861abfaf3d99de31c1efe2c7414bf3/diff/var16K/var/lib/docker/overlay2/472e0a836ebf11b07eb16761815326dff8861abfaf3d99de31c1efe2c7414bf3/diff32K/var/lib/docker/overlay2/472e0a836ebf11b07eb16761815326dff8861abfaf3d99de31c1efe2c7414bf34.0K/var/lib/docker/overlay2/f0475e9472279658f679d8980b1776f2c00aedc6ccfd6e31a02dcbebe3d3a2df/work12K/var/lib/docker/overlay2/f0475e9472279658f679d8980b1776f2c00aedc6ccfd6e31a02dcbebe3d3a2df/diff/var/lib/dpkg16K/var/lib/docker/overlay2/f0475e9472279658f679d8980b1776f2c00aedc6ccfd6e31a02dcbebe3d3a2df/diff/var/lib20K/var/lib/docker/overlay2/f0475e9472279658f679d8980b1776f2c00aedc6ccfd6e31a02dcbebe3d3a2df/diff/var4.0K/var/lib/docker/overlay2/f0475e9472279658f679d8980b1776f2c00aedc6ccfd6e31a02dcbebe3d3a2df/diff/usr/share/postgresql/1264K/var/lib/docker/overlay2/f0475e9472279658f679d8980b1776f2c00aedc6ccfd6e31a02dcbebe3d3a2df/diff/usr/share/postgresql68K/var/lib/docker/overlay2/f0475e9472279658f679d8980b1776f2c00aedc6ccfd6e31a02dcbebe3d3a2df/diff/usr/share72K/var/lib/docker/overlay2/f0475e9472279658f679d8980b1776f2c00aedc6ccfd6e31a02dcbebe3d3a2df/diff/usr.........4.0K/var/lib/docker/containers/63955d57625448a53e4a391dcf6f97952e30f44a13ee65ac24d90e747a1ffca1/checkpoints4.0K/var/lib/docker/containers/63955d57625448a53e4a391dcf6f97952e30f44a13ee65ac24d90e747a1ffca1/mounts36K/var/lib/docker/containers/63955d57625448a53e4a391dcf6f97952e30f44a13ee65ac24d90e747a1ffca14.0K/var/lib/docker/containers/23a77fd2325f88deec8cb45480326a1044e914b08b85553ae9d0d29b19e81698/checkpoints4.0K/var/lib/docker/containers/23a77fd2325f88deec8cb45480326a1044e914b08b85553ae9d0d29b19e81698/mounts7.0M/var/lib/docker/containers/23a77fd2325f88deec8cb45480326a1044e914b08b85553ae9d0d29b19e816984.0K/var/lib/docker/containers/14f5a4c98e5653a15e334880b9a58504adda416b06f359655bf251073b160ab1/checkpoints4.0K/var/lib/docker/containers/14f5a4c98e5653a15e334880b9a58504adda416b06f359655bf251073b160ab1/mounts35G/var/lib/docker/containers/14f5a4c98e5653a15e334880b9a58504adda416b06f359655bf251073b160ab14.0K/var/lib/docker/containers/3d03387e4de11ca77ce0dc4399f8bb58f62cd70faabbe36b7230355dcbc6b39b/checkpoints4.0K/var/lib/docker/containers/3d03387e4de11ca77ce0dc4399f8bb58f62cd70faabbe36b7230355dcbc6b39b/mounts42M/var/lib/docker/containers/3d03387e4de11ca77ce0dc4399f8bb58f62cd70faabbe36b7230355dcbc6b39b35G/var/lib/docker/containers4.0K/var/lib/docker/swarm4.0K/var/lib/docker/buildkit/executor4.0K/var/lib/docker/buildkit/content/ingest8.0K/var/lib/docker/buildkit/content92K/var/lib/docker/buildkit16K/var/lib/docker/plugins56K/var/lib/docker/network/files60K/var/lib/docker/network38G/var/lib/docker&lt;/code>&lt;/pre>&lt;p>If you look at the output above, it clearly shows that &lt;code>/var/lib/docker&lt;/code> directory alone consumes 38G out of the available 50GB on the &lt;code>/dev/root&lt;/code> file system. Also it specifically says that docker container ID &lt;code>14f5a4c98e5653a15e334880b9a58504adda416b06f359655bf251073b160ab1&lt;/code> consumes 35G of disk space.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">35G/var/lib/docker/containers/14f5a4c98e5653a15e334880b9a58504adda416b06f359655bf251073b160ab1&lt;/code>&lt;/pre>&lt;p>I checked if my application container writes anything to the container filesystem. It appeared that my application wrote only less than 1MB of data to to a docker volume. Also it wrote only to the docker volume under &lt;code>/var/lib/docker/volumes&lt;/code>and not to the container file system &lt;code>/var/lib/docker/containers&lt;/code>. So it doesn&amp;rsquo;t explain why the container file system is so bulky (35GB).&lt;/p>&lt;p>I finally checked the log file where the container logs are stored.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$ sudo sh -c &amp;quot;du -ch /var/lib/docker/containers/*/*-json.log&amp;quot;35G/var/lib/docker/containers/14f5a4c98e5653a15e334880b9a58504adda416b06f359655bf251073b160ab1/14f5a4c98e5653a15e334880b9a58504adda416b06f359655bf251073b160ab1-json.log7.0M/var/lib/docker/containers/23a77fd2325f88deec8cb45480326a1044e914b08b85553ae9d0d29b19e81698/23a77fd2325f88deec8cb45480326a1044e914b08b85553ae9d0d29b19e81698-json.log42M/var/lib/docker/containers/3d03387e4de11ca77ce0dc4399f8bb58f62cd70faabbe36b7230355dcbc6b39b/3d03387e4de11ca77ce0dc4399f8bb58f62cd70faabbe36b7230355dcbc6b39b-json.log4.0K/var/lib/docker/containers/63955d57625448a53e4a391dcf6f97952e30f44a13ee65ac24d90e747a1ffca1/63955d57625448a53e4a391dcf6f97952e30f44a13ee65ac24d90e747a1ffca1-json.log35Gtotal&lt;/code>&lt;/pre>&lt;h2 id="root-cause-of-the-problem">Root Cause of the Problem&lt;/h2>&lt;p>The above output shows that the JSON log file of container &lt;code>14f5a4c98e5653a15e334880b9a58504adda416b06f359655bf251073b160ab1&lt;/code> is 35GB in size.&lt;/p>&lt;p>This is the root cause of the problem. The container or the application is generating too many logs or the docker container has been running for several months without being restarted or respawned.&lt;/p>&lt;p>In my case, both were causing the problem. My container has been running for more than 2 months undisturbed and also it was logging all the Postgres DB SQL queries to stdout.&lt;/p>&lt;p>Mainly, I didn&amp;rsquo;t explicitly configure the docker daemon to rotate the log file when it reaches a specific size, say 200MB or 1GB on disk.&lt;/p>&lt;h2 id="how-to-solve-the-issue">How to solve the issue&lt;/h2>&lt;p>There is a temporary solution and a permanent solution to this problem.&lt;/p>&lt;h3 id="temporary-solution">Temporary Solution&lt;/h3>&lt;p>The temporary solution is to free up the disk space to prevent the application from misbehaving or crashing in a production system. This is a more easy and quick solution than killing and restarting the container in a production system.&lt;/p>&lt;p>You could use either one of the commands below to truncate the log file. You need to be a sudo user to write to the log file. I highly recommend using the truncate command and it worked well for me.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">echo &amp;quot;&amp;quot; &amp;gt; $(docker inspect --format='{{.LogPath}}' &amp;lt;container_name_or_id&amp;gt;)&lt;/code>&lt;/pre>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">truncate -s 0 $(docker inspect --format='{{.LogPath}}' &amp;lt;container_name_or_id&amp;gt;)&lt;/code>&lt;/pre>&lt;p>&lt;strong>Note:&lt;/strong> Do this at your own risk, as you are trying to compete with the docker daemon to write into the log file that has been opened and used by docker daemon.&lt;/p>&lt;p>After you truncate the log file, make sure that &lt;code>docker logs &amp;lt;container-id&amp;gt;&lt;/code> shows the newer logs. If there was any mess up due to the truncate command, you&amp;rsquo;ll get an error like the one shown below:&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">error from daemon in stream: Error grabbing logs: invalid character '\x00' looking for beginning of value&lt;/code>&lt;/pre>&lt;p>This error means you have messed up the json contents of the JSON log file. It will do no harm to the functionality of running container but just that you&amp;rsquo;ll no longer be able to see meaning logs from the &lt;code>docker logs&lt;/code> command output.&lt;/p>&lt;p>But, when you remove and recreate the container to implement the permanent solution explained in the next section (during a scheduled maintenance window of your production system), this problem would go away.&lt;/p>&lt;h3 id="permanent-solution">Permanent Solution&lt;/h3>&lt;p>The permanent solution is to configure docker daemon or the docker container to use only the maximum specified size for the log file. When the max file size is reached, the docker daemon should rotate the logs and start logging from the beginning of the file. This way the log file doesn&amp;rsquo;t grow in size automatically.&lt;/p>&lt;p>Also, I had to disable logging in my GORM postgres DB driver to prevent unnecessary logging for each SQL query in a production build.&lt;/p>&lt;p>Follow the instructions in the docker documentation page on &lt;a href="https://docs.docker.com/config/containers/logging/json-file/">JSON File logging driver&lt;/a> for setting up the max log file size and to turn on the automatic log rotation feature.&lt;/p>&lt;p>Basically, you need to create a daemon.json file under /etc/docker directory with the following content.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">{ &amp;quot;log-driver&amp;quot;: &amp;quot;json-file&amp;quot;, &amp;quot;log-opts&amp;quot;: { &amp;quot;max-size&amp;quot;: &amp;quot;10m&amp;quot;, &amp;quot;max-file&amp;quot;: &amp;quot;3&amp;quot; }}&lt;/code>&lt;/pre>&lt;p>It basically limits the JSON log file size to 10MB maximum and thereafter the file will be rotated automatically and logged from the beggining of the file.&lt;/p>&lt;p>Also, the daemon could create maximum 3 such log files only. The default value for this field is 1.&lt;/p>&lt;p>&lt;strong>Note:&lt;/strong> You need to restart the docker daemon to make this configuration to take effect. Also all the containers running in the host need to be restarted.&lt;/p>&lt;p>Alternatively, you could just configure a container to limit the log file size and enable automatic log rotation using the following command:&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$ docker run \ --log-driver json-file --log-opt max-size=10m \ --log-opt max-file=3 \ alpine echo hello world&lt;/code>&lt;/pre>&lt;p>That&amp;rsquo;s all folks! Hope that was useful.&lt;/p></description></item><item><title>Kubernetes Cluster Remote Access</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-cluster-remote-access/</link><pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-cluster-remote-access/</guid><description>&lt;p>Kubernetes is a popular cluster and container management/orchestration platform widely used in pulic and private clouds. SocketXP TLS VPN solution (a lightweight VPN) provides secure remote access to private Kubernetes Clusters in your private or public cloud.&lt;/p>&lt;p>SocketXP agent is available as a docker container in the &lt;a href="https://hub.docker.com/r/expresssocket/socketxp">SocketXP DockerHub Repository&lt;/a> and can be run in the following modes in a Kubernetes Cluster:&lt;/p>&lt;ul>&lt;li>&lt;!-- raw HTML omitted -->Standalone container&lt;!-- raw HTML omitted --> - Runs as a separate deployment&lt;/li>&lt;li>&lt;!-- raw HTML omitted -->Sidecar container&lt;!-- raw HTML omitted --> - Runs alongside your application container in a pod&lt;/li>&lt;/ul>&lt;h2 id="standalone-container">Standalone Container:&lt;/h2>&lt;p>First go to &lt;a href="https://porta.socketxp.com/">SocketXP Portal&lt;/a>. Signup for a free account and get your authtoken there. Use the authtoken to create a Kubernetes secret as shown below.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl create secret generic socketxp-credentials --from-literal&lt;span style="color:#f92672">=&lt;/span>authtoken&lt;span style="color:#f92672">=[&lt;/span>your-auth-token-goes-here&lt;span style="color:#f92672">]&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Verify that the secret &lt;code>socketxp-credentials&lt;/code> got created.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ kubectl get secretsNAME TYPE DATA AGEdefault-token-5skb7 kubernetes.io/service-account-token &lt;span style="color:#ae81ff">3&lt;/span> 4hsocketxp-credentials Opaque &lt;span style="color:#ae81ff">1&lt;/span> 4h$&lt;/code>&lt;/pre>&lt;/div>&lt;p>We&amp;rsquo;ll use the below &lt;code>config.json&lt;/code> file to configure the SocketXP agent Docker container. In this example, we are trying to create a secure public web URL and a TLS VPN tunnel to the Kubernetes API server.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">&lt;span style="color:#960050;background-color:#1e0010">$&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">cat&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">config.json&lt;/span>{ &lt;span style="color:#f92672">&amp;#34;tunnel_enabled&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>, &lt;span style="color:#f92672">&amp;#34;tunnels&amp;#34;&lt;/span> : [{ &lt;span style="color:#f92672">&amp;#34;destination&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;https://kubernetes.default&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;protocol&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;tls&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;custom_domain&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;subdomain&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span> }], &lt;span style="color:#f92672">&amp;#34;relay_enabled&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">false&lt;/span>, }&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next create a Kubernetes configmap to store the above SocketXP agent configuration file.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl create configmap socketxp-configmap --from-file&lt;span style="color:#f92672">=&lt;/span>/home/test-user/config.json&lt;/code>&lt;/pre>&lt;/div>&lt;p>Verify that the &lt;code>socketxp-configmap&lt;/code> got created.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ kubectl describe configmaps socketxp-configmapName: socketxp-configmapNamespace: defaultLabels: &amp;lt;none&amp;gt;Annotations: &amp;lt;none&amp;gt;Data&lt;span style="color:#f92672">====&lt;/span>config.json:----&lt;span style="color:#f92672">{&lt;/span> &lt;span style="color:#e6db74">&amp;#34;tunnel_enabled&amp;#34;&lt;/span>: true, &lt;span style="color:#e6db74">&amp;#34;tunnels&amp;#34;&lt;/span> : &lt;span style="color:#f92672">[{&lt;/span> &lt;span style="color:#e6db74">&amp;#34;destination&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;https://kubernetes.default&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;protocol&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;tls&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;custom_domain&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;subdomain&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span> &lt;span style="color:#f92672">}]&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;relay_enabled&amp;#34;&lt;/span>: false &lt;span style="color:#f92672">}&lt;/span>Events: &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now that we have created the authtoken secret and the configmap needed by the SocketXP agent, it&amp;rsquo;s time to launch the SocketXP Docker container &lt;code>expresssocket/socketxp:latest&lt;/code> as a Kubernetes Deployment.&lt;/p>&lt;p>Here is the &lt;code>deployment.yaml&lt;/code> file we&amp;rsquo;ll use to create a standalone SocketXP agent deployment.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#ae81ff">$cat deployment.yaml &lt;/span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">apps/v1&lt;/span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Deployment&lt;/span>&lt;span style="color:#f92672">metadata&lt;/span>: &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span> &lt;span style="color:#f92672">labels&lt;/span>: &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span>&lt;span style="color:#f92672">spec&lt;/span>: &lt;span style="color:#f92672">replicas&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">selector&lt;/span>: &lt;span style="color:#f92672">matchLabels&lt;/span>: &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span> &lt;span style="color:#f92672">template&lt;/span>: &lt;span style="color:#f92672">metadata&lt;/span>: &lt;span style="color:#f92672">labels&lt;/span>: &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span> &lt;span style="color:#f92672">spec&lt;/span>: &lt;span style="color:#f92672">containers&lt;/span>: - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">expresssocket/socketxp:latest&lt;/span> &lt;span style="color:#f92672">env&lt;/span>: - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">AUTHTOKEN&lt;/span> &lt;span style="color:#f92672">valueFrom&lt;/span>: &lt;span style="color:#f92672">secretKeyRef&lt;/span>: &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp-credentials&lt;/span> &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">authtoken&lt;/span> &lt;span style="color:#f92672">volumeMounts&lt;/span>: - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">config-volume&lt;/span> &lt;span style="color:#f92672">mountPath&lt;/span>: &lt;span style="color:#ae81ff">/data&lt;/span> &lt;span style="color:#f92672">volumes&lt;/span>: - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">config-volume&lt;/span> &lt;span style="color:#f92672">configMap&lt;/span>: &lt;span style="color:#75715e"># Provide the name of the ConfigMap containing the files you want&lt;/span> &lt;span style="color:#75715e">#to add to the container&lt;/span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp-configmap&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>::: warning Note:We have created a separate volume named &lt;code>config-volume&lt;/code> and mounted it under &lt;code>/data&lt;/code> directory inside the container, so that the &lt;code>socketxp-configmap&lt;/code> will be available as a &lt;code>config.json&lt;/code> file under the &lt;code>/data&lt;/code> directory in the running container.:::&lt;/p>&lt;p>Next, check if the pods are created from the deployment and running.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ kubectl get podsNAME READY STATUS RESTARTS AGEsocketxp-75cb4dd7c9-bhxfp 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 4s$&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now you can retrieve the SocketXP Public URL created for your Kubernetes API server from the SocketXP Portal Page at: &lt;a href="https://portal.socketxp.com/#/tunnels">https://portal.socketxp.com/#/tunnels&lt;/a> or from the pod logs as shown below.&lt;/p>&lt;pre>&lt;code>$ kubectl logs socketxp-75cb4dd7c9-bhxfp......Login Succeeded.User [] Email [test-user@gmail.com].Connected.Public URL -&amp;gt; https://test-user-fn4mda420.socketxp.com&lt;/code>&lt;/pre>&lt;p>You can now use the above SocketXP Public URL to access the Kubernetes Cluster&amp;rsquo;s API server remotely using a &lt;code>kubectl&lt;/code> utility or directly using your custom application.&lt;/p>&lt;p>If you are using a locally installed &lt;code>kubectl&lt;/code> utility from your laptop to remotely access the Kubernetes, then update the API server URL in the &lt;code>kubectl&lt;/code> config file located at $HOME/.kube/config to use the SocketXP Public URL &lt;code>https://test-user-fn4mda420.socketxp.com&lt;/code>&lt;/p>&lt;pre>&lt;code>apiVersion: v1clusters:- cluster: certificate-authority: /Users/test-user/.minikube/ca.crt server: https://test-user-fn4mda420.socketxp.com name: minikubecontexts:- context: cluster: minikube user: minikube name: minikube......&lt;/code>&lt;/pre>&lt;p>Please ensure that you also copy the client certificate, CA certificate and private key files from your Kubernetes cluster&amp;rsquo;s master node to your laptop in the appropriate folder as specified in the kubectl config file.&lt;/p>&lt;p>Verify that the config works fine, using the following command:&lt;/p>&lt;pre>&lt;code>kubectl config view&lt;/code>&lt;/pre>&lt;p>Now remote access or remote manage your private Kubernetes Cluster from your laptop by executing any &lt;code>kubectl&lt;/code> command locally:&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ kubectl get podsNAME READY STATUS RESTARTS AGEsocketxp-75cb4dd7c9-bhxfp 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 1h&lt;/code>&lt;/pre>&lt;/div>&lt;p>::: warning Note:When you create more than one replica of the SocketXP agent pod using the deployment, each pod would be assigned a unique SocketXP Public URL. This is because each SocketXP agent pod running in the Kubernetes Cluster will fetch a new Public URL from the SocketXP Cloud Gateway.:::&lt;/p>&lt;h2 id="sidecar-container">Sidecar Container:&lt;/h2>&lt;p>You have a containerized application and you want to expose the application via a public URL, run SocketXP agent as a sidecar container.&lt;/p>&lt;p>For example, let&amp;rsquo;s say you want to run an &lt;code>nginx&lt;/code> server and access it remotely using a public URL. We could run SocketXP agent Docker container as a sidecar container that runs alongside the &lt;code>nginx&lt;/code> server.&lt;/p>&lt;p>Follow the same steps mentioned in the previous section to create a Kubernetes secret for storing the authtoken and a configmap for storing the SocketXP config file.&lt;/p>&lt;p>But update the &lt;code>tunnels[].destination&lt;/code> in the SocketXP config file to the nginx server&amp;rsquo;s local web URL, which is &lt;code>http://localhost:80&lt;/code>. Note the presence of &lt;code>http&lt;/code> (and not &lt;code>https&lt;/code>)in the local URL. This is unlike the Kubernetes API server&amp;rsquo;s local web URL shown in the previous section.&lt;/p>&lt;p>The nginx server used in this example is not configured to use it&amp;rsquo;s own SSL certificate and private key. It&amp;rsquo;s a HTTP server and not a HTTPS server.&lt;/p>&lt;p>So update the &lt;code>protocol&lt;/code> field in the config file to &lt;code>http&lt;/code>.&lt;/p>&lt;p>However, if you have setup your application as a HTTPS server with its SSL certificate and private key, then set the &lt;code>protocol&lt;/code> as &lt;code>tls&lt;/code>.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">&lt;span style="color:#960050;background-color:#1e0010">$&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">cat&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">config.json&lt;/span>{ &lt;span style="color:#f92672">&amp;#34;tunnel_enabled&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>, &lt;span style="color:#f92672">&amp;#34;tunnels&amp;#34;&lt;/span> : [{ &lt;span style="color:#f92672">&amp;#34;destination&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;http://localhost:80&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;protocol&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;http&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;custom_domain&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;subdomain&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span> }], &lt;span style="color:#f92672">&amp;#34;relay_enabled&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">false&lt;/span>, }&lt;/code>&lt;/pre>&lt;/div>&lt;p>And the &lt;code>deployment.yaml&lt;/code> file should look this.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#ae81ff">$ cat deployment.yaml &lt;/span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">apps/v1&lt;/span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Deployment&lt;/span>&lt;span style="color:#f92672">metadata&lt;/span>: &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span> &lt;span style="color:#f92672">labels&lt;/span>: &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span>&lt;span style="color:#f92672">spec&lt;/span>: &lt;span style="color:#f92672">replicas&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">selector&lt;/span>: &lt;span style="color:#f92672">matchLabels&lt;/span>: &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span> &lt;span style="color:#f92672">template&lt;/span>: &lt;span style="color:#f92672">metadata&lt;/span>: &lt;span style="color:#f92672">labels&lt;/span>: &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span> &lt;span style="color:#f92672">spec&lt;/span>: &lt;span style="color:#f92672">containers&lt;/span>: &lt;span style="color:#75715e"># nginx container&lt;/span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">nginx&lt;/span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">nginx:latest&lt;/span> &lt;span style="color:#f92672">ports&lt;/span>: - &lt;span style="color:#f92672">containerPort&lt;/span>: &lt;span style="color:#ae81ff">80&lt;/span> &lt;span style="color:#75715e"># socketxp container&lt;/span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">expresssocket/socketxp:latest&lt;/span> &lt;span style="color:#f92672">env&lt;/span>: - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">AUTHTOKEN&lt;/span> &lt;span style="color:#f92672">valueFrom&lt;/span>: &lt;span style="color:#f92672">secretKeyRef&lt;/span>: &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp-credentials&lt;/span> &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">authtoken&lt;/span> &lt;span style="color:#f92672">volumeMounts&lt;/span>: - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">config-volume&lt;/span> &lt;span style="color:#f92672">mountPath&lt;/span>: &lt;span style="color:#ae81ff">/data&lt;/span> &lt;span style="color:#f92672">volumes&lt;/span>: - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">config-volume&lt;/span> &lt;span style="color:#f92672">configMap&lt;/span>: &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp-configmap&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The only major difference between this &lt;code>deployment.yaml&lt;/code> file and the one used in the previous section is the presence of the &lt;code>nginx&lt;/code> container specifications under the &lt;code>containers spec&lt;/code> section of the file.&lt;/p>&lt;p>Create a deployment using the &lt;code>deployment.yaml&lt;/code> file and check the status of the pods.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ kubectl apply -f deployment.yaml deployment.apps/socketxp created&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl get podsNAME READY STATUS RESTARTS AGEsocketxp-6477b747d-llrrw 2/2 Running &lt;span style="color:#ae81ff">0&lt;/span> 5m&lt;/code>&lt;/pre>&lt;/div>&lt;p>Retrieve the SocketXP Public URL assigned to your pod from the portal page or from the pod container&amp;rsquo;s logs as shown below.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl logs socketxp-6477b747d-llrrw socketxp......Using config file: /data/config.jsonLogin Succeeded.User &lt;span style="color:#f92672">[]&lt;/span> Email &lt;span style="color:#f92672">[&lt;/span>test-user@gmail.com&lt;span style="color:#f92672">]&lt;/span>.Connected.Public URL -&amp;gt; https://test-user-3bui06m5.socketxp.com&lt;/code>&lt;/pre>&lt;/div>&lt;p>Access the above SocketXP public URL in a browser and you should see the welcome message from the &lt;code>nginx&lt;/code> server as shown below.&lt;/p>&lt;p>&lt;img src="../assets/img/kubernetes-cluster-remote-access/nginx-pod.png" alt="kubernetes cluster docker container remote access">&lt;/p>&lt;p>::: warning Note:When you create more than one replica of your application pod, each pod would be assigned a unique SocketXP Public URL. This is because each SocketXP agent running as a sidecar container inside the application pod will fetch a new Public URL from the SocketXP Cloud Gateway.:::&lt;/p></description></item><item><title>Kubernetes Dashboard Remote Access</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-dashboard-remote-access/</link><pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-dashboard-remote-access/</guid><description>&lt;h1 id="kubernetes-dashboard-remote-access">Kubernetes Dashboard Remote Access&lt;/h1>&lt;p>Kubernetes is a popular cluster and container management/orchestration platform widely used in pulic and private clouds. Kubernetes Dashboard is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources.&lt;/p>&lt;p>SocketXP TLS VPN solution (a lightweight VPN) provides secure remote access to private Kubernetes Clusters in your private cloud or public cloud. SocketXP also provides a secure public URL to access your local private applications including Kubernetes Dashboard.&lt;/p>&lt;p>SocketXP agent is available as a docker container in the &lt;a href="https://hub.docker.com/r/expresssocket/socketxp">SocketXP DockerHub Repository&lt;/a>. Run the SocketXP Docker container as a standalone container (as explained in the below sections) in your Kubernetes cluster to setup remote access to your Kubernetes Dashboard.&lt;/p>&lt;h2 id="deploying-the-dashboard-ui">Deploying the Dashboard UI:&lt;/h2>&lt;p>Follow the instructions in the &lt;a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/">Kubernetes Open Source Project page&lt;/a> on how to deploy and setup the Dashboard UI in your Kubernetes Cluster.&lt;/p>&lt;p>&lt;a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/">https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/&lt;/a>&lt;/p>&lt;p>As explained in the Kubernetes official documentation, you need to run the &lt;code>kubectl&lt;/code> CLI utility in proxy mode to access your dashboard in a web browser.&lt;/p>&lt;h2 id="overall-strategy">Overall Strategy&lt;/h2>&lt;p>Here is the overall strategy to setup remote access to your Kubernetes Dashboard:&lt;/p>&lt;ul>&lt;li>Deploy SocketXP VPN agent Docker container in your K8 cluster.&lt;/li>&lt;li>Install the &lt;code>kubectl&lt;/code> CLI utility locally on your laptop.&lt;/li>&lt;li>Setup the kubectl config file in your laptop with SocketXP Public URL, K8 SSL Certs, and Key.&lt;/li>&lt;li>Remote access your private Kubernetes cluster from your laptop using the &lt;code>kubectl&lt;/code> CLI utility.&lt;/li>&lt;li>Run kubectl in proxy mode in your laptop.&lt;/li>&lt;li>Access your Kubernetes dashboard in a web browser via the local kubectl proxy.&lt;/li>&lt;/ul>&lt;h2 id="socketxp-agent-docker-container-deployment">SocketXP Agent Docker Container Deployment:&lt;/h2>&lt;p>First go to &lt;a href="https://porta.socketxp.com/">SocketXP Portal&lt;/a>. Signup for a free account and get your authtoken there. Use the authtoken to create a Kubernetes secret as shown below.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl create secret generic socketxp-credentials --from-literal&lt;span style="color:#f92672">=&lt;/span>authtoken&lt;span style="color:#f92672">=[&lt;/span>your-auth-token-goes-here&lt;span style="color:#f92672">]&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Verify that the secret &lt;code>socketxp-credentials&lt;/code> got created.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ kubectl get secretsNAME TYPE DATA AGEdefault-token-5skb7 kubernetes.io/service-account-token &lt;span style="color:#ae81ff">3&lt;/span> 4hsocketxp-credentials Opaque &lt;span style="color:#ae81ff">1&lt;/span> 4h$&lt;/code>&lt;/pre>&lt;/div>&lt;p>We&amp;rsquo;ll use the below &lt;code>config.json&lt;/code> file to configure the SocketXP agent Docker container. In this example, we are trying to create a secure public web URL and a TLS VPN tunnel to the Kubernetes API server.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">&lt;span style="color:#960050;background-color:#1e0010">$&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">cat&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">config.json&lt;/span>{ &lt;span style="color:#f92672">&amp;#34;tunnel_enabled&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>, &lt;span style="color:#f92672">&amp;#34;tunnels&amp;#34;&lt;/span> : [{ &lt;span style="color:#f92672">&amp;#34;destination&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;https://kubernetes.default&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;protocol&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;tls&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;custom_domain&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;subdomain&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span> }], &lt;span style="color:#f92672">&amp;#34;relay_enabled&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">false&lt;/span>, }&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next create a Kubernetes configmap to store the above SocketXP agent configuration file.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl create configmap socketxp-configmap --from-file&lt;span style="color:#f92672">=&lt;/span>/home/test-user/config.json&lt;/code>&lt;/pre>&lt;/div>&lt;p>Verify that the &lt;code>socketxp-configmap&lt;/code> got created.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ kubectl describe configmaps socketxp-configmapName: socketxp-configmapNamespace: defaultLabels: &amp;lt;none&amp;gt;Annotations: &amp;lt;none&amp;gt;Data&lt;span style="color:#f92672">====&lt;/span>config.json:----&lt;span style="color:#f92672">{&lt;/span> &lt;span style="color:#e6db74">&amp;#34;tunnel_enabled&amp;#34;&lt;/span>: true, &lt;span style="color:#e6db74">&amp;#34;tunnels&amp;#34;&lt;/span> : &lt;span style="color:#f92672">[{&lt;/span> &lt;span style="color:#e6db74">&amp;#34;destination&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;https://kubernetes.default&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;protocol&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;tls&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;custom_domain&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;subdomain&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span> &lt;span style="color:#f92672">}]&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;relay_enabled&amp;#34;&lt;/span>: false &lt;span style="color:#f92672">}&lt;/span>Events: &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now that we have created the authtoken secret and the configmap needed by the SocketXP agent, it&amp;rsquo;s time to launch the SocketXP Docker container &lt;code>expresssocket/socketxp:latest&lt;/code> as a Kubernetes Deployment.&lt;/p>&lt;p>Here is the &lt;code>deployment.yaml&lt;/code> file we&amp;rsquo;ll use to create a standalone SocketXP agent deployment.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#ae81ff">$cat deployment.yaml &lt;/span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">apps/v1&lt;/span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Deployment&lt;/span>&lt;span style="color:#f92672">metadata&lt;/span>: &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span> &lt;span style="color:#f92672">labels&lt;/span>: &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span>&lt;span style="color:#f92672">spec&lt;/span>: &lt;span style="color:#f92672">replicas&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">selector&lt;/span>: &lt;span style="color:#f92672">matchLabels&lt;/span>: &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span> &lt;span style="color:#f92672">template&lt;/span>: &lt;span style="color:#f92672">metadata&lt;/span>: &lt;span style="color:#f92672">labels&lt;/span>: &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span> &lt;span style="color:#f92672">spec&lt;/span>: &lt;span style="color:#f92672">containers&lt;/span>: - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">expresssocket/socketxp:latest&lt;/span> &lt;span style="color:#f92672">env&lt;/span>: - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">AUTHTOKEN&lt;/span> &lt;span style="color:#f92672">valueFrom&lt;/span>: &lt;span style="color:#f92672">secretKeyRef&lt;/span>: &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp-credentials&lt;/span> &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">authtoken&lt;/span> &lt;span style="color:#f92672">volumeMounts&lt;/span>: - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">config-volume&lt;/span> &lt;span style="color:#f92672">mountPath&lt;/span>: &lt;span style="color:#ae81ff">/data&lt;/span> &lt;span style="color:#f92672">volumes&lt;/span>: - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">config-volume&lt;/span> &lt;span style="color:#f92672">configMap&lt;/span>: &lt;span style="color:#75715e"># Provide the name of the ConfigMap containing the files you want&lt;/span> &lt;span style="color:#75715e">#to add to the container&lt;/span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp-configmap&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>::: warning Note:We have created a separate volume named &lt;code>config-volume&lt;/code> and mounted it under &lt;code>/data&lt;/code> directory inside the container, so that the &lt;code>socketxp-configmap&lt;/code> will be available as a &lt;code>config.json&lt;/code> file under the &lt;code>/data&lt;/code> directory in the running container.:::&lt;/p>&lt;p>Next, check if the pods are created from the deployment and running.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ kubectl get podsNAME READY STATUS RESTARTS AGEsocketxp-75cb4dd7c9-bhxfp 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 4s$&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now you can retrieve the SocketXP Public URL created for your Kubernetes API server from the SocketXP Portal Page at: &lt;a href="https://portal.socketxp.com/#/tunnels">https://portal.socketxp.com/#/tunnels&lt;/a> or from the pod logs as shown below.&lt;/p>&lt;pre>&lt;code>$ kubectl logs socketxp-75cb4dd7c9-bhxfp......Login Succeeded.User [] Email [test-user@gmail.com].Connected.Public URL -&amp;gt; https://test-user-fn4mda420.socketxp.com&lt;/code>&lt;/pre>&lt;p>You can now use the above SocketXP Public URL to access the Kubernetes Cluster&amp;rsquo;s API server remotely using a &lt;code>kubectl&lt;/code> utility.&lt;/p>&lt;h2 id="local-kubectl-installation">Local kubectl installation&lt;/h2>&lt;p>Install the &lt;code>kubectl&lt;/code> CLI utility locally on your laptop to remote access your Kubernetes cluster. Follow the instructions here to download and install &lt;code>kubectl&lt;/code> on your laptop:&lt;/p>&lt;p>&lt;a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">https://kubernetes.io/docs/tasks/tools/install-kubectl/&lt;/a>&lt;/p>&lt;p>After you have installed the kubectl CLI utility, overwrite the &lt;code>kubectl&lt;/code> config file located at $HOME/.kube/config in your laptop with the one from your cluster&amp;rsquo;s master node($HOME/.kube/config).&lt;/p>&lt;p>Next, update the API server URL in your &lt;code>kubectl&lt;/code> config file to use the SocketXP Public URL &lt;code>https://test-user-fn4mda420.socketxp.com&lt;/code>, as shown below.&lt;/p>&lt;pre>&lt;code>apiVersion: v1clusters:- cluster: certificate-authority: /Users/test-user/.minikube/ca.crt server: https://test-user-fn4mda420.socketxp.com name: minikubecontexts:- context: cluster: minikube user: minikube name: minikube......&lt;/code>&lt;/pre>&lt;p>Please ensure that you also copy the client certificate, CA certificate and private key files or authtoken from your Kubernetes cluster&amp;rsquo;s master node to your laptop in the appropriate folder as specified in the kubectl config file.&lt;/p>&lt;p>Verify that the config works fine, using the following command:&lt;/p>&lt;pre>&lt;code>kubectl config view&lt;/code>&lt;/pre>&lt;p>Now remote access or remote manage your private Kubernetes Cluster from your laptop by executing any &lt;code>kubectl&lt;/code> command locally:&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ kubectl get podsNAME READY STATUS RESTARTS AGEsocketxp-75cb4dd7c9-bhxfp 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 1h&lt;/code>&lt;/pre>&lt;/div>&lt;p>::: warning Note:When you create more than one replica of the SocketXP agent pod using the deployment, each pod would be assigned a unique SocketXP Public URL. This is because each SocketXP agent pod running in the Kubernetes Cluster will fetch a new Public URL from the SocketXP Cloud Gateway.:::&lt;/p>&lt;h2 id="run-kubectl-in-proxy-mode">Run kubectl in proxy mode&lt;/h2>&lt;p>To remote access your Kubernetes Dashboard, run the &lt;code>kubectl&lt;/code> CLI utility in proxy mode in your laptop as shown below:&lt;/p>&lt;pre>&lt;code>$ kubectl proxy Starting to serve on 127.0.0.1:8001&lt;/code>&lt;/pre>&lt;p>Let this command continue to run in the foreground.&lt;/p>&lt;h2 id="remote-access-kubernetes-dashboard">Remote access Kubernetes Dashboard:&lt;/h2>&lt;p>Now you can remote access your Kubernetes Dashboard from your laptop using the following local URL via the kubectl proxy. Kubectl will make Dashboard available at:&lt;/p>&lt;p>&lt;a href="http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/">http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/&lt;/a>.&lt;/p>&lt;p>Now you can view or manage your k8 resources.&lt;/p>&lt;p>&lt;img src="../assets/img/kubernetes-dashboard-remote-access/k8-dashboard-remote-access.png" alt="kubernetes dashboard remote access">&lt;/p></description></item><item><title>Kubernetes Microservice Remote Access</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-microservice-remote-access/</link><pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-microservice-remote-access/</guid><description>&lt;p>In this document, we&amp;rsquo;ll discuss how to develop, debug, and test a microservice locally on your laptop, without having to run all the other microservices of an app also on your laptop.&lt;/p>&lt;p>The solution discussed in this document could also be used to debug, fix, and test a problem reported by your customer in your live production cluster by routing accesses to your microservice (that has the bug) to the one running in your laptop with the fix.&lt;/p>&lt;h2 id="problem-statement">Problem Statement:&lt;/h2>&lt;p>Let&amp;rsquo;s say your team is focussed on developing and testing a front-end ReactJS microservice for your company&amp;rsquo;s application that has like 25 other microservices such as Spring Boot, Elastic Search, Redis, Postgres, Grafana, and so on, in it.&lt;/p>&lt;p>As a dev, you would usually try to run all of these microservices in your laptop so that you could develop,build and test changes to your front-end ReactJS App natively on your laptop. But running all these 25 microservices on your laptop will make it into a toaster, because you are pushing your laptop cpu, memory and power consumption to their maximum capacity throughout the day.&lt;/p>&lt;h2 id="workarounds-used-today">Workarounds Used Today&lt;/h2>&lt;p>Many try to workaround this problem by upgrade their RAM or disk or fan module or even the laptop itself.&lt;/p>&lt;p>This adds more costs to your organization. Sometimes it may make you wonder, &amp;ldquo;Was it really a smart decision to convert your monolithic applications into microservices, in the first place?&amp;rdquo;&lt;/p>&lt;p>Many organizations that have started their journey into the microservices world, for the benefits it offers, have faced the exact same problem. And, fortunately, &lt;a href="https://www.socketxp.com">SocketXP&lt;/a> has a solution to address this problem.&lt;/p>&lt;h2 id="wall-power-sockets-analogy">Wall Power Sockets Analogy&lt;/h2>&lt;p>Let me ask you this question. Would you run a huge power generator in your house just because you needed some electricity to run your TV, Air Conditioner and Washing machine? The answer would be &amp;ldquo;no&amp;rdquo;. You&amp;rsquo;d just install a couple of sockets on your wall, run a power cable to the energy company in your city and hook up your electrical equipments to those wall sockets. Neat and simple isn&amp;rsquo;t it?&lt;/p>&lt;p>Then why do we try to run all our microservices(both 800 pound gorillas and tiny little mice ones) on that poor little laptop on your lap, all day long, just because you need to focus on developing and testing one front-end microservice? You could very well install software &amp;ldquo;wall sockets&amp;rdquo; on your laptop and have your ReactJS front-end microservice talk to 25 other microservices running remotely in a cloud (or some on-prem cluster perhaps?) via the software &amp;ldquo;wall sockets&amp;rdquo; And vice-versa.&lt;/p>&lt;p>Our SocketXP Microservice Remote Access solution would help create those software &amp;ldquo;wall-sockets&amp;rdquo; in your laptop, for all your remote microservices.&lt;/p>&lt;h2 id="how-socketxp-microservice-remote-access-solution-works">How SocketXP Microservice Remote Access Solution Works?&lt;/h2>&lt;p>You need to download and run a SocketXP proxy agent (also available as Docker Container) on your laptop and on your remote cluster where other N-1 microservices of your application reside.&lt;/p>&lt;p>SocketXP proxy agent will create &amp;ldquo;TCP/IP&amp;rdquo; sockets on your laptop (one for each of the microservice running in a remote cluster) and run a bidirectional TLS tunnel between your laptop and your remote cluster. We&amp;rsquo;d also adjust DNS records (for those remotely running microservices) in your laptop to route to the localhost IP address. So that, any requests from your front-end microservices to these 25 other microservices, would be routed to these local sockets.&lt;/p>&lt;p>::: tip Why this is important:Your front-end microservice would continue to connect to other microservices of the app as usual, without requiring any modifications to its code or configurations.:::&lt;/p>&lt;h2 id="docker-container-demo">Docker Container Demo&lt;/h2>&lt;p>For this exercise, we are going to run Postgres DB as a Docker Container in a remote server and make a python based backend microservice (under development in my laptop) access the remote DB via the SocketXP proxy.&lt;/p>&lt;p>Postgres DB is accessible via the hostname &lt;code>postgres&lt;/code> and port &lt;code>5432&lt;/code>&lt;/p>&lt;p>Here is the simple backend microservice written in python.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-python" data-lang="python">&lt;span style="color:#960050;background-color:#1e0010">$&lt;/span>cat backend&lt;span style="color:#f92672">.&lt;/span>py &lt;span style="color:#f92672">import&lt;/span> psycopg2con &lt;span style="color:#f92672">=&lt;/span> psycopg2&lt;span style="color:#f92672">.&lt;/span>connect(database&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;postgres&amp;#34;&lt;/span>, user&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;gvelrajan&amp;#34;&lt;/span>, password&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;pa$$word123&amp;#34;&lt;/span>, host&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;postgres&amp;#34;&lt;/span>, port&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;5432&amp;#34;&lt;/span>)&lt;span style="color:#66d9ef">print&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;Database opened successfully&amp;#34;&lt;/span>)cur &lt;span style="color:#f92672">=&lt;/span> con&lt;span style="color:#f92672">.&lt;/span>cursor()cur&lt;span style="color:#f92672">.&lt;/span>execute(&lt;span style="color:#e6db74">&amp;#39;&amp;#39;&amp;#39;CREATE TABLE STUDENT&lt;/span>&lt;span style="color:#e6db74"> (ADMISSION INT PRIMARY KEY NOT NULL,&lt;/span>&lt;span style="color:#e6db74"> NAME TEXT NOT NULL,&lt;/span>&lt;span style="color:#e6db74"> AGE INT NOT NULL,&lt;/span>&lt;span style="color:#e6db74"> COURSE CHAR(50),&lt;/span>&lt;span style="color:#e6db74"> DEPARTMENT CHAR(50));&amp;#39;&amp;#39;&amp;#39;&lt;/span>)&lt;span style="color:#66d9ef">print&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;Table created successfully&amp;#34;&lt;/span>)cur&lt;span style="color:#f92672">.&lt;/span>execute(&lt;span style="color:#e6db74">&amp;#34;INSERT INTO STUDENT (ADMISSION,NAME,AGE,COURSE,DEPARTMENT) VALUES (34231, &amp;#39;Dave&amp;#39;, 19, &amp;#39;Information Technology&amp;#39;, &amp;#39;ICT&amp;#39;)&amp;#34;&lt;/span>);cur&lt;span style="color:#f92672">.&lt;/span>execute(&lt;span style="color:#e6db74">&amp;#34;INSERT INTO STUDENT (ADMISSION,NAME,AGE,COURSE,DEPARTMENT) VALUES (34232, &amp;#39;Gary&amp;#39;, 18, &amp;#39;Electrical Engineering&amp;#39;, &amp;#39;ICT&amp;#39;)&amp;#34;&lt;/span>);cur&lt;span style="color:#f92672">.&lt;/span>execute(&lt;span style="color:#e6db74">&amp;#34;INSERT INTO STUDENT (ADMISSION,NAME,AGE,COURSE,DEPARTMENT) VALUES (34233, &amp;#39;Abel&amp;#39;, 17, &amp;#39;Computer Science&amp;#39;, &amp;#39;ICT&amp;#39;)&amp;#34;&lt;/span>);&lt;span style="color:#66d9ef">print&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;Student records added to the table successfully&amp;#34;&lt;/span>)con&lt;span style="color:#f92672">.&lt;/span>commit()con&lt;span style="color:#f92672">.&lt;/span>close()&lt;/code>&lt;/pre>&lt;/div>&lt;p>The python backend microservice connects to the DB using the hostname &amp;lsquo;postgres&amp;rsquo; and port &amp;lsquo;5432&amp;rsquo;&lt;/p>&lt;h3 id="run-the-postgres-db-docker-container">Run the Postgres DB Docker Container&lt;/h3>&lt;p>First create a docker volume&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ docker volume create postgres&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next run the Postgres DB container and mount the &lt;code>postgres&lt;/code> volume in the host machine to the directory &lt;code>/var/lib/postgresql/data&lt;/code> inside the container.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ docker run --name postgres --restart unless-stopped --net appnet --mount source&lt;span style="color:#f92672">=&lt;/span>postgres,target&lt;span style="color:#f92672">=&lt;/span>/var/lib/postgresql/data -e POSTGRES_USER&lt;span style="color:#f92672">=&lt;/span>gvelrajan -e POSTGRES_PASSWORD&lt;span style="color:#f92672">=&lt;/span>pa$$word123 -e POSTGRES_DB&lt;span style="color:#f92672">=&lt;/span>postgres -d postgres:latest&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="run-socketxp-proxy-agent-container-in-a-clusterserver">Run SocketXP Proxy Agent container in a Cluster/Server&lt;/h3>&lt;p>First go to &lt;a href="https://portal.socketxp.com">SocketXP Portal&lt;/a>. Signup for a free account and get your authtoken there.&lt;/p>&lt;p>Use your authtoken in the below config file.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">&lt;span style="color:#960050;background-color:#1e0010">$&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">cat&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">config.json&lt;/span>{ &lt;span style="color:#f92672">&amp;#34;authtoken&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;lt;your-auth-token-goes-here&amp;gt;&amp;#34;&lt;/span> &lt;span style="color:#e6db74">&amp;#34;tunnel_enabled&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>, &lt;span style="color:#f92672">&amp;#34;tunnels&amp;#34;&lt;/span> : [{ &lt;span style="color:#f92672">&amp;#34;destination&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;tcp://postgres:5432&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;iot_device_id&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;postgres-mservice-cluster1&amp;#34;&lt;/span>, }] }&lt;/code>&lt;/pre>&lt;/div>&lt;p>Store the config.json file in a local directory(/home/gvelrajan/config/config.json) and map the directory into the SocketXP docker container at /data as shown in the command below to start the docker container.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">docker run --name socketxp --restart unless-stopped --net appnet -d -v /home/gvelrajan/config:/data expresssocket/socketxp:latest&lt;/code>&lt;/pre>&lt;/div>&lt;p>Execute the following command to check the logs for success status or any errors:&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">docker logs socketxp&lt;/code>&lt;/pre>&lt;/div>&lt;p>It would have something like this:&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">Connected.TCP tunnel &lt;span style="color:#f92672">[&lt;/span>gvelrajan-5bz4y6hi&lt;span style="color:#f92672">]&lt;/span> created.&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now go to the SocketXP Portal&amp;rsquo;s &lt;a href="https://portal.socketxp.com/#/tunnels">Tunnel Page&lt;/a>. Check the name of the tunnel created and its status.&lt;/p>&lt;h3 id="run-socketxp-proxy-agent-on-your-laptop">Run SocketXP Proxy Agent on your Laptop&lt;/h3>&lt;p>First download the SocketXP Proxy Agent in your laptop from the &lt;a href="https://www.socketxp.com/download">SocketXP download page&lt;/a>. SocketXP Proxy Agent is available for all OS versions - Windows/Mac/Linux.&lt;/p>&lt;p>Use the same SocketXP authtoken you used in the previous section to authenticate the proxy agent with SocketXP Cloud Gateway.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">&lt;span style="color:#960050;background-color:#1e0010">$&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">cat&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">config.json&lt;/span>{ &lt;span style="color:#f92672">&amp;#34;authtoken&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;lt;your-auth-token-goes-here&amp;gt;&amp;#34;&lt;/span> &lt;span style="color:#e6db74">&amp;#34;tunnel_enabled&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>, &lt;span style="color:#f92672">&amp;#34;tunnels&amp;#34;&lt;/span> : [{ &lt;span style="color:#f92672">&amp;#34;destination&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;tcp://postgres:5432&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;iot_device_id&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;postgres-mservice-cluster1&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;iot_slave&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span> }] }&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ socketxp --config config.json&lt;/code>&lt;/pre>&lt;/div>&lt;p>The above config requests socketxp agent to run in slave mode and create a local proxy TCP socket in your laptop at &lt;code>127.0.0.1:5432&lt;/code>. Also it requests socketxp agent to create a secure TLS connection to the &lt;code>postgres-mservice-cluster1&lt;/code> proxy device (running in your remote cluster or server) we created in the previous section.&lt;/p>&lt;blockquote>&lt;p>&lt;strong>Note:&lt;/strong>Although it is called TCP socket, we run TLS on top of it. So your data is securely transmitted over the internet end-to-end.&lt;/p>&lt;/blockquote>&lt;p>Next, update the DNS record in your laptop with the following command:&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ echo &lt;span style="color:#e6db74">&amp;#34;127.0.0.1 postgres&amp;#34;&lt;/span> | sudo tee -a /etc/hosts &amp;gt; /dev/null&lt;/code>&lt;/pre>&lt;/div>&lt;p>We need to add the above hostname mapping for our &lt;code>postgres&lt;/code> microservice so that any local requests to the microservice will be routed to our SocketXP proxy agent listening at IP &lt;code>127.0.0.1&lt;/code>.&lt;/p>&lt;h3 id="run-the-python-backend-microserver">Run the python backend microserver&lt;/h3>&lt;p>We are all set to run the python backend microservice under development in our laptop.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ python backend.pyDatabase opened successfullyTable created successfullyStudent records added to the table successfully&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let&amp;rsquo;s validate this success by running an SQL query in our remote postgres database.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sql" data-lang="sql">&lt;span style="color:#960050;background-color:#1e0010">$&lt;/span> docker &lt;span style="color:#66d9ef">exec&lt;/span> &lt;span style="color:#f92672">-&lt;/span>it postgres psql &lt;span style="color:#f92672">-&lt;/span>U gvelrajan postgrespostgres&lt;span style="color:#f92672">=#&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">\&lt;/span>&lt;span style="color:#66d9ef">c&lt;/span> postgrespostgres&lt;span style="color:#f92672">=#&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">\&lt;/span>d List &lt;span style="color:#66d9ef">of&lt;/span> relations &lt;span style="color:#66d9ef">Schema&lt;/span> &lt;span style="color:#f92672">|&lt;/span> Name &lt;span style="color:#f92672">|&lt;/span> &lt;span style="color:#66d9ef">Type&lt;/span> &lt;span style="color:#f92672">|&lt;/span> &lt;span style="color:#66d9ef">Owner&lt;/span> &lt;span style="color:#75715e">--------+---------+-------+-----------&lt;/span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">public&lt;/span> &lt;span style="color:#f92672">|&lt;/span> student &lt;span style="color:#f92672">|&lt;/span> &lt;span style="color:#66d9ef">table&lt;/span> &lt;span style="color:#f92672">|&lt;/span> gannygans(&lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#66d9ef">row&lt;/span>)postgres&lt;span style="color:#f92672">=#&lt;/span> &lt;span style="color:#66d9ef">select&lt;/span> &lt;span style="color:#f92672">*&lt;/span> &lt;span style="color:#66d9ef">from&lt;/span> student; admission &lt;span style="color:#f92672">|&lt;/span> name &lt;span style="color:#f92672">|&lt;/span> age &lt;span style="color:#f92672">|&lt;/span> course &lt;span style="color:#f92672">|&lt;/span> department &lt;span style="color:#75715e">-----------+------+-----+----------------------------------------------------+----------------------------------------------------&lt;/span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#ae81ff">34231&lt;/span> &lt;span style="color:#f92672">|&lt;/span> Dave &lt;span style="color:#f92672">|&lt;/span> &lt;span style="color:#ae81ff">19&lt;/span> &lt;span style="color:#f92672">|&lt;/span> Information Technology &lt;span style="color:#f92672">|&lt;/span> ICT &lt;span style="color:#ae81ff">34232&lt;/span> &lt;span style="color:#f92672">|&lt;/span> Gary &lt;span style="color:#f92672">|&lt;/span> &lt;span style="color:#ae81ff">18&lt;/span> &lt;span style="color:#f92672">|&lt;/span> Electrical Engineering &lt;span style="color:#f92672">|&lt;/span> ICT &lt;span style="color:#ae81ff">34233&lt;/span> &lt;span style="color:#f92672">|&lt;/span> Abel &lt;span style="color:#f92672">|&lt;/span> &lt;span style="color:#ae81ff">17&lt;/span> &lt;span style="color:#f92672">|&lt;/span> Computer Science &lt;span style="color:#f92672">|&lt;/span> ICT (&lt;span style="color:#ae81ff">3&lt;/span> &lt;span style="color:#66d9ef">rows&lt;/span>)(&lt;span style="color:#66d9ef">END&lt;/span>)&lt;/code>&lt;/pre>&lt;/div>&lt;p>Awesome! We are able to develop/test our local python backend service and make it to seamlessly connect with the remote &lt;code>postgres&lt;/code> DB microservice.&lt;/p>&lt;h2 id="make-a-remote-nginx-microservice-locally-accessible">Make a remote NGINX microservice locally accessible&lt;/h2>&lt;p>Now suppose, in addition to the postgres db microservice, we also need to make an NGINX microserice locally accessible from our laptop, use the following &lt;code>config.json&lt;/code> file to setup the SocketXP Proxy Agent in the remote server/cluster.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">&lt;span style="color:#960050;background-color:#1e0010">$&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">cat&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">config.json&lt;/span>{ &lt;span style="color:#f92672">&amp;#34;authtoken&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;lt;your-auth-token-goes-here&amp;gt;&amp;#34;&lt;/span> &lt;span style="color:#e6db74">&amp;#34;tunnel_enabled&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>, &lt;span style="color:#f92672">&amp;#34;tunnels&amp;#34;&lt;/span> : [ { &lt;span style="color:#f92672">&amp;#34;destination&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;tcp://postgres:5432&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;iot_device_id&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;postgres-mservice-cluster1&amp;#34;&lt;/span>, }, { &lt;span style="color:#f92672">&amp;#34;destination&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;tcp://nginx:80&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;iot_device_id&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;nginx-mservice-cluster1&amp;#34;&lt;/span>, }, ] }&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, go to your laptop, and update the config.json file to create a local Proxy for the nginx microservice, as shown below.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">&lt;span style="color:#960050;background-color:#1e0010">$&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">cat&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">config.json&lt;/span>{ &lt;span style="color:#f92672">&amp;#34;authtoken&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;lt;your-auth-token-goes-here&amp;gt;&amp;#34;&lt;/span> &lt;span style="color:#e6db74">&amp;#34;tunnel_enabled&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>, &lt;span style="color:#f92672">&amp;#34;tunnels&amp;#34;&lt;/span> : [ { &lt;span style="color:#f92672">&amp;#34;destination&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;tcp://postgres:5432&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;iot_device_id&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;postgres-mservice-cluster1&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;iot_slave&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span> }, { &lt;span style="color:#f92672">&amp;#34;destination&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;tcp://nginx:80&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;iot_device_id&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;nginx-mservice-cluster1&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;iot_slave&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span> }, ] }&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ socketxp --config config.json&lt;/code>&lt;/pre>&lt;/div>&lt;p>The above config will create a TCP listening socket at port 80 in your laptop. Make sure port 80 is free in your laptop and not used by any other application.&lt;/p>&lt;p>Next, add the follow DNS entry into your &lt;code>/etc/hosts&lt;/code> file, so that any local DNS resolution requests to access your &lt;code>ngnix&lt;/code> service will be routed to the SocketXP Proxy Agent listening on IP 127.0.0.1.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ echo &lt;span style="color:#e6db74">&amp;#34;127.0.0.1 nginx&amp;#34;&lt;/span> | sudo tee -a /etc/hosts &amp;gt; /dev/null&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now open up a browser in your laptop and point it to: http://nginx:80 or simply http://nginx. It will open up the NNGINX home page as shown below.&lt;/p>&lt;p>&lt;img src="https://dev-to-uploads.s3.amazonaws.com/i/naop7dmmxxkordq6uqdh.png" alt="socketxp microservice remote access">&lt;/p>&lt;h2 id="share-same-set-of-microservices-between-multiple-developers">Share Same Set of Microservices Between Multiple Developers:&lt;/h2>&lt;p>SocketXP Microservice Remote Access solution allows multiple developers to share same set of microservices running in a development server or K8s cluster. You can create a many-to-one channel for each microservice that needs to be accessed remotely.&lt;/p>&lt;p>In SocketXP parlance, the SocketXP service proxy agent running as a standalone deployment along side a microservice (that needs to be accessed remotely) in a K8s cluster is called the master. And the SocketXP service proxy agent running in the developer&amp;rsquo;s laptop is called the slave. Many slave instances of SocketXP proxy agent could run simultaneously on different developer&amp;rsquo;s laptop and connect using a secure channel to the master instance of SocketXP proxy agent.&lt;/p>&lt;p>For the example usecase discussed in this document, each developer would run a SocketXP proxy agent on their laptop with the following config.json file to connect to the Postgresql DB and Nginx microservice running in your remote server/cluster. This communication channel goes via the single instance of the SocketXP proxy agent running in your remote server or cluster.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">&lt;span style="color:#960050;background-color:#1e0010">$&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">cat&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">config.json&lt;/span>{ &lt;span style="color:#f92672">&amp;#34;authtoken&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;lt;your-auth-token-goes-here&amp;gt;&amp;#34;&lt;/span> &lt;span style="color:#e6db74">&amp;#34;tunnel_enabled&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>, &lt;span style="color:#f92672">&amp;#34;tunnels&amp;#34;&lt;/span> : [ { &lt;span style="color:#f92672">&amp;#34;destination&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;tcp://postgres:5432&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;iot_device_id&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;postgres-mservice-cluster1&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;iot_slave&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span> }, { &lt;span style="color:#f92672">&amp;#34;destination&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;tcp://nginx:80&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;iot_device_id&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;nginx-mservice-cluster1&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;iot_slave&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span> }, ] }&lt;/code>&lt;/pre>&lt;/div>&lt;p>Basically a many-to-one secure communication channel is created to aid multiple developers share the same set of microservices running in a remote server or cluster.&lt;/p>&lt;h2 id="kubernetes-microservice-remote-access">Kubernetes Microservice Remote Access:&lt;/h2>&lt;p>So far, in our dicussion we have run the SocketXP proxy agent as a Docker container or as a binary natively. We could potentially deploy SocketXP proxy agent as a Kubernetes Deployment to remote access microservices running in a K8s cluster.&lt;/p>&lt;p>To remotely access a microservice running in a Kubernetes(K8s) cluster or a minikube cluster, you need to run the SocketXP proxy agent as a Standalone Deployment in the cluster.&lt;/p>&lt;p>Instructions to run SocketXP proxy agent Docker container as a standalone deployment in a Kubernetes cluster can be found in this documentation page:&lt;/p>&lt;p>&lt;a href="https://www.socketxp.com/docs/guide/kubernetes-cluster-remote-access.html#standalone-container">https://www.socketxp.com/docs/guide/kubernetes-cluster-remote-access.html#standalone-container&lt;/a>&lt;/p>&lt;p>The only difference is that you need to use the following &lt;code>config.json&lt;/code> file(same as the one used earlier above):&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">&lt;span style="color:#960050;background-color:#1e0010">$&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">cat&lt;/span> &lt;span style="color:#960050;background-color:#1e0010">config.json&lt;/span>{ &lt;span style="color:#f92672">&amp;#34;authtoken&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;&amp;lt;your-auth-token-goes-here&amp;gt;&amp;#34;&lt;/span> &lt;span style="color:#e6db74">&amp;#34;tunnel_enabled&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>, &lt;span style="color:#f92672">&amp;#34;tunnels&amp;#34;&lt;/span> : [ { &lt;span style="color:#f92672">&amp;#34;destination&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;tcp://postgres:5432&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;iot_device_id&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;postgres-mservice-cluster1&amp;#34;&lt;/span>, }, { &lt;span style="color:#f92672">&amp;#34;destination&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;tcp://nginx:80&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;#34;iot_device_id&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;nginx-mservice-cluster1&amp;#34;&lt;/span>, }, ] }&lt;/code>&lt;/pre>&lt;/div>&lt;p>The above config.json file assumes that your Postgresql DB and Nginx microservices are accessible within the cluster using the DNS names &lt;code>postgres&lt;/code> and &lt;code>nginx&lt;/code>.&lt;/p>&lt;p>::: warning Need Support?We are looking for early users to evaluate our SocketXP Microservice Remote Access Solution. Please write to us at: &lt;a href="support@socketxp.com">support@socketxp.com&lt;/a> if you have any questions or need any support.:::&lt;/p></description></item><item><title>Kubernetes Pod Remote SSH Access</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-pod-remote-ssh-access/</link><pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-pod-remote-ssh-access/</guid><description>&lt;p>Kubernetes is a popular cluster and container management/orchestration platform widely used in public and private clouds.&lt;/p>&lt;p>SocketXP TLS VPN solution is a lightweight VPN that provides secure remote SSH access to pods running private Kubernetes Clusters in your on-prem cloud or public cloud or multi-cloud or hybrid cloud.&lt;/p>&lt;p>SocketXP Solution provides two options to SSH into your Kubernetes pods:&lt;/p>&lt;ul>&lt;li>Install and run SocketXP agent inside your app container&lt;/li>&lt;li>Run SocketXP agent container as a standalone container&lt;/li>&lt;/ul>&lt;h2 id="option-1-install-socketxp-inside-your-app-container">Option #1: Install SocketXP inside your app container:&lt;/h2>&lt;p>Let&amp;rsquo;s assume for the illustration purpose that you have the following helloworld nodejs app.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$cat myapp.js var http &lt;span style="color:#f92672">=&lt;/span> require&lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">&amp;#39;http&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>;//create a server object:http.createServer&lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#66d9ef">function&lt;/span> &lt;span style="color:#f92672">(&lt;/span>req, res&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#f92672">{&lt;/span>res.writeHead&lt;span style="color:#f92672">(&lt;/span>200, &lt;span style="color:#f92672">{&lt;/span>&lt;span style="color:#e6db74">&amp;#39;Content-Type&amp;#39;&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;text/html&amp;#39;&lt;/span>&lt;span style="color:#f92672">})&lt;/span>;res.write&lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">&amp;#39;&amp;lt;h1&amp;gt;Hello World!&amp;lt;/h1&amp;gt;&amp;#39;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>; //write a response to the clientres.end&lt;span style="color:#f92672">()&lt;/span>; //end the response&lt;span style="color:#f92672">})&lt;/span>.listen&lt;span style="color:#f92672">(&lt;/span>80&lt;span style="color:#f92672">)&lt;/span>; //the http server object listens on port &lt;span style="color:#ae81ff">80&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And your Dockerfile looks something like this:&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$cat Dockerfile FROM centos:latest&lt;span style="color:#75715e"># your app specific stuff &lt;/span>RUN yum -y install nodejsRUN mkdir -p /usr/src/appCOPY ./myapp.js /usr/src/appWORKDIR /usr/src/appEXPOSE &lt;span style="color:#ae81ff">80&lt;/span> ENTRYPOINT &lt;span style="color:#f92672">[&lt;/span>&lt;span style="color:#e6db74">&amp;#34;node&amp;#34;&lt;/span>,&lt;span style="color:#e6db74">&amp;#34;myapp.js&amp;#34;&lt;/span>&lt;span style="color:#f92672">]&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You might build a Docker container image with the name &lt;code>helloworld&lt;/code>, as shown below:&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ docker build -t socketxp/helloworld .&lt;/code>&lt;/pre>&lt;/div>&lt;p>And run your &lt;code>helloworld&lt;/code> Docker container as shown below:&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ docker run --name &lt;span style="color:#e6db74">&amp;#34;helloworld&amp;#34;&lt;/span> -p 80:8000 socketxp/helloworld&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now you could access your app on port 8000 locally on your host machine as:&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ curl http://localhost:8000&amp;lt;h1&amp;gt;Hello World!&amp;lt;/h1&amp;gt;&lt;/code>&lt;/pre>&lt;/div>&lt;p>Alternatively, you could run your &lt;code>helloworld&lt;/code> Docker container as a pod in a Kubernetes cluster using the following &lt;code>deployment.yaml&lt;/code>.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#ae81ff">kube-master-node$ cat deployment.yaml &lt;/span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">apps/v1&lt;/span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Deployment &lt;/span>&lt;span style="color:#f92672">metadata&lt;/span>: &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">helloworld&lt;/span>&lt;span style="color:#f92672">spec&lt;/span>: &lt;span style="color:#f92672">replicas&lt;/span>: &lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#f92672">minReadySeconds&lt;/span>: &lt;span style="color:#ae81ff">15&lt;/span> &lt;span style="color:#f92672">strategy&lt;/span>: &lt;span style="color:#f92672">type&lt;/span>: &lt;span style="color:#ae81ff">RollingUpdate &lt;/span> &lt;span style="color:#f92672">rollingUpdate&lt;/span>: &lt;span style="color:#f92672">maxUnavailable&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">maxSurge&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">template&lt;/span>: &lt;span style="color:#f92672">metadata&lt;/span>: &lt;span style="color:#f92672">labels&lt;/span>: &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">helloworld &lt;/span> &lt;span style="color:#f92672">spec&lt;/span>: &lt;span style="color:#f92672">containers&lt;/span>: - &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">socketxp/hello-world:latest &lt;/span> &lt;span style="color:#f92672">imagePullPolicy&lt;/span>: &lt;span style="color:#ae81ff">Always &lt;/span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">helloworld &lt;/span> &lt;span style="color:#f92672">ports&lt;/span>: - &lt;span style="color:#f92672">containerPort&lt;/span>: &lt;span style="color:#ae81ff">80&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You could create a Kubernetes service using the &lt;code>service.yaml&lt;/code> shown below to expose your &lt;code>helloworld&lt;/code> Nodejs app to the internet.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#ae81ff">kube-master-node$ cat service.yaml &lt;/span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Service &lt;/span>&lt;span style="color:#f92672">metadata&lt;/span>: &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">helloworld&lt;/span>&lt;span style="color:#f92672">spec&lt;/span>: &lt;span style="color:#f92672">externalIPs&lt;/span>: - &lt;span style="color:#ae81ff">34.160.0.3&lt;/span> &lt;span style="color:#f92672">type&lt;/span>: &lt;span style="color:#ae81ff">LoadBalancer &lt;/span> &lt;span style="color:#f92672">ports&lt;/span>: - &lt;span style="color:#f92672">port&lt;/span>: &lt;span style="color:#ae81ff">80&lt;/span> &lt;span style="color:#f92672">protocol&lt;/span>: &lt;span style="color:#ae81ff">TCP &lt;/span> &lt;span style="color:#f92672">targetPort&lt;/span>: &lt;span style="color:#ae81ff">80&lt;/span> &lt;span style="color:#f92672">selector&lt;/span>: &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">helloworld &lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kube-master-node$ kubectl apply -f service.yaml&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now you could access your &lt;code>helloworld&lt;/code> nodejs app using the public IP as shown below:&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$curl http://34.160.0.3:80&amp;lt;h1&amp;gt;Hello World!&amp;lt;/h1&amp;gt;&lt;/code>&lt;/pre>&lt;/div>&lt;p>So far so good.&lt;/p>&lt;h3 id="add-remote-ssh-access-capability">Add Remote SSH Access Capability&lt;/h3>&lt;p>But in the future, if your &lt;code>helloworld&lt;/code> app behaves erratically and you may want to SSH into one of the pods from a remote machine to debug it, you need a Kubernetes Pod Remote Access Service like SocketXP.&lt;/p>&lt;p>Follow the instructions below to package SocketXP Remote Access Agent binary along with your &lt;code>helloworld&lt;/code> nodejs app in your &lt;code>socketxp/helloworld:latest&lt;/code> Docker container, so that you could SSH into your pods from remote.&lt;/p>&lt;p>Here are the instructions to package the SocketXP Remote Access Agent binary into your &lt;code>helloworld&lt;/code> docker container. We&amp;rsquo;ll name the new docker container, as &lt;code>helloworld-ssh&lt;/code> docker container because the new container has remote SSH access capabilities.&lt;/p>&lt;p>First, download SocketXP agent to your local directory&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">curl -O https://portal.socketxp.com/download/linux/socketxp &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> chmod +wx socketxp&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create a shell script file named &lt;code>script.sh&lt;/code> with the following contents.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$cat script.sh &lt;span style="color:#75715e">#!/bin/sh&lt;/span>/var/local/socketxp/socketxp login $AUTHTOKEN --no-auto-update/var/local/socketxp/socketxp connect tcp://localhost:22 --iot-device-id $HOSTNAME --enable-sshd --ssh-username $SSH_USERNAME --ssh-password $SSH_PASSWORD --no-auto-update &amp;amp;&lt;span style="color:#75715e"># Run myapp&lt;/span>node /usr/src/app/myapp.js&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here is a sample Dockerfile that packages the SocketXP agent binary, the script.sh file, and instructions to run it, along with the contents of your &lt;code>helloworld&lt;/code> nodejs app Dockerfile.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$cat Dockerfile FROM centos:latest&lt;span style="color:#75715e"># your app specific stuff &lt;/span>RUN yum -y install nodejsRUN mkdir -p /usr/src/appCOPY ./myapp.js /usr/src/appWORKDIR /usr/src/appEXPOSE &lt;span style="color:#ae81ff">8000&lt;/span> &lt;span style="color:#75715e">#ENTRYPOINT [&amp;#34;node&amp;#34;,&amp;#34;myapp.js&amp;#34;]&lt;/span>&lt;span style="color:#75715e"># SocketXP Environ Variables&lt;/span>ENV LANG C.UTF-8ENV AUTHTOKEN &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>ENV SSH_USERNAME &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>ENV SSH_PASSWORD &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>ENV IOT_DEVICE_ID &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#75715e"># Install and run SocketXP agent binary&lt;/span>RUN mkdir -p /var/local/socketxpCOPY script.sh /var/local/socketxp/COPY socketxp /var/local/socketxp/WORKDIR /var/local/socketxpRUN chmod a+x socketxpEXPOSE &lt;span style="color:#ae81ff">22&lt;/span>ENTRYPOINT &lt;span style="color:#f92672">[&lt;/span>&lt;span style="color:#e6db74">&amp;#34;/bin/sh&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;script.sh&amp;#34;&lt;/span>&lt;span style="color:#f92672">]&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now let&amp;rsquo;s create a new Docker image named &lt;code>helloworld-ssh&lt;/code> as shown below:&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ docker build -t socketxp/helloworld-ssh .&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="run-as-docker-container-on-host-machine">Run as Docker Container on Host Machine:&lt;/h3>&lt;p>Now you could run this Docker container on your host machine as shown below:&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ docker run --name helloworld-ssh -d -p 80:8000 -e SSH_USERNAME&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;test-user&amp;#34;&lt;/span> -e SSH_PASSWORD&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;pa&lt;/span>$$&lt;span style="color:#e6db74">word123&amp;#34;&lt;/span> -e AUTHTOKEN&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&amp;lt;your-socketxp-authtoken-goes-here&amp;gt;&amp;#34;&lt;/span> socketxp/helloworld-ssh:latest&lt;/code>&lt;/pre>&lt;/div>&lt;p>You could access your nodejs app from your host machine as shown below:&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$curl http://localhost:8000&amp;lt;h1&amp;gt;Hello World!&amp;lt;/h1&amp;gt;&lt;/code>&lt;/pre>&lt;/div>&lt;p>You could SSH into your Docker container from the &lt;a href="https://portal.socketxp.com/#/devices">SocketXP Portal&amp;rsquo;s IoT Device Page&lt;/a>. Click the terminal icon next to your container listed in the page. [&lt;strong>Note&lt;/strong>: The Docker container ID of the app will be listed under the DeviceID column]. It will take you to an SSH login window in a new tab. Provide the same SSH login credentials that you provided in the &amp;ldquo;docker run&amp;rdquo; command above. You&amp;rsquo;ll get shell access to your &lt;code>helloworld-ssh&lt;/code> Docker container.&lt;/p>&lt;h3 id="run-as-kubernetes-pod">Run as Kubernetes Pod:&lt;/h3>&lt;p>Alternatively, you could run the &lt;code>socketxp/helloworld-ssh:latest&lt;/code> Docker container as a Kubernetes pod/deployment/service and you could remote SSH into this pod using the SSH credentials you have setup.&lt;/p>&lt;p>Here is how your new deployment.yaml file would look like:&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#ae81ff">$cat deployment.yaml &lt;/span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">apps/v1&lt;/span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Deployment &lt;/span>&lt;span style="color:#f92672">metadata&lt;/span>: &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">helloworld-ssh&lt;/span>&lt;span style="color:#f92672">spec&lt;/span>: &lt;span style="color:#f92672">replicas&lt;/span>: &lt;span style="color:#ae81ff">2&lt;/span> &lt;span style="color:#f92672">minReadySeconds&lt;/span>: &lt;span style="color:#ae81ff">15&lt;/span> &lt;span style="color:#f92672">strategy&lt;/span>: &lt;span style="color:#f92672">type&lt;/span>: &lt;span style="color:#ae81ff">RollingUpdate &lt;/span> &lt;span style="color:#f92672">rollingUpdate&lt;/span>: &lt;span style="color:#f92672">maxUnavailable&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">maxSurge&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">template&lt;/span>: &lt;span style="color:#f92672">metadata&lt;/span>: &lt;span style="color:#f92672">labels&lt;/span>: &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">helloworld-ssh &lt;/span> &lt;span style="color:#f92672">spec&lt;/span>: &lt;span style="color:#f92672">containers&lt;/span>: - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">helloworld-ssh&lt;/span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">socketxp/helloworld-ssh:latest&lt;/span> &lt;span style="color:#f92672">imagePullPolicy&lt;/span>: &lt;span style="color:#ae81ff">Always&lt;/span> &lt;span style="color:#f92672">env&lt;/span>: - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">AUTHTOKEN&lt;/span> &lt;span style="color:#f92672">valueFrom&lt;/span>: &lt;span style="color:#f92672">secretKeyRef&lt;/span>: &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp-credentials&lt;/span> &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">authtoken &lt;/span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">SSH_USERNAME &lt;/span> &lt;span style="color:#f92672">valueFrom&lt;/span>: &lt;span style="color:#f92672">secretKeyRef&lt;/span>: &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp-credentials&lt;/span> &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">ssh_username &lt;/span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">SSH_PASSWORD&lt;/span> &lt;span style="color:#f92672">valueFrom&lt;/span>: &lt;span style="color:#f92672">secretKeyRef&lt;/span>: &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp-credentials&lt;/span> &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">ssh_password &lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>But, before you could apply this &lt;code>deployment.yaml&lt;/code> in a Kubernetes cluster, you need to create the environment variables used by SocketXP agent in the &lt;code>deployment.yaml&lt;/code> file above.&lt;/p>&lt;h3 id="socketxp-env-variables-creation">SocketXP ENV variables creation&lt;/h3>&lt;p>First go to &lt;a href="https://portal.socketxp.com">SocketXP Portal&lt;/a>. Signup for a free account and get your authtoken there. Use the authtoken to create a Kubernetes secret as shown below.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl create secret generic socketxp-credentials --from-literal&lt;span style="color:#f92672">=&lt;/span>authtoken&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&amp;lt;your-auth-token-goes-here&amp;gt;&amp;#34;&lt;/span> --from-literal&lt;span style="color:#f92672">=&lt;/span>ssh_username&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;test-user&amp;#34;&lt;/span> --from-literal&lt;span style="color:#f92672">=&lt;/span>ssh_password&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;pa&lt;/span>$$&lt;span style="color:#e6db74">word123&amp;#34;&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You could specify your own SSH login username and password in the above CLI. You&amp;rsquo;ll be prompted to provide the same username and password when you attempt to SSH login using any SSH client.&lt;/p>&lt;p>Verify that the secret &lt;code>socketxp-credentials&lt;/code> got created.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ kubectl get secretsNAME TYPE DATA AGEdefault-token-5skb7 kubernetes.io/service-account-token &lt;span style="color:#ae81ff">3&lt;/span> 4hsocketxp-credentials Opaque &lt;span style="color:#ae81ff">1&lt;/span> 4h$&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now that we have created the secrets needed by the SocketXP agent, it&amp;rsquo;s time to launch the &lt;code>helloworld-ssh&lt;/code> deployment using the &lt;code>deployment.yaml&lt;/code> file above.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kube-master-node$ kubectl apply -f deployment.yaml&lt;/code>&lt;/pre>&lt;/div>&lt;p>Your &lt;code>helloworld&lt;/code> loadbalancer service could continue to run as it is without any changes. This is because SocketXP Remote Access Agent doesn&amp;rsquo;t require any Kubernetes Service to be created for it. The agent would automatically create a secure TLS VPN tunnel to the SocketXP Cloud Gateway, from where you could SSH into your pods.&lt;/p>&lt;p>Go to the SocketXP Cloud Gateway Portal&amp;rsquo;s &lt;a href="https://portal.socketxp.com/#/devices">IoT Devices Page&lt;/a> using your browser. You&amp;rsquo;ll see your pods (2 replicas) listed as IoT Devices. You could SSH into your pods by clicking the terminal icon next to them. It will open up an SSH Login window in a new tab. Enter the same SSH login credentials you provided when creating the Kubernetes secret &lt;code>socketxp-credentials&lt;/code>.&lt;/p>&lt;p>After successful authentication, you&amp;rsquo;ll be provided with the pod&amp;rsquo;s shell prompt.&lt;/p>&lt;h2 id="option-2-run-socketxp-as-a-standalone-container">Option #2: Run SocketXP as a Standalone Container:&lt;/h2>&lt;p>In this second option, you need to run SocketXP Agent Docker container as a standalone container pod in your Kubernetes cluster, so that you could get remote SSH access to other pods(your apps) running in the cluster, through this SocketXP Agent pod.&lt;/p>&lt;p>SocketXP Agent Docker container is shipped with an SSH server daemon and Kubernetes &lt;a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">&lt;code>kubectl&lt;/code>&lt;/a> CLI utility packaged in it. SocketXP Docker container can be downloaded from the &lt;a href="https://hub.docker.com/r/expresssocket/socketxp-kubectl">SocketXP DockerHub Repository&lt;/a>. This turn-key solution from SocketXP lets you to setup a remote SSH into a pod in your private cluster, in just few minutes.&lt;/p>&lt;p>Let&amp;rsquo;s get started.&lt;/p>&lt;p>First go to &lt;a href="https://porta.socketxp.com/">SocketXP Portal&lt;/a>. Signup for a free account and get your authtoken there. Use the authtoken to create a Kubernetes secret as shown below.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl create secret generic socketxp-credentials --from-literal&lt;span style="color:#f92672">=&lt;/span>authtoken&lt;span style="color:#f92672">=[&lt;/span>your-auth-token-goes-here&lt;span style="color:#f92672">]&lt;/span> --from-literal&lt;span style="color:#f92672">=&lt;/span>ssh_username&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;test-user&amp;#34;&lt;/span> --from-literal&lt;span style="color:#f92672">=&lt;/span>ssh_password&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;password123&amp;#34;&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You could specify your own SSH login username and password in the above CLI. You&amp;rsquo;ll be prompted to provide the same username and password when you attempt to SSH login using any SSH client.&lt;/p>&lt;p>Verify that the secret &lt;code>socketxp-credentials&lt;/code> got created.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ kubectl get secretsNAME TYPE DATA AGEdefault-token-5skb7 kubernetes.io/service-account-token &lt;span style="color:#ae81ff">3&lt;/span> 4hsocketxp-credentials Opaque &lt;span style="color:#ae81ff">1&lt;/span> 4h$&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now that we have created the secrets needed by the SocketXP agent, it&amp;rsquo;s time to launch the SocketXP Docker container &lt;code>expresssocket/socketxp-kubectl:latest&lt;/code> as a Kubernetes Deployment.&lt;/p>&lt;p>Here is the &lt;code>deployment.yaml&lt;/code> file we&amp;rsquo;ll use to create a standalone SocketXP agent deployment.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#ae81ff">$cat deployment.yaml &lt;/span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">apps/v1&lt;/span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Deployment&lt;/span>&lt;span style="color:#f92672">metadata&lt;/span>: &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span> &lt;span style="color:#f92672">labels&lt;/span>: &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span>&lt;span style="color:#f92672">spec&lt;/span>: &lt;span style="color:#f92672">replicas&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span> &lt;span style="color:#f92672">selector&lt;/span>: &lt;span style="color:#f92672">matchLabels&lt;/span>: &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span> &lt;span style="color:#f92672">template&lt;/span>: &lt;span style="color:#f92672">metadata&lt;/span>: &lt;span style="color:#f92672">labels&lt;/span>: &lt;span style="color:#f92672">app&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span> &lt;span style="color:#f92672">spec&lt;/span>: &lt;span style="color:#f92672">containers&lt;/span>: - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp&lt;/span> &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">expresssocket/socketxp-kubectl:latest&lt;/span> &lt;span style="color:#f92672">imagePullPolicy&lt;/span>: &lt;span style="color:#ae81ff">Always&lt;/span> &lt;span style="color:#f92672">env&lt;/span>: - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">AUTHTOKEN&lt;/span> &lt;span style="color:#f92672">valueFrom&lt;/span>: &lt;span style="color:#f92672">secretKeyRef&lt;/span>: &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp-credentials&lt;/span> &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">authtoken &lt;/span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">SSH_USERNAME &lt;/span> &lt;span style="color:#f92672">valueFrom&lt;/span>: &lt;span style="color:#f92672">secretKeyRef&lt;/span>: &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp-credentials&lt;/span> &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">ssh_username &lt;/span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">SSH_PASSWORD&lt;/span> &lt;span style="color:#f92672">valueFrom&lt;/span>: &lt;span style="color:#f92672">secretKeyRef&lt;/span>: &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">socketxp-credentials&lt;/span> &lt;span style="color:#f92672">key&lt;/span>: &lt;span style="color:#ae81ff">ssh_password &lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now create a deployment using the &lt;code>deployment.yaml&lt;/code> file and check the status of the pods.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ kubectl apply -f deployment.yaml deployment.apps/socketxp created&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">$ kubectl get podsNAME READY STATUS RESTARTS AGEsocketxp-75cb4dd7c9-bhxfp 1/1 Running &lt;span style="color:#ae81ff">0&lt;/span> 4s$&lt;/code>&lt;/pre>&lt;/div>&lt;p>Check the pod logs to see if everything went on well as we had expected and there are no errors.&lt;/p>&lt;pre>&lt;code>$ kubectl logs socketxp-75cb4dd7c9-bhxfp......Login Succeeded.User [] Email [test-user@gmail.com].Connected.TCP tunnel [test-user-vbmyrruv] created.Access the tunnel using SocketXP agent in IoT Slave Mode.&lt;/code>&lt;/pre>&lt;p>You can now SSH into this pod from the SocketXP Portal&amp;rsquo;s IoT Device Access Page: &lt;a href="%22https://portal.socketxp.com/#/devices%22">https://portal.socketxp.com/#/devices&lt;/a>. Click the terminal icon there, to SSH into the &lt;code>socketxp-75cb4dd7c9-bhxfp&lt;/code> pod.&lt;/p>&lt;p>You&amp;rsquo;ll be prompted to enter the SSH username and the SSH password. This should match the ones you provided earlier when you configured the kubernetes secret &lt;code>socketxp-credentials&lt;/code> above.&lt;/p>&lt;p>Once you are logged in, you&amp;rsquo;ll be provided with the pod container&amp;rsquo;s root shell prompt, which would look like this.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">&lt;span style="color:#f92672">[&lt;/span>root@socketxp-75cb4dd7c9-bhxfp socketxp&lt;span style="color:#f92672">]&lt;/span>&lt;span style="color:#75715e">#&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>expresssocket/socketxp-kubectl&lt;/code> container is packaged with the &lt;code>kubectl&lt;/code> utility in it. So you could execute any kubectl command from this pod shell.&lt;/p>&lt;pre>&lt;code>[root@socketxp-75cb4dd7c9-bhxfp socketxp]# kubectl get podsNAME READY STATUS RESTARTS AGEsocketxp-75cb4dd7c9-bhxfp 1/1 Running 0 7m[root@socketxp-75cb4dd7c9-bhxfp socketxp]# &lt;/code>&lt;/pre>&lt;p>You could now connect to the shell of any other pod in the cluster, using the following kubectl command:&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl exec --stdin --tty &amp;lt;pod-name&amp;gt; -c &amp;lt;container-name&amp;gt;&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can skip the &lt;code>-c&lt;/code> option in the above command if the pod has only one container.&lt;/p>&lt;h3 id="security-best-practices">Security Best Practices&lt;/h3>&lt;p>Enterprises usually run Kubernetes clusters privately and not publicly. They don&amp;rsquo;t expose the clusters to the internet for security reasons. Ideally Kubernetes clusters should not be directly exposed to the internet by configuring the ClusterIP or the NodeIP with a public IP address. Only the application that needs to be exposed to the internet, should be exposed to the internet via a load-balancer configured with public IP. This approach reduces the attack surface for any potential attacks.&lt;/p>&lt;p>Moreover, you should use a VPN solution to gain remote SSH access to the private cluster and its resources.&lt;/p>&lt;p>But most VPN solutions in the market are heavy, clunky and cost more money. It is highly recommended to use a lightweight VPN soloution such as SocketXP to securely access your private Kubernetes cluster resources.&lt;/p></description></item><item><title>Kubernetes Worker Node Remote SSH Access</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-worker-node-remote-ssh-access/</link><pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-worker-node-remote-ssh-access/</guid><description>&lt;p>Kubernetes is a very popular and widely deployed container management and orchestration platform, preferred by devops engineers worldwide today.&lt;/p>&lt;p>Usually Kubernetes clusters and their worker nodes are not exposed to the public Internet but the apps running in them are.&lt;/p>&lt;p>SocketXP TLS VPN solution is a lightweight VPN that provides secure remote SSH access to private Kubernetes Clusters in your on-prem cloud or public cloud or multi-cloud or hybrid cloud.&lt;/p>&lt;p>::: tip Note:We at SocketXP are looking for beta customers to evaluate and provide feedback for our Kubernetes Remote Access Solution. Please feel free to reach out to us at: &lt;a href="mailto:support@socketxp.com">support@socketxp.com&lt;/a>:::&lt;/p>&lt;h2 id="overall-strategy----in-a-nutshell">Overall Strategy &amp;ndash; In a nutshell&lt;/h2>&lt;p>We&amp;rsquo;ll install SocketXP agent in your worker nodes and configure it to function as an SSH server. SocketXP agent will also establish a secure TLS VPN connection with the SocketXP Cloud Gateway. You could then, remote SSH into your Kubernetes worker nodes from the SocketXP Cloud Gateway Portal using your browser. No SSH client is required to SSH into your worker nodes.&lt;/p>&lt;p>Let&amp;rsquo;s get started!&lt;/p>&lt;h3 id="step-1--download-and-install">Step 1: Download and Install&lt;/h3>&lt;p>&lt;a href="https://www.socketxp.com/download/">Download and install&lt;/a> the SocketXP agent on your Kubernetes Worker Node.&lt;/p>&lt;h3 id="step-2-get-your-authentication-token">Step 2: Get your Authentication Token&lt;/h3>&lt;p>Sign up at &lt;a href="https://portal.socketxp.com">https://portal.socketxp.com&lt;/a> and get your authentication token.&lt;/p>&lt;p>&lt;img src="https://dev-to-uploads.s3.amazonaws.com/i/8h8dtakf5nl1c85kem9z.jpg" alt="SocketXP AuthToken">&lt;/p>&lt;p>Use the following command to authenticate you node with the SocketXP Cloud Gateway using the auth token.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ socketxp login &amp;lt;your-auth-token-goes-here&amp;gt;&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="step-3-create-socketxp-tls-vpn-tunnel-for-remote-ssh-access">Step 3: Create SocketXP TLS VPN Tunnel for Remote SSH Access&lt;/h3>&lt;p>Use the following command to create a secure and private TLS tunnel VPN connection to the SocketXP Cloud Gateway.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ socketxp connect tcp://localhost:22 --iot-device-id &lt;span style="color:#e6db74">&amp;#34;kube-worker-node-001&amp;#34;&lt;/span> --enable-ssh --ssh-username &lt;span style="color:#e6db74">&amp;#34;test-user&amp;#34;&lt;/span> --ssh-password &lt;span style="color:#e6db74">&amp;#34;password123&amp;#34;&lt;/span>TCP tunnel &lt;span style="color:#f92672">[&lt;/span>test-user-gmail-com-34445&lt;span style="color:#f92672">]&lt;/span> created.Access the tunnel using SocketXP agent in IoT Slave Mode&lt;/code>&lt;/pre>&lt;/div>&lt;p>Where TCP port 22 is the default port at which the SocketXP agent would listen for SSH connections from any SSH clients. The &amp;ldquo;&amp;ndash;iot-device-id&amp;rdquo; represents a unique identifier assigned to the Kubernetes worker node within your organization. It could be any string value but it must be unique for each of your worker node.&lt;/p>&lt;p>::: warning Security Info:SocketXP &lt;!-- raw HTML omitted -->does not&lt;!-- raw HTML omitted --> create any public TCP tunnel endpoints that can be connected and accessed by anyone in the internet using an SSH client. SocketXP TCP tunnel endpoints are not exposed to the internet and can be accessed only using the SocketXP agent (using the auth token of the user) or through the XTERM terminal in the SocketXP Portal page.&lt;/p>&lt;p>SocketXP also has the option to setup and use your private/public keys to remote SSH into your worker nodes.:::&lt;/p>&lt;p>You could now remote SSH into your Kubernetes worker node by clicking the terminal icon as shown in the screenshot below.&lt;/p>&lt;p>&lt;img src="https://dev-to-uploads.s3.amazonaws.com/i/bnlax2jo0tb5v0s5kta1.png" alt="SSH Kubernetes Worker Node">&lt;/p>&lt;p>Next, you&amp;rsquo;ll will be prompted to provide your SSH login and password.&lt;/p>&lt;p>Once your credentials are authenticated with your SSH server you&amp;rsquo;ll be logged into your device&amp;rsquo;s shell prompt.&lt;/p>&lt;p>The screen capture below shows the &amp;ldquo;htop&amp;rdquo; shell command output from an SSH session created using the XTERM window in the SocketXP Portal page.&lt;/p>&lt;p>&lt;img src="https://dev-to-uploads.s3.amazonaws.com/i/tcg50e43i1dbnleqyka1.jpg" alt="SSH Kubernetes Worker Node">&lt;/p>&lt;h2 id="configuring-socketxp-agent-to-run-in-slave-mode">Configuring SocketXP agent to run in slave mode&lt;/h2>&lt;p>This is an alternate method for SSH into your private worker node from a remote location using the SocketXP Remote SSH Access solution.&lt;/p>&lt;p>If you don&amp;rsquo;t want to access your worker node using a browser(via SocketXP Portal) and you want to access it using an SSH client (such as PuTTy) installed on your laptop or desktop, follow the instructions below.&lt;/p>&lt;p>First download and install the regular SocketXP agent software on your access device (such as a laptop running Windows or Mac OS). Next, configure the agent to run in slave mode using the command option &amp;ldquo;&amp;ndash;iot-slave&amp;rdquo; as shown in the example below. Also, specify the name of the private TCP tunnel you want to connect to, using the &lt;code>--tunnel-name&lt;/code> option.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ socketxp connect tcp://localhost:3000 --iot-slave --tunnel-name test-user-gmail-com-34445Listening &lt;span style="color:#66d9ef">for&lt;/span> TCP connections at:Local URL -&amp;gt; tcp://localhost:3000Accessing the IoT device from your laptop&lt;/code>&lt;/pre>&lt;/div>&lt;p>::: tip Why this is important?SocketXP IoT Agent when run in Slave Mode acts like a localproxy server. It proxies all connections to a user-specified local port (10111 in the example above) in your laptop/PC to the SocketXP Cloud Gateway using a secure SSL/TLS tunnel. Also the SocketXP Agent authenticates itself with the SocketXP Cloud Gateway using your auth token. This ensures that only legitimate, authenticated users are permitted to access your private worker nodes. SocketXP ensures Zero-Trust security on all connected devices.:::&lt;/p>&lt;p>Now you can SSH access your Kubernetes Worker Node using the above SocketXP local endpoint, as shown below.&lt;/p>&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ ssh -i ~/.ssh/test-user-private.key test-user@localhost -p &lt;span style="color:#ae81ff">3000&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>::: tip Tip:You can also use &lt;a href="https://www.putty.org/">PuTTY&lt;/a> SSH client to remote SSH into your device using the same parameters show above. Similarly, you can use PuTTY or &lt;a href="https://filezilla-project.org/">FileZilla&lt;/a> to perform SFTP actions such as file upload and file download to your private Kubernetes Worker Nodes.:::&lt;/p>&lt;p>::: tip Want to be a Beta customer?:We at SocketXP are looking for Beta customers to evaluate and provide feedback for our Kubernetes Remote Access Solution that includes Worker Node/Pod SSH access/Microservice Remote Access/Database Remote access. Please feel free to connect with us at: &lt;a href="support@socketxp.com">support@socketxp.com&lt;/a>:::&lt;/p></description></item><item><title>Autoupdate Kubernetes Deployment on GitHub Webhook</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/autoupdate-kubernetes-deployment-github-webhook/</link><pubDate>Sat, 25 Jul 2020 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/autoupdate-kubernetes-deployment-github-webhook/</guid><description>&lt;p>In this article, I&amp;rsquo;ll discuss how to autoupdate a Kubernetes workload or deployment on receiving a GitHub Webhook when a new version of the app is released.&lt;/p>&lt;h2 id="prerequisites">Prerequisites&lt;/h2>&lt;p>For understanding and following the instructions in this demo you need to have a basic understanding and some working knowledge of the following tools.&lt;/p>&lt;ul>&lt;li>GitHub&lt;/li>&lt;li>Docker&lt;/li>&lt;li>Kubernetes Cluster&lt;/li>&lt;li>Helm&lt;/li>&lt;li>SocketXP&lt;/li>&lt;/ul>&lt;h2 id="continuous-deployment-strategy">Continuous Deployment Strategy&lt;/h2>&lt;p>Our goal is to upgrade our demo app in a GitOps fashion, meaning we don&amp;rsquo;t want to upgrade whenever a new checkin happens into our app git repo but when a change or release is created in our Devops template or script git repo.&lt;/p>&lt;p>In this demo, I&amp;rsquo;ll show you how to create a separate git repo for the helm chart of our app. Whenever a new release is tagged on the master branch for this helm chart git repo we&amp;rsquo;ll upgrade the helm chart release in our Kubernetes Cluster, which in turn will upgrade our http-server app in our Kubernetes cluster.&lt;/p>&lt;p>&lt;img src="https://www.socketxp.com/wp-content/uploads/2020/07/Kubernetes-autoupdate-github-webhook-helm-chart.png" alt="Kubernetes-autoupdate-github-webhook-helm-chart">{.aligncenter .wp-image-1198 .size-full width=&amp;ldquo;1024&amp;rdquo; height=&amp;ldquo;768&amp;rdquo;}&lt;/p>&lt;h2 id="demo-app">Demo App&lt;/h2>&lt;p>We&amp;rsquo;ll use the same demo app we used in our last week&amp;rsquo;s demo. I&amp;rsquo;m displaying it again over here for convenience.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$cat http-server.jsvar http = require('http');var port = 8080var handleRequest = function(request, response) {console.log('Received HTTP request for URL: ' + request.url);response.writeHead(200);response.end('Hello World!, v1.0');};var server = http.createServer(handleRequest);server.listen(port);&lt;/code>&lt;/pre>&lt;p>Also the Dockerfile used to build our http-server app from our last week&amp;rsquo;s demo.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">FROM alpine:latestRUN apk update &amp;amp;&amp;amp; apk add nodejsRUN mkdir -p /usr/src/appCOPY ./http-server.js /usr/src/appWORKDIR /usr/src/appEXPOSE 8080CMD [&amp;quot;node&amp;quot;,&amp;quot;http-server.js&amp;quot;]&lt;/code>&lt;/pre>&lt;p>Refer to our &lt;a href="https://www.socketxp.com/webhookrelay/docker-compose-auto-update-github-webhook/">&lt;strong>last week&amp;rsquo;s demo article&lt;/strong>&lt;/a> on how to build a version 1.0 of the docker image and publish it to a DockerHub registry.&lt;/p>&lt;h2 id="kubernetes-cluster">Kubernetes Cluster&lt;/h2>&lt;p>Install Kubernetes Cluster or a Minikube Cluster following the instructions here:&lt;/p>&lt;ul>&lt;li>&lt;a href="http://www.ethernetresearch.com/geekzone/kubernetes-tutorial-how-to-install-kubernetes-on-ubuntu/">How to install Kubernetes on Ubuntu&lt;/a>&lt;/li>&lt;li>&lt;a href="http://www.ethernetresearch.com/kubernetes/kubernetes-how-to-install-minikube-in-a-vm/">How to install minikube in a VM&lt;/a>&lt;/li>&lt;/ul>&lt;p>Check the nodes and the pods available.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$kubectl get nodesNAME   STATUS ROLES  AGE  VERSIONminikube Ready  master 3h45m v1.18.3&lt;/code>&lt;/pre>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$kubectl get podsNo resources found in default namespace.$&lt;/code>&lt;/pre>&lt;h2 id="helm">Helm&lt;/h2>&lt;p>I have already created an helm chart for this demo and is available at: &lt;strong>&lt;a href="https://github.com/socketxp-com/kubernetes-webhook-autoupdate-helm-chart">https://github.com/socketxp-com/kubernetes-webhook-autoupdate-helm-chart&lt;/a>&lt;/strong>&lt;/p>&lt;p>But if you want to create your own helm chart follow the instructions below to install helm and create a helm chart.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$ brew install helm&lt;/code>&lt;/pre>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$ helm create http-server-chart$ lshttp-server-chart$ cd http-server-chart$ lsChart.yaml charts templates values.yaml$&lt;/code>&lt;/pre>&lt;p>Now edit the values.yaml file and update the repository and tag fields to your app&amp;rsquo;s docker image name and tag. Also update templates such as deployment.yaml or service.yaml under the templates folder, if required.&lt;/p>&lt;p>Commit the changes to your chart into a git repository, so that the changes/versions can be tracked. Also this will help in upgrading the corresponding workloads in a Kubernetes cluster, in a GitOps fashion.&lt;/p>&lt;h2 id="run-socketxp-on-the-master-node">Run SocketXP on the master node&lt;/h2>&lt;p>Download and install SocketXP agent on the master node of your Kubernetes cluster&amp;rsquo;s master node. You can follow the &lt;a href="https://www.socketxp.com/download/">&lt;strong>SocketXP download instructions here&lt;/strong>&lt;/a>.&lt;/p>&lt;p>Next jump into the root directory of the git cloned chart repo and execute the below command. The repo already has the files &amp;ldquo;update-kube.sh&amp;rdquo; and &amp;ldquo;filter-rules.sh&amp;rdquo; from the last week&amp;rsquo;s demo. The only change is in the update-kube.sh file to execute a &amp;ldquo;helm upgrade&amp;rdquo; command instead of &amp;ldquo;docker-compose up -d&amp;rdquo; command.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$ cat update-kube.sh#!/bin/bashgit pullhelm upgrade http-server http-server-chart&lt;/code>&lt;/pre>&lt;p>Execute the SocketXP relay command.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$ socketxp relay http://localhost:8443 --exec ./update-kube.sh --filter filter-rules.jsonconnected.Public URL -&amp;gt; https://webhook.socketxp.com/ganeshvelrajan-rmzlayq9&lt;/code>&lt;/pre>&lt;p>Now pick up this SocketXP Public Webhook URL and go to your GitHub Webhook Settings page and paste this URL in the Public Webhook URL text box. Make sure you set the content type of GitHub webhook to &amp;ldquo;application/json&amp;rdquo;. Also set the secret with your own secret. This secret will be used to validate the sender when a GitHub webhook is received at your Kubernetes Cluster master node.&lt;/p>&lt;p>&lt;img src="https://www.socketxp.com/wp-content/uploads/2020/07/Kubernetes-Github-Webhook-.png" alt="Kubernetes Github Webhook-socketxp">{.aligncenter .wp-image-1201 .size-full width=&amp;ldquo;2484&amp;rdquo; height=&amp;ldquo;1802&amp;rdquo;}&lt;/p>&lt;h2 id="install-helm-chart">Install Helm Chart&lt;/h2>&lt;p>Install the helm chart manually using the following command and bring up the http-server app in the kubernetes cluster. We need to do this only once when we bring up the app for the very first time. Thereafter, we&amp;rsquo;ll automate the upgrade process using SocketXP.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$lsLICENSE filter-rules.json update-kube.shREADME.md http-server-chart$helm install http-server http-server-chart/NAME: http-serverLAST DEPLOYED: Fri Jul 10 11:59:39 2020NAMESPACE: defaultSTATUS: deployedREVISION: 1NOTES:1. Get the application URL by running these commands:export NODE_PORT=$(kubectl get --namespace default -o jsonpath=&amp;quot;{.spec.ports[0].nodePort}&amp;quot; services http-server-http-server-chart)export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=&amp;quot;{.items[0].status.addresses[0].address}&amp;quot;)echo http://$NODE_IP:$NODE_PORT&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s follow the instructions in the above output and copy paste the above commands.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$export NODE_PORT=$(kubectl get --namespace default -o jsonpath=&amp;quot;{.spec.ports[0].nodePort}&amp;quot; services http-server-http-server-chart)export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=&amp;quot;{.items[0].status.addresses[0].address}&amp;quot;)$ export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=&amp;quot;{.items[0].status.addresses[0].address}&amp;quot;)$echo http://$NODE_IP:$NODE_PORThttp://192.168.99.100:32194$&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s try to access our http-server application at the above mentioned URL.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$curl http://192.168.99.100:32194Hello World!, v1.0&lt;/code>&lt;/pre>&lt;p>Great. Our app is running version 1.0 as expected.&lt;/p>&lt;p>Next verify the helm release version currently deployed.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$helm lsNAME    NAMESPACE REVISION UPDATED               STATUS CHART         APP VERSIONhttp-server default 1    2020-07-10 11:59:39.651311 +0530 IST deployed http-server-chart-0.1.0 1.16.0&lt;/code>&lt;/pre>&lt;h2 id="begin-demo---upgrade-the-app-and-the-chart">Begin Demo - Upgrade the app and the chart&lt;/h2>&lt;p>Now let&amp;rsquo;s edit our http-server app like we did in the last week&amp;rsquo;s demo and push a docker container image version 2.0 to DockerHub. Please refer to our last week&amp;rsquo;s demo article if you forgot the instruction for doing so.&lt;/p>&lt;p>Next let&amp;rsquo;s edit the helm-chart and upgrade the image version and the chart version to 2.0 and 0.2.0 respectively.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$vim http-server-chart/values.yaml# Default values for http-server-chart.# This is a YAML-formatted file.# Declare variables to be passed into your templates.replicaCount: 1image:repository: gvelrajan/http-serverpullPolicy: IfNotPresent# Overrides the image tag whose default is the chart appVersion.tag: &amp;quot;v2.0&amp;quot;......&lt;/code>&lt;/pre>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">...# This is the chart version. This version number should be incremented each time you make changes# to the chart and its templates, including the app version.# Versions are expected to follow Semantic Versioning (https://semver.org/)version: 0.2.0......&lt;/code>&lt;/pre>&lt;p>Commit the changes and push to GitHub.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$git add .$git commit -m &amp;quot;image version and chart version updated&amp;quot;[master 1765b2e] image version and chart version updated4 files changed, 35 insertions(+), 2 deletions(-)create mode 100644 filter-rules.jsoncreate mode 100755 update-kube.sh$git pushCounting objects: 7, done.Delta compression using up to 8 threads.Compressing objects: 100% (7/7), done.Writing objects: 100% (7/7), 839 bytes | 839.00 KiB/s, done.Total 7 (delta 3), reused 0 (delta 0)remote: Resolving deltas: 100% (3/3), completed with 3 local objects.To https://github.com/socketxp-com/kubernetes-webhook-autoupdate-helm-chart.git2f12153..1765b2e master -&amp;gt; master&lt;/code>&lt;/pre>&lt;p>As soon as we do git push for the chart into its online repo, GitHub will trigger a webhook notification for the git push event. SocketXP will receive the webhook, validate the signature and try to match it with the fiter rules. Since this is not a release webhook, SocketXP will throw a mismatch error and not further process this webhook.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">Connected.Public URL -&amp;gt; https://webhook.socketxp.com/ganeshvelrajan-rmzlayq9Webhook received:POST / HTTP/1.1Host: webhook.socketxp.comAccept: */*Accept-Encoding: gzipContent-Length: 9397Content-Type: application/jsonUser-Agent: GitHub-Hookshot/f2f2346X-Github-Delivery: 4aba8354-c279-11ea-980e-6cd6cfda12a3X-Github-Event: pushX-Hub-Signature: sha1=14e1f0a3d368d9b5dadd7d41cd575af2f45065ce{&amp;quot;ref&amp;quot;:&amp;quot;refs/heads/master&amp;quot;,&amp;quot;before&amp;quot;:&amp;quot;2f12153ef944b939bc81712e1d047d281d2deabe&amp;quot;,&amp;quot;after&amp;quot;:&amp;quot;1765b2ecd58ad3c6d3fec15c7b3198063b543405&amp;quot;,&amp;quot;repository&amp;quot;:{&amp;quot;id&amp;quot;:278398971,&amp;quot;node_id&amp;quot;:&amp;quot;MDEwOlJlcG9zaXRvcnkyNzgzOTg5NzE=&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;kubernetes-webhook-autoupdate-helm-chart&amp;quot;,...]}}#Rule: Webhook signature matched.#Rule: ref in payload didn't matchWebhook received didn't match with the filter rules.Command not executed.&lt;/code>&lt;/pre>&lt;h2 id="release-version-020-of-the-chart">Release version 0.2.0 of the chart&lt;/h2>&lt;p>So this proves that our http-server app will not be upgraded on receiving just any ad-hoc wehbook notifications from GitHub. It gets upgraded only on receiving an official release webhook for the app&amp;rsquo;s helm chart.&lt;/p>&lt;p>Now, let&amp;rsquo;s go ahead and create an official release version 0.2.0 of the helm chart for our app. We do this in the GitHub release page for our app&amp;rsquo;s helm-chart repo.&lt;/p>&lt;p>[ Note: But before doing this make sure you update the print string in our http-server app to print &amp;ldquo;v2.0&amp;rdquo;. Also build a new Docker container image for the upgraded app and tag it as v2.0. Refer to our last week&amp;rsquo;d demo if you need to know how to perform these actions]&lt;/p>&lt;p>&lt;img src="https://www.socketxp.com/wp-content/uploads/2020/07/Helm-Chart-Release-Version-0.2.0.png" alt="Kubernetes Continuous Deployment Helm Chart GitHub Webhook">{.aligncenter .wp-image-1202 .size-full width=&amp;ldquo;2106&amp;rdquo; height=&amp;ldquo;1184&amp;rdquo;}&lt;/p>&lt;p>SocketXP will receive the release webhook from GitHub. It will process the webhook, validate the signature and match the ref field in the webhook payload with the filter rules.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">Webhook received:POST / HTTP/1.1Host: webhook.socketxp.comAccept: */*Accept-Encoding: gzipContent-Length: 8725Content-Type: application/jsonUser-Agent: GitHub-Hookshot/f2f2346X-Github-Delivery: b08b5ec8-c27a-11ea-8774-72206af92c06X-Github-Event: pushX-Hub-Signature: sha1=d6fefb9919e8b322226f6dcd485bd2f2591e80c8{&amp;quot;ref&amp;quot;:&amp;quot;refs/tags/v0.2.0&amp;quot;,&amp;quot;before&amp;quot;:&amp;quot;0000000000000000000000000000000000000000&amp;quot;,&amp;quot;after&amp;quot;:&amp;quot;1765b2ecd58ad3c6d3fec15c7b3198063b543405&amp;quot;,&amp;quot;repository&amp;quot;:{&amp;quot;id&amp;quot;:278398971,&amp;quot;node_id&amp;quot;:&amp;quot;MDEwOlJlcG9zaXRvcnkyNzgzOTg5NzE=&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;kubernetes-webhook-autoupdate-helm-chart&amp;quot;,&amp;quot;full_name&amp;quot;:&amp;quot;socketxp-com/kubernetes-webhook-autoupdate-helm-chart&amp;quot;,......]}}#Rule: Webhook signature matched.#Rule: ref in payload matched.All rules matched.Executing command: [./update-kube.sh]From https://github.com/socketxp-com/kubernetes-webhook-autoupdate-helm-chart* [new tag]    v0.2.0  -&amp;gt; v0.2.0Already up to date.Current branch master is up to date.Release &amp;quot;http-server&amp;quot; has been upgraded. Happy Helming!NAME: http-serverLAST DEPLOYED: Fri Jul 10 12:42:09 2020NAMESPACE: defaultSTATUS: deployedREVISION: 2NOTES:1. Get the application URL by running these commands:export NODE_PORT=$(kubectl get --namespace default -o jsonpath=&amp;quot;{.spec.ports[0].nodePort}&amp;quot; services http-server-http-server-chart)export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=&amp;quot;{.items[0].status.addresses[0].address}&amp;quot;)echo http://$NODE_IP:$NODE_PORT&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s follow the instructions above output and copy paste the above commands in a separate terminal.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$ export NODE_PORT=$(kubectl get --namespace default -o jsonpath=&amp;quot;{.spec.ports[0].nodePort}&amp;quot; services http-server-http-server-chart)$ export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=&amp;quot;{.items[0].status.addresses[0].address}&amp;quot;)$ echo http://$NODE_IP:$NODE_PORThttp://192.168.99.100:32194&lt;/code>&lt;/pre>&lt;p>Now, let&amp;rsquo;s access the URL using curl.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$curl http://192.168.99.100:32194Hello World!, v2.0&lt;/code>&lt;/pre>&lt;p>Also confirm the kubernetes workload(pod) has the correct versions.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$kubectl describe podsName:    http-server-http-server-chart-5947946f5f-vb5cnNamespace:  defaultPriority:  0Node:    minikube/192.168.99.100Start Time: Fri, 10 Jul 2020 02:30:00 +0530Labels:   app.kubernetes.io/instance=http-serverapp.kubernetes.io/name=http-server-chartpod-template-hash=5947946f5fAnnotations:Status:   RunningIP:     172.17.0.5IPs:IP:     172.17.0.5Controlled By: ReplicaSet/http-server-http-server-chart-5947946f5fContainers:http-server-chart:Container ID: docker://7deb29b8786bc474374be82b27c33858cad8ec4b86754c29ad6467bf41f4ecc9Image:     gvelrajan/http-server:v2.0Image ID:   docker-pullable://gvelrajan/http-server@sha256:2a9ee82f76758758c77659260091380b20e3b3d095efdc6480d66e47c5842476Port:     8080/TCPHost Port:   0/TCPState:     RunningStarted:   Fri, 10 Jul 2020 02:30:03 +0530Ready:     TrueRestart Count: 0Liveness:   http-get http://:http/ delay=0s timeout=1s period=10s #success=1 #failure=3Readiness:   http-get http://:http/ delay=0s timeout=1s period=10s #success=1 #failure=3Environment:Mounts:/var/run/secrets/kubernetes.io/serviceaccount from http-server-http-server-chart-token-rmppc (ro)Conditions:Type       StatusInitialized   TrueReady      TrueContainersReady TruePodScheduled   TrueVolumes:http-server-http-server-chart-token-rmppc:Type:    Secret (a volume populated by a Secret)SecretName: http-server-http-server-chart-token-rmppcOptional:  falseQoS Class:   BestEffortNode-Selectors:Tolerations:  node.kubernetes.io/not-ready:NoExecute for 300snode.kubernetes.io/unreachable:NoExecute for 300sEvents:Type  Reason  Age From       Message----  ------  ---- ----       -------Normal Scheduled 10h default-scheduler Successfully assigned default/http-server-http-server-chart-5947946f5f-vb5cn to minikubeNormal Pulled  10h kubelet, minikube Container image &amp;quot;gvelrajan/http-server:v2.0&amp;quot; already present on machineNormal Created  10h kubelet, minikube Created container http-server-chartNormal Started  10h kubelet, minikube Started container http-server-chart$&lt;/code>&lt;/pre>&lt;p>Also let&amp;rsquo;s check the helm release version currently deployed.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$helm lsNAME    NAMESPACE REVISION UPDATED               STATUS CHART         APP VERSIONhttp-server default 2    2020-07-10 12:42:09.288038 +0530 IST deployed http-server-chart-0.2.0 1.16.0$&lt;/code>&lt;/pre>&lt;p>Everything confirms that we are have successfully upgraded our app to version 2.0 by upgrading the helm chart release version to 0.2.0&lt;/p>&lt;h2 id="rolling-back-to-the-previous-working-version">Rolling back to the previous working version&lt;/h2>&lt;p>Suppose, the upgrade to version 2.0 of the app introduced some severe bugs or problems that is unacceptable by any user, we would want to quickly go back to the previous working version, so that customers are not affected by our upgrade.&lt;/p>&lt;p>Helm makes the process of rollback very simple. Let&amp;rsquo;s do it. We&amp;rsquo;ll rollback to revision #1 of the helm chart from revision #2.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$helm rollback http-server 1Rollback was a success! Happy Helming!$helm lsNAME    NAMESPACE REVISION UPDATED               STATUS CHART         APP VERSIONhttp-server default 3    2020-07-10 12:53:04.755063 +0530 IST deployed http-server-chart-0.1.0 1.16.0&lt;/code>&lt;/pre>&lt;p>Curl the app&amp;rsquo;s web URL and verify if the rollback is successful.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$curl http://192.168.99.100:32194Hello World!, v1.0&lt;/code>&lt;/pre>&lt;p>Verify the pods running in the Kubernetes cluster have the right version. Notice that the pod running the old version of our http-server app is terminating and the pod running the new version is up and running.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$kubectl describe podsName:           http-server-http-server-chart-5947946f5f-vb5cnNamespace:        defaultPriority:         0Node:           minikube/192.168.99.100Start Time:        Fri, 10 Jul 2020 02:30:00 +0530Labels:          app.kubernetes.io/instance=http-serverapp.kubernetes.io/name=http-server-chartpod-template-hash=5947946f5fAnnotations:Status:          Terminating (lasts 10h)Termination Grace Period: 30sIP:            172.17.0.5IPs:IP:     172.17.0.5Controlled By: ReplicaSet/http-server-http-server-chart-5947946f5fContainers:http-server-chart:Container ID: docker://7deb29b8786bc474374be82b27c33858cad8ec4b86754c29ad6467bf41f4ecc9Image:     gvelrajan/http-server:v2.0Image ID:   docker-pullable://gvelrajan/http-server@sha256:2a9ee82f76758758c77659260091380b20e3b3d095efdc6480d66e47c5842476Port:     8080/TCPHost Port:   0/TCPState:     RunningStarted:   Fri, 10 Jul 2020 02:30:03 +0530Ready:     TrueRestart Count: 0Liveness:   http-get http://:http/ delay=0s timeout=1s period=10s #success=1 #failure=3Readiness:   http-get http://:http/ delay=0s timeout=1s period=10s #success=1 #failure=3Environment:Mounts:/var/run/secrets/kubernetes.io/serviceaccount from http-server-http-server-chart-token-rmppc (ro)Conditions:Type       StatusInitialized   TrueReady      TrueContainersReady TruePodScheduled   TrueVolumes:http-server-http-server-chart-token-rmppc:Type:    Secret (a volume populated by a Secret)SecretName: http-server-http-server-chart-token-rmppcOptional:  falseQoS Class:   BestEffortNode-Selectors:Tolerations:  node.kubernetes.io/not-ready:NoExecute for 300snode.kubernetes.io/unreachable:NoExecute for 300sEvents:Type  Reason  Age From       Message----  ------  ---- ----       -------Normal Scheduled 10h default-scheduler Successfully assigned default/http-server-http-server-chart-5947946f5f-vb5cn to minikubeNormal Pulled  10h kubelet, minikube Container image &amp;quot;gvelrajan/http-server:v2.0&amp;quot; already present on machineNormal Created  10h kubelet, minikube Created container http-server-chartNormal Started  10h kubelet, minikube Started container http-server-chartNormal Killing  10h kubelet, minikube Stopping container http-server-chartName:    http-server-http-server-chart-bdd75ff4-7jmbxNamespace:  defaultPriority:  0Node:    minikube/192.168.99.100Start Time: Fri, 10 Jul 2020 02:40:56 +0530Labels:   app.kubernetes.io/instance=http-serverapp.kubernetes.io/name=http-server-chartpod-template-hash=bdd75ff4Annotations:Status:   RunningIP:     172.17.0.4IPs:IP:     172.17.0.4Controlled By: ReplicaSet/http-server-http-server-chart-bdd75ff4Containers:http-server-chart:Container ID: docker://239aadd09eb54185b3aa559d5ba3a89f2cb99a30c5b340270b834f9e25de24cbImage:     gvelrajan/http-server:v1.0Image ID:   docker-pullable://gvelrajan/http-server@sha256:07e70e778e1002519064503ece0d334291f4fbb7a7196b894f605ff5e3076e18Port:     8080/TCPHost Port:   0/TCPState:     RunningStarted:   Fri, 10 Jul 2020 02:40:57 +0530Ready:     TrueRestart Count: 0Liveness:   http-get http://:http/ delay=0s timeout=1s period=10s #success=1 #failure=3Readiness:   http-get http://:http/ delay=0s timeout=1s period=10s #success=1 #failure=3Environment:Mounts:/var/run/secrets/kubernetes.io/serviceaccount from http-server-http-server-chart-token-rmppc (ro)Conditions:Type       StatusInitialized   TrueReady      TrueContainersReady TruePodScheduled   TrueVolumes:http-server-http-server-chart-token-rmppc:Type:    Secret (a volume populated by a Secret)SecretName: http-server-http-server-chart-token-rmppcOptional:  falseQoS Class:   BestEffortNode-Selectors:Tolerations:  node.kubernetes.io/not-ready:NoExecute for 300snode.kubernetes.io/unreachable:NoExecute for 300sEvents:Type  Reason  Age From       Message----  ------  ---- ----       -------Normal Scheduled 10h default-scheduler Successfully assigned default/http-server-http-server-chart-bdd75ff4-7jmbx to minikubeNormal Pulled  10h kubelet, minikube Container image &amp;quot;gvelrajan/http-server:v1.0&amp;quot; already present on machineNormal Created  10h kubelet, minikube Created container http-server-chartNormal Started  10h kubelet, minikube Started container http-server-chart$&lt;/code>&lt;/pre>&lt;p>That&amp;rsquo;s all folks!! I hope you were able to follow the instructions and auto deploy your app in Kubernetes using SocketXP and Helm chart in a GitOps fashion.&lt;/p>&lt;h2 id="conclusion">Conclusion&lt;/h2>&lt;p>SocketXP and Helm are two great tools to setup a full-automated Continuous Deployment(CD) pipeline for your app running in your Kubernetes Cluster. Though you might want to expose your app ( a front-end microservice) running in your Kubernetes Cluster to the internet, you wouldn&amp;rsquo;t want to and shouldn&amp;rsquo;t expose your Kubernetes Cluster itself to the internet.&lt;/p>&lt;p>When you are not exposing the Kubernetes Cluster to the internet, it may be challenging to receive webhook notifications from GitHub, Gitlab, DockerHub, GCR and other online version control and artifact repositories. SocketXP solves this problem by quickly creating a secure webhook relay tunnel between the online registry and your Kubernetes Cluster, so that webhooks could reach your private Kubernetes Cluster.&lt;/p>&lt;p>SocketXP is a freemium online webhook relay service, which has free and paid plans for developers, businesses and enterprise customers. &lt;a href="https://portal.socketxp.com">&lt;strong>Sign up for your free SocketXP account here&lt;/strong>&lt;/a>.&lt;/p></description></item><item><title>Docker Compose Auto Update on GitHub Webhook</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/docker-compose-auto-update-on-github-webhook/</link><pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/docker-compose-auto-update-on-github-webhook/</guid><description>&lt;p>In the last week&amp;rsquo;s demo, we discussed how to setup a &lt;a href="https://www.socketxp.com/webhookrelay/automatically-deploy-github-local-server-git-push/">&lt;strong>Continuous Deployment(CD) pipeline for a native NodeJS application&lt;/strong>&lt;/a>.&lt;/p>&lt;p>In this article, I&amp;rsquo;ll show you how to build a GitOps style CD pipeline that keeps a Docker Compose deployment in sync with a docker-compose.yaml hosted on a git repository.&lt;/p>&lt;h2 id="whats-new-in-this-demo">What&amp;rsquo;s new in this demo?&lt;/h2>&lt;p>We have added some cool new features to SocketXP agent, such as:&lt;/p>&lt;ul>&lt;li>ability to filter incoming webhooks on user specified filter rules&lt;/li>&lt;li>ability to execute a command or a script file when an incoming webhook matches with the user specified rules&lt;/li>&lt;/ul>&lt;p>Secondly, and more importantly, we&amp;rsquo;ll be running our NodeJS web app inside a Docker container and bring it up using a Docker Compose YAML file.&lt;/p>&lt;h2 id="prerequisites">Prerequisites&lt;/h2>&lt;p>Here are the prerequisites for this demo.&lt;/p>&lt;ul>&lt;li>GitHub&lt;/li>&lt;li>SocketXP Account&lt;/li>&lt;li>SocketXP Agent Installation&lt;/li>&lt;li>Docker Engine, Docker Compose and DockerHub Account&lt;/li>&lt;/ul>&lt;p>&lt;img src="https://www.socketxp.com/wp-content/uploads/2020/07/Docker-compose-auto-update-github-webhook.png" alt="Docker-compose-auto-update-github-webhook">{.aligncenter .wp-image-1148 .size-full width=&amp;ldquo;1024&amp;rdquo; height=&amp;ldquo;768&amp;rdquo;}&lt;/p>&lt;p>Repository with scripts that I used for this article can be found here: &lt;a href="https://github.com/socketxp-com/docker-compose-autoupdate-on-github-webhook">https://github.com/socketxp-com/docker-compose-autoupdate-on-github-webhook&lt;/a>.&lt;/p>&lt;h2 id="demo-app">Demo App&lt;/h2>&lt;p>We&amp;rsquo;ll use the same nodejs http server that we used in the last week&amp;rsquo;s demo.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$cat http-server.js var http = require('http');var port = 8080var handleRequest = function(request, response) { console.log('Received HTTP request for URL: ' + request.url); response.writeHead(200); response.end('Hello World!, v1.0');};var server = http.createServer(handleRequest);server.listen(port); &lt;/code>&lt;/pre>&lt;h2 id="dockerfile">Dockerfile&lt;/h2>&lt;p>Here is the Dockerfile we&amp;rsquo;ll use to package the above nodejs app into a Docker container.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">FROM alpine:latestRUN apk update &amp;amp;&amp;amp; apk add nodejsRUN mkdir -p /usr/src/appCOPY ./http-server.js /usr/src/appWORKDIR /usr/src/appEXPOSE 8080 CMD [&amp;quot;node&amp;quot;,&amp;quot;http-server.js&amp;quot;]&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s build the Docker image using the following Docker command.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$docker build -t gvelrajan/http-server:v1.0 .Sending build context to Docker daemon 56.83kBStep 1/7 : FROM alpine:latest ---&amp;gt; f70734b6a266Step 2/7 : RUN apk update &amp;amp;&amp;amp; apk add nodejs ---&amp;gt; Using cache...Successfully built b6b06a819b63Successfully tagged gvelrajan/http-server:v1.0$&lt;/code>&lt;/pre>&lt;p>Next, let&amp;rsquo;s push this Docker image to DockerHub image registry, so that it can be downloaded by the production team to deploy it.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$docker push gvelrajan/http-server:v1.0The push refers to repository [docker.io/gvelrajan/http-server]3b8d0f6b058e: Layer already exists 947da554eff8: Layer already exists cc403a71dcfb: Pushed 3e207b409db3: Layer already exists v1.0: digest: sha256:07e70e778e1002519064503ece0d334291f4fbb7a7196b894f605ff5e3076e18 size: 1154&lt;/code>&lt;/pre>&lt;h2 id="docker-compose">Docker Compose&lt;/h2>&lt;p>Docker Compose is a very helpful tool to bring up various services of an application as Docker containers using a simple single command. In our demo example we have just a single service - the http web service, packaged as a Docker container.&lt;/p>&lt;p>Here is the docker-compose yaml file we&amp;rsquo;ll use to bring up our http-server docker container in production.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$cat docker-compose.yml version: '3'services: web: image: &amp;quot;gvelrajan/http-server:v1.0&amp;quot; ports: - &amp;quot;8080:8080&amp;quot;$&lt;/code>&lt;/pre>&lt;p>Now, bring up the app container by executing the following command.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$docker-compose up -dCreating network &amp;quot;docker-compose-autoupdate-on-github-webhook_default&amp;quot; with the default driverCreating docker-compose-autoupdate-on-github-webhook_web_1_fd2bb2cedec9 ... done&lt;/code>&lt;/pre>&lt;p>Check if the docker container is up and running, and listening on the correct port(8080).&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES899ab99abedf gvelrajan/http-server:v1.0 &amp;quot;node http-server.js&amp;quot; 2 seconds ago Up 1 second 0.0.0.0:8080-&amp;gt;8080/tcp docker-compose-autoupdate-on-github-webhook_web_1_64dc034e55ac$&lt;/code>&lt;/pre>&lt;h2 id="curl-the-web-server">Curl the web server&lt;/h2>&lt;p>Verify that our web app is running and serving content on port 8080.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$curl http://localhost:8080Hello World!, v1.0&lt;/code>&lt;/pre>&lt;p>Our initial deployment is very successful. But this was a manual deployment. Every time a newer version of the app is released we have to manually download and deploy the application. This is not what we wanted. Our goal for this demo is to build a Continuous Deployment(CD) pipeline for our nodejs based web service running as a docker container.&lt;/p>&lt;p>Let&amp;rsquo;s begin by setting up various stages in the CD pipeline, like we did in the last week&amp;rsquo;s demo.&lt;/p>&lt;h2 id="auto-deployment-strategy">Auto deployment strategy&lt;/h2>&lt;p>Our auto deployment strategy has changed and is slightly different from the one used in the last week&amp;rsquo;s native app demo. Here is the auto deployment strategy for this demo:&lt;/p>&lt;p>When a new version of our app is available, we&amp;rsquo;ll update the docker-compose.yml file in the GitHub and release a new version of our app in the GitHub. GitHub will trigger a webhook for this release event. We&amp;rsquo;ll listen for this webhook event and automatically do a git pull on the docker-compose.yml file. We&amp;rsquo;ll use the new docker-compose.yml to re-spin our nodejs web service docker container in production.&lt;/p>&lt;p>&lt;strong>Note:&lt;/strong> GitHub will trigger webhooks for all sorts of push events. We don&amp;rsquo;t want to react to all those webhooks and auto deploy our containers on every GitHub push. We want to auto deploy only when a new release is made.&lt;/p>&lt;h2 id="socketxp-webhook-relay-service">SocketXP Webhook Relay Service&lt;/h2>&lt;p>We&amp;rsquo;ll use &lt;strong>SocketXP Webhook Relay Service&lt;/strong> to relay webhooks from GitHub to our local production server (where the nodejs app is running as a docker container). SocketXP can also filter GitHub webhooks based on user specified rules in JSON format. We&amp;rsquo;ll discuss more about it in the next section. Additionally, SocketXP can execute a script or a command, if an incoming webhook matches with the user specified rules.&lt;/p>&lt;p>Before creating webhook rules, it is better to study the webhook in detail and pick up a few fields in the webhook payload to match on. For this we need to make GitHub trigger a release webhook and capture the webhook and study it.&lt;/p>&lt;h2 id="webhook-capture-and-study">Webhook Capture and Study&lt;/h2>&lt;p>First, let&amp;rsquo;s setup the SocketXP agent to relay any webhooks from the internet to our local web service.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$ socketxp relay http://localhost:8080Connected.Public URL -&amp;gt; https://webhook.socketxp.com/ganeshvelrajan-1awy5t0h&lt;/code>&lt;/pre>&lt;p>Pick up the above SocketXP Public URL and head to your GitHub repository. Setup the webhook settings for your GitHub project as shown below. Setup your secret and copy paste the SocketXP Public URL in the webhook URL text box. Also set the content-type to &amp;ldquo;application/json&amp;rdquo; in the drop-down box. Save the changes.&lt;/p>&lt;p>&lt;img src="https://www.socketxp.com/wp-content/uploads/2020/07/docker-compose-auto-deploy-github-webhook.png" alt="docker-compose-auto-deploy-github-webhook">{.aligncenter .wp-image-1162 .size-full width=&amp;ldquo;1024&amp;rdquo; height=&amp;ldquo;709&amp;rdquo;}&lt;/p>&lt;h2 id="triggering-github-release-webhook">Triggering GitHub Release Webhook&lt;/h2>&lt;p>Next, create a new release for your GitHub project in the release settings page and create a new tag for your release, as shown below.&lt;/p>&lt;p>&lt;img src="https://www.socketxp.com/wp-content/uploads/2020/07/docker-compose-auto-update-github-webhook-release.png" alt="docker-compose-auto-update-github-webhook-release-v1.0">{.aligncenter .wp-image-1164 .size-full width=&amp;ldquo;1089&amp;rdquo; height=&amp;ldquo;539&amp;rdquo;}&lt;/p>&lt;p>GitHub will trigger a webhook for this release event in your project. We can view details of the triggered webhook either in the GitHub Webhook Settings page or in the SocketXP Portal Webhook Logs page.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">Request URL: https://webhook.socketxp.com/ganeshvelrajan-1awy5t0hRequest method: POSTAccept: */*content-type: application/jsonUser-Agent: GitHub-Hookshot/e509757X-GitHub-Delivery: ca631384-bd0e-11ea-8ba6-ea05e17356b6X-GitHub-Event: pushX-Hub-Signature: sha1=c902f69547901dc0658ba54eaee5cde6c0a5ef00{ &amp;quot;ref&amp;quot;: &amp;quot;refs/tags/v1.0&amp;quot;, &amp;quot;before&amp;quot;: &amp;quot;0000000000000000000000000000000000000000&amp;quot;, &amp;quot;after&amp;quot;: &amp;quot;b5595d43b2c681257e0bae638f9de814df968901&amp;quot;, &amp;quot;repository&amp;quot;: { &amp;quot;id&amp;quot;: 276817731, &amp;quot;node_id&amp;quot;: &amp;quot;MDEwOlJlcG9zaXRvcnkyNzY4MTc3MzE=&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;docker-compose-autoupdate-on-github-webhook&amp;quot;, &amp;quot;full_name&amp;quot;: &amp;quot;socketxp-com/docker-compose-autoupdate-on-github-webhook&amp;quot;, &amp;quot;private&amp;quot;: false, ... ...}&lt;/code>&lt;/pre>&lt;p>In the above webhook capture, we see the signature field in the header that encodes the secret we set in the GitHub webhook settings page. Verifying this signature using the secrete we provided, authenticates the validity of the sender.&lt;/p>&lt;p>Secondly, in the above webhook capture, the payload contains the release tags (&amp;ldquo;refs/tags/v1.0&amp;rdquo;) in the &amp;ldquo;ref&amp;rdquo; field.&lt;/p>&lt;p>Let&amp;rsquo;s try to filter our incoming webhook based on these two fields in the webhook.&lt;/p>&lt;h2 id="socketxp-webhook-filter-rules-json-file">SocketXP Webhook Filter-Rules JSON File&lt;/h2>&lt;p>Here is how the SocketXP filter-rules JSON file should look like to match the two fields we picked up in the webhook above.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$cat filter-rules.json { &amp;quot;and&amp;quot;: [ { &amp;quot;match&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;payload-hash-sha1&amp;quot;, &amp;quot;secret&amp;quot;: &amp;quot;asdfasdf23421dsaf&amp;quot;, &amp;quot;parameter&amp;quot;: { &amp;quot;source&amp;quot;: &amp;quot;header&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;X-Hub-Signature&amp;quot; } } }, { &amp;quot;match&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;value&amp;quot;, &amp;quot;value&amp;quot;: &amp;quot;refs/tags&amp;quot;, &amp;quot;parameter&amp;quot;: { &amp;quot;source&amp;quot;: &amp;quot;payload&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;ref&amp;quot; } } } ]}&lt;/code>&lt;/pre>&lt;p>Make sure you update the &amp;ldquo;secret&amp;rdquo; field in the filter-rules file above with your GitHub webhook secret.&lt;/p>&lt;h2 id="socketxp-exec-command-option">SocketXP Exec Command Option&lt;/h2>&lt;p>Here is the script we&amp;rsquo;d like to execute to automatically update our Docker containers running in our production or test server when a new release is made in the GitHub project.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$cat update-docker.sh #!/bin/bashgit pulldocker-compose up -d&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s kill our SocketXP agent and re-run it with the two new arguments: &amp;ldquo;filter-rules.json&amp;rdquo; file and the &amp;ldquo;update-docker.sh&amp;rdquo; script file, as shown below.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">socketxp relay http://localhost:8080 --exec update-docker.sh --filter filter-rules.jsonConnected.Public URL -&amp;gt; https://webhook.socketxp.com/ganeshvelrajan-1awy5t0h&lt;/code>&lt;/pre>&lt;p>Note that the SocketXP Webhook Public URL has not changed despite killing and re-executing the command. This is because we are requesting a public URL for a webhook relay service again and that too for the same local service endpoints (http://localhost:8080). This nice feature of SocektXP saves us from updating the GitHub Webhook URL settings again.&lt;/p>&lt;h2 id="begin-demo">Begin Demo&lt;/h2>&lt;p>Now that the CD pipeline is all set, let&amp;rsquo;s upgrade our NodeJS web app and release a version 2.0 of the artifact. We also need to update the docker-compose.yml file to reflect the updated app version.&lt;/p>&lt;h3 id="app-upgrade">App Upgrade&lt;/h3>&lt;p>Here is a new version of our app (for the sake of this demo, we have simply updated the version string printed to 2.0).&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$cat http-server.js var http = require('http');var port = 8080var handleRequest = function(request, response) { console.log('Received HTTP request for URL: ' + request.url); response.writeHead(200); response.end('Hello World!, v2.0');};var server = http.createServer(handleRequest);server.listen(port);&lt;/code>&lt;/pre>&lt;h3 id="build-push-new-app-container">Build Push New App Container&lt;/h3>&lt;p>Build a new docker container containing this new version of our app.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$docker build -t gvelrajan/http-server:v2.0 .Sending build context to Docker daemon 69.63kBStep 1/7 : FROM alpine:latest ---&amp;gt; f70734b6a266Step 2/7 : RUN apk update &amp;amp;&amp;amp; apk add nodejs......Successfully tagged gvelrajan/http-server:v2.0&lt;/code>&lt;/pre>&lt;p>Push the docker image to DockerHub.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$docker push gvelrajan/http-server:v2.0The push refers to repository [docker.io/gvelrajan/http-server]4a303822279a: Pushed 947da554eff8: Layer already exists cc403a71dcfb: Layer already exists 3e207b409db3: Layer already exists v2.0: digest: sha256:2a9ee82f76758758c77659260091380b20e3b3d095efdc6480d66e47c5842476 size: 1154&lt;/code>&lt;/pre>&lt;p>Update the docker-compose file with the new docker image version.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$cat docker-compose.yml version: '3'services: web: image: &amp;quot;gvelrajan/http-server:v2.0&amp;quot; ports: - &amp;quot;8080:8080&amp;quot;&lt;/code>&lt;/pre>&lt;h3 id="git-commit-and-release-webhook">Git Commit and Release Webhook&lt;/h3>&lt;p>Perform git commit and push the changes to the GitHub repository. Create new release version tag &amp;ldquo;version 2.0&amp;rdquo; for your project in GitHub.&lt;/p>&lt;p>&lt;img src="https://www.socketxp.com/wp-content/uploads/2020/07/docker-compose-auto-update-github-webhook-release-2.png" alt="docker-compose-auto-update-github-webhook-release-v2.0">{.aligncenter .wp-image-1165 .size-full width=&amp;ldquo;1972&amp;rdquo; height=&amp;ldquo;1608&amp;rdquo;}&lt;/p>&lt;p>This event will make GitHub trigger a webhook to SocketXP agent.&lt;/p>&lt;h3 id="verify-webhook-received">Verify Webhook Received&lt;/h3>&lt;p>Check the console where SocketXP agent is running. It should have received the webhook from GitHub and applied the filters on it.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$ socketxp relay http://localhost:8080 --exec update-docker.sh --filter filter-rules.jsonConnected.Public URL -&amp;gt; https://webhook.socketxp.com/ganeshvelrajan-1awy5t0hWebhook received:POST / HTTP/1.1Host: webhook.socketxp.comAccept: */*content-type: application/jsonUser-Agent: GitHub-Hookshot/e509757X-GitHub-Delivery: fa44ca8e-bd31-11ea-9def-3e0cd2844448X-GitHub-Event: pushX-Hub-Signature: sha1=d32c79c8de3282e07400711c4e7d3d6542ef79eb{ &amp;quot;ref&amp;quot;: &amp;quot;refs/tags/2.0&amp;quot;, &amp;quot;before&amp;quot;: &amp;quot;0000000000000000000000000000000000000000&amp;quot;, &amp;quot;after&amp;quot;: &amp;quot;b5595d43b2c681257e0bae638f9de814df968901&amp;quot;, &amp;quot;repository&amp;quot;: { &amp;quot;id&amp;quot;: 276817731, &amp;quot;node_id&amp;quot;: &amp;quot;MDEwOlJlcG9zaXRvcnkyNzY4MTc3MzE=&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;docker-compose-autoupdate-on-github-webhook&amp;quot;, &amp;quot;full_name&amp;quot;: &amp;quot;socketxp-com/docker-compose-autoupdate-on-github-webhook&amp;quot;, &amp;quot;private&amp;quot;: false, ... ...}#Rule: Webhook signature matched.#Rule: ref in payload matched.All rules matched.Executing command: [update-docker.sh]......&lt;/code>&lt;/pre>&lt;p>Check if the docker container has been upgraded and running version 2.0.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe7ef0e98cd45 gvelrajan/http-server:v2.0 &amp;quot;node http-server.js&amp;quot; About a minute ago Up About a minute 0.0.0.0:8080-&amp;gt;8080/tcp docker-compose-autoupdate-on-github-webhook_web_1_64dc034e55ac&lt;/code>&lt;/pre>&lt;h3 id="verify-the-upgraded-app">Verify the upgraded app&lt;/h3>&lt;p>Curl the web service and verify if the app has been upgraded.&lt;/p>&lt;pre>&lt;code class="language-{.source-code}" data-lang="{.source-code}">$curl http://localhost:8080Hello World!, v2.0&lt;/code>&lt;/pre>&lt;p>Our web app prints &amp;ldquo;v2.0&amp;rdquo;, denoting that it got updated, automatically!&lt;/p>&lt;h2 id="conclusion">Conclusion&lt;/h2>&lt;p>Setting up Docker Compose to auto deploy on GitHub push or release webhook notification is a great way to set up Continuous Deployment(CD) pipeline. The challenge is in receiving, processing and filtering GitHub webhooks in the production servers or in the internal test servers. SocketXP is a great tool to receive, store, forward, filter and take actions based on webhooks. SocketXP Webhook Relay Service takes care of bulk of the work involved in setting up an automated CD pipeline. So that, developers can focus on their core work, while leaving the rest to SocketXP. Moreover, SocketXP has a free tier for developers. And our paid plans start at just $1.99/month.&lt;/p></description></item><item><title>How to install Jenkins on Ubuntu 20.04 LTS</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/how-to-install-jenkins-on-ubuntu-16-04/</link><pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/how-to-install-jenkins-on-ubuntu-16-04/</guid><description>&lt;p>In this tutorial, I&amp;rsquo;ll show you how to install Jenkins on Ubuntu 16.04 and configure Jenkins. You can also refer to the &lt;a href="https://pkg.jenkins.io/debian-stable/">Jenkins download page&lt;/a> to see the instructions.&lt;/p>&lt;h3 id="prerequisites">Prerequisites:&lt;/h3>&lt;ul>&lt;li>Ubuntu 16.04&lt;/li>&lt;li>Java 7 or 8&lt;/li>&lt;/ul>&lt;h3 id="installing-java-8">Installing Java 8&lt;/h3>&lt;p>Java is a prerequisite to run Jenkins. We need to install Java first. Jenkins works best with Java version 7 or 8 but not 9.&lt;/p>&lt;p>Use the following command to install Java version 8.&lt;/p>&lt;pre>&lt;code>$ sudo apt install openjdk-8-jre-headless&lt;/code>&lt;/pre>&lt;h2 id="installing-jenkins">Installing Jenkins:&lt;/h2>&lt;p>Add the repository key to the local apt package manager.&lt;/p>&lt;pre>&lt;code>$ wget -q -O - https://pkg.jenkins.io/debian/jenkins-ci.org.key | sudo apt-key add -&lt;/code>&lt;/pre>&lt;p>Next, we need to append the Debian package repository address to the server&amp;rsquo;s &lt;em>&amp;lsquo;sources.list&amp;rsquo;&lt;/em>&lt;/p>&lt;pre>&lt;code>$ echo deb https://pkg.jenkins.io/debian-stable binary/ | sudo tee /etc/apt/sources.list.d/jenkins.list&lt;/code>&lt;/pre>&lt;p>Next, update the package manager so that apt-get will use newly added repository.&lt;/p>&lt;pre>&lt;code>$ sudo apt-get update&lt;/code>&lt;/pre>&lt;p>Now install Jenkins.&lt;/p>&lt;pre>&lt;code>$ sudo apt-get install jenkins&lt;/code>&lt;/pre>&lt;p>&lt;/p>&lt;h2 id="starting-jenkins">Starting Jenkins:&lt;/h2>&lt;pre>&lt;code>$ sudo systemctl start jenkins&lt;/code>&lt;/pre>&lt;p>Check the status of the Jenkins using the below command.&lt;/p>&lt;pre>&lt;code>$ sudo systemctl status jenkins jenkins.service - LSB: Start Jenkins at boot time Loaded: loaded (/etc/init.d/jenkins; bad; vendor preset: enabled) Active: active (exited) since Thu 2018-11-15 05:55:29 UTC; 40min ago Docs: man:systemd-sysv-generator(8) Process: 6353 ExecStart=/etc/init.d/jenkins start (code=exited, status=0/SUCCES Tasks: 0 Memory: 0B CPU: 0Nov 15 05:55:28 jenkins systemd[1]: Starting LSB: Start Jenkins at boot time...Nov 15 05:55:28 jenkins jenkins[6353]: Correct java version foundNov 15 05:55:28 jenkins jenkins[6353]: * Starting Jenkins Automation Server jenkNov 15 05:55:28 jenkins su[6399]: Successful su for jenkins by rootNov 15 05:55:28 jenkins su[6399]: + ??? root:jenkinsNov 15 05:55:28 jenkins su[6399]: pam_unix(su:session): session opened for user jNov 15 05:55:29 jenkins jenkins[6353]: ...done.Nov 15 05:55:29 jenkins systemd[1]: Started LSB: Start Jenkins at boot time.Nov 15 05:59:13 jenkins systemd[1]: Started LSB: Start Jenkins at boot time.&lt;/code>&lt;/pre>&lt;p>&lt;/p>&lt;h2 id="configuring-jenkins">Configuring Jenkins:&lt;/h2>&lt;p>Jenkins runs on port 8080 on the localhost. Use your browser to connect to Jenkins using the below URL&lt;/p>&lt;pre>&lt;code>http://localhost:8080/&lt;/code>&lt;/pre>&lt;p>&lt;strong>Note&lt;/strong>: If you are installing Jenkins in a Virtual Machine in a cloud, then use the public IP address of the Virtual Machine provided by your cloud service provider. Also make sure you have updated the cloud provider&amp;rsquo;s firewall rule setting to allow traffic from external sources to port 8080.&lt;/p>&lt;p>After you connect to the Jenkins server, the browser will display an &amp;ldquo;Unlock&amp;rdquo; Jenkins page as shown below.&lt;/p>&lt;p>&lt;img src="http://www.ethernetresearch.com/wp-content/uploads/2018/11/unlock-jenkins.jpg" alt="unlock-jenkins : How to install Jenkins on Ubuntu">{.aligncenter .wp-image-1241 .size-full width=&amp;ldquo;1137&amp;rdquo; height=&amp;ldquo;590&amp;rdquo;}&lt;/p>&lt;p>Use the below command to retrieve the key to unlock Jenkins.&lt;/p>&lt;pre>&lt;code>$ sudo cat /var/lib/jenkins/secrets/initialAdminPasswordb6e2d139844c47f79dad47285cd3998d$&lt;/code>&lt;/pre>&lt;p>In the next page, choose the option to &amp;ldquo;install the suggested plugins&amp;rdquo;.&lt;/p>&lt;p>&lt;img src="http://www.ethernetresearch.com/wp-content/uploads/2018/11/jenkins-customize.jpg" alt="jenkins-customize : How to install Jenkins on Ubuntu">{.aligncenter .wp-image-1238 .size-full width=&amp;ldquo;1137&amp;rdquo; height=&amp;ldquo;590&amp;rdquo;}&lt;/p>&lt;p>It will begin installing the recommended plugins. This will take few minutes to complete.&lt;/p>&lt;p>&lt;img src="http://www.ethernetresearch.com/wp-content/uploads/2018/11/jenkins-getting-started.jpg" alt="jenkins-getting-started : How to install Jenkins on Ubuntu">{.aligncenter .wp-image-1239 .size-full width=&amp;ldquo;1137&amp;rdquo; height=&amp;ldquo;590&amp;rdquo;}&lt;/p>&lt;p>Next, it will prompt us to add new users to administer the Jenkins server. We can skip this step and continue as an &amp;ldquo;admin&amp;rdquo; user using the password we have already provided to login. But, we&amp;rsquo;ll configure a user named &amp;ldquo;ganesh&amp;rdquo; with some credentials, as shown below.&lt;/p>&lt;p>&lt;img src="http://www.ethernetresearch.com/wp-content/uploads/2018/11/Jenkins-Add-User.jpg" alt="Jenkins-Add-User : How to install Jenkins on Ubuntu">{.aligncenter .wp-image-1237 .size-full width=&amp;ldquo;1024&amp;rdquo; height=&amp;ldquo;706&amp;rdquo;}&lt;/p>&lt;p>After this is done. We&amp;rsquo;ll see the &amp;ldquo;Jenkins is Ready!&amp;rdquo; screen, as shown below.&lt;/p>&lt;p>&lt;img src="http://www.ethernetresearch.com/wp-content/uploads/2018/11/jenkins-ready.jpg" alt="jenkins-ready : How to install Jenkins on Ubuntu">{.aligncenter .wp-image-1240 .size-full width=&amp;ldquo;1129&amp;rdquo; height=&amp;ldquo;607&amp;rdquo;}&lt;/p>&lt;p>Click the &amp;ldquo;Start Using Jenkins&amp;rdquo; button to view the Jenkins Main Dashboard.&lt;/p>&lt;p>&lt;img src="http://www.ethernetresearch.com/wp-content/uploads/2018/11/Jenkinks-Dashboard.jpg" alt="Jenkinks-Dashboard : How to install Jenkins on Ubuntu">{.aligncenter .wp-image-1236 .size-full width=&amp;ldquo;1024&amp;rdquo; height=&amp;ldquo;595&amp;rdquo;}&lt;/p>&lt;p>At this point, Jenkins is successfully installed and configured. You can start adding projects in Jenkins now.&lt;/p>&lt;p>&lt;/p></description></item><item><title>Kubernetes Security - Configuring Network Policies</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-security-configuring-network-policies/</link><pubDate>Mon, 12 Nov 2018 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-security-configuring-network-policies/</guid><description>&lt;p>Kubernetes Network Policies are similar to ACL&amp;rsquo;s and Route Policies that one could configure in networking devices such as Routers, Switches and Firewalls. If you are CCNA Certified or familiar working with Cisco routers, then Network Policy is a familiar topic to you.&lt;/p>&lt;p>But, even otherwise, it is a simple and an easy topic to learn, using the learning materials prescribed below.&lt;/p>&lt;p>Kubernetes Network policies are rules configured in the Kubernetes Cluster to instruct Kubernetes to allow or deny any communication between pods, for security reasons. You can even control or restrict communication between a pod and an external entity such as a webserver or a web application or an end user.&lt;/p>&lt;p>There are ingress network policies and egress network policies, similar to ingress ACL&amp;rsquo;s and egress ACL&amp;rsquo;s in routers. It tells the direction (of the traffic) in which the rule must be applied on a pod.&lt;/p>&lt;p>By default, when no network policies are configured, Kubernetes permits all traffic by default. It doesn&amp;rsquo;t restrict any communication between pods or with any external agents outside the cluster, by default.&lt;/p>&lt;p>You need to configure network policies to begin adding rules to whitelist pods or any external agents to communicate with. Anything that is not whitelisted will be prevented from communicating with the pods running in the cluster.&lt;/p>&lt;p>&lt;strong>Please Note&lt;/strong>: Merely configuring network policies doesn&amp;rsquo;t guarantee that network traffic will be restricted. You need to install a network plugin (Calico, Flannel etc) to take these rules and put them to work in the cluster.&lt;/p>&lt;p>There are several level at which network policies can be configured - at the cluster level or namespace level or at the pod level.&lt;/p>&lt;p>I found the following github documentation and the CNCF presentation by Ahmet very useful. It covers almost everything you need to know about Kubernetes Network Policies.&lt;/p>&lt;h3 id="github">GitHub:&lt;/h3>&lt;p>&lt;a href="https://github.com/ahmetb/kubernetes-network-policy-recipes">https://github.com/ahmetb/kubernetes-network-policy-recipes&lt;/a>&lt;/p>&lt;h3 id="cncf-presentation---youtube-video">CNCF Presentation - YouTube Video&lt;/h3>&lt;p>&lt;a href="https://www.youtube.com/watch?v=3gGpMmYeEO8">https://www.youtube.com/watch?v=3gGpMmYeEO8&lt;/a>&lt;/p></description></item><item><title>Kubernetes Security - User Authentication and Authorization (RBAC)</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-security-user-authentication-authorization-rbac/</link><pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-security-user-authentication-authorization-rbac/</guid><description>&lt;p>In the article, we&amp;rsquo;ll discuss how to create credentials and add a new user to the Kubernetes Cluster to perform various roles in the cluster.&lt;/p>&lt;p>To add a new user, we as an Admin, should create and approve the SSL Private Keys and Certificate for the user using the Kubernetes Certificate Manager.&lt;/p>&lt;p>So, let&amp;rsquo;s get started.&lt;/p>&lt;h2 id="authentication">Authentication:&lt;/h2>&lt;h3 id="create-private-key">Create Private Key&lt;/h3>&lt;p>First, let&amp;rsquo;s create a private key for the user, in a separate folder.&lt;/p>&lt;pre>&lt;code>minikube:~$ mkdir credentialsminikube:~$ cd credentials/minikube:~/credentials$ openssl genrsa -out ganesh.key 2048Generating RSA private key, 2048 bit long modulus...........................................................................................+++...............................................................................+++e is 65537 (0x10001)minikube:~/credentials$&lt;/code>&lt;/pre>&lt;h3 id="create-certificate-signing-request">Create Certificate Signing Request&lt;/h3>&lt;p>Next, let&amp;rsquo;s create the Certificate Signing Request (CSR) for the private key we generated above.&lt;/p>&lt;pre>&lt;code>minikube:~/credentials$ openssl req -new -key ganesh.key -out ganesh.csr -subj &amp;quot;/CN=ganesh/O=EthernetResearch&amp;quot;\nminikube:~/credentials$minikube:~/credentials$ ls ganesh.csr ganesh.keyminikube:~/credentials$&lt;/code>&lt;/pre>&lt;p>Now let&amp;rsquo;s encode the CSR into a base64 format.&lt;/p>&lt;pre>&lt;code>minikube:~/credentials$ cat ganesh.csr | base64LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ2REQ0NBVndDQVFBd0x6RVJNQThHQTFVRUF3d0laM1psYkhKaGFtRXhHakFZQmdOVkJBb01FVVYwYUdWeQpibVYwVW1WelpXRnlZMmh1TUlJQklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUExNTNGClNzaWRCdzF5cUZhSGdWblJjdmJYNjhzZUNSYm9KbEVOY1BSUFkySEFwcVJJaWVUZW9ESGpPWktWOExsZkFoYlgKUTNVYXJCSUxVcHd2UkVwUy9iWnFJM0d6eGxHbnYzOE9iZGx0azRjYW8xK0xnTnEzaDZIWUlpUklwWllzUDVCYgoybFFyWDdsNkR4dktaOTdrdThtRUR1cmdUb3o4NzlZNll5ZVJobUlaOXdkK2RUSjMvZEo0VUl0RG44YWl3MUhuCm5SMXE4cDdYd205YjVEU1YxNnAzdENZTFF6YlpxUCt0UEd4N0VKU05hSTNZNkkzamhDcmYrYUtvZGJjU1dBR0YKTUJxVjJkajY4N2RxbU42WEdraTRCbCtZd2VaV3doTy9Gb2YxZTJuWlZVd0Q1aVRlRkFmK0h6WGJibFhpK0RCMwpMd2QyWm85QU1lblVQZTZHQ3dJREFRQUJvQUF3RFFZSktvWklodmNOQVFFTEJRQURnZ0VCQUY3VDBHNU5tVy9zCm5UTTVPWitmcC84K3BDb0xPblNqbHk3TC9XdVRwWjdNUi83MjRGZE1ObWtUUHZCc0ZqTHhEUGhMaXhpWk5LcEQKYVc3RGxNVWw1a1FydzlnOGdXTnd2S1BkMmV4OGpQb2ErZENpbngvejlBdGlHN244OFlveFd1MFJHS0lzWGh5cgpmYnVCdVdaejExTDBhRWl5YU41VXNrOWNZL21IYlN6dERlV01IZ3VUb1hSUFNiU0wxZFJTSThLSlZ4RWV6eWdZCmVKeHdYYnB6VEtteStDT0pHMmgycmszY0t4V1FiY0VESXgyem5zVStOK0lROWw3ZlZIR3R4V0dXVWpZQ0hSUkcKeG5hT1Q3Q3NHM3JEaWVRN1BBR3hoNFZTZ3JnMmlMQStIYklrbHJkdm1mVkNVdGFJa1NYYnpCZGZpYWFSWkJFagowRzRLaGp0bjQ4QT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUgUkVRVUVTVC0tLS0tCg==&lt;/code>&lt;/pre>&lt;h3 id="submit-certificate-signing-request-to-k8s">Submit Certificate Signing Request to K8s&lt;/h3>&lt;p>Let&amp;rsquo;s use the base64 encoded CSR in our request to Kubernetes Certificate Manager.&lt;/p>&lt;pre>&lt;code>minikube:~/credentials$ cat signing-request.yaml apiVersion: certificates.k8s.io/v1beta1kind: CertificateSigningRequestmetadata: name: ganesh-csrspec: groups: - system:authenticated request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ2REQ0NBVndDQVFBd0x6RVJNQThHQTFVRUF3d0laM1psYkhKaGFtRXhHakFZQmdOVkJBb01FVVYwYUdWeQpibVYwVW1WelpXRnlZMmh1TUlJQklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUExNTNGClNzaWRCdzF5cUZhSGdWblJjdmJYNjhzZUNSYm9KbEVOY1BSUFkySEFwcVJJaWVUZW9ESGpPWktWOExsZkFoYlgKUTNVYXJCSUxVcHd2UkVwUy9iWnFJM0d6eGxHbnYzOE9iZGx0azRjYW8xK0xnTnEzaDZIWUlpUklwWllzUDVCYgoybFFyWDdsNkR4dktaOTdrdThtRUR1cmdUb3o4NzlZNll5ZVJobUlaOXdkK2RUSjMvZEo0VUl0RG44YWl3MUhuCm5SMXE4cDdYd205YjVEU1YxNnAzdENZTFF6YlpxUCt0UEd4N0VKU05hSTNZNkkzamhDcmYrYUtvZGJjU1dBR0YKTUJxVjJkajY4N2RxbU42WEdraTRCbCtZd2VaV3doTy9Gb2YxZTJuWlZVd0Q1aVRlRkFmK0h6WGJibFhpK0RCMwpMd2QyWm85QU1lblVQZTZHQ3dJREFRQUJvQUF3RFFZSktvWklodmNOQVFFTEJRQURnZ0VCQUY3VDBHNU5tVy9zCm5UTTVPWitmcC84K3BDb0xPblNqbHk3TC9XdVRwWjdNUi83MjRGZE1ObWtUUHZCc0ZqTHhEUGhMaXhpWk5LcEQKYVc3RGxNVWw1a1FydzlnOGdXTnd2S1BkMmV4OGpQb2ErZENpbngvejlBdGlHN244OFlveFd1MFJHS0lzWGh5cgpmYnVCdVdaejExTDBhRWl5YU41VXNrOWNZL21IYlN6dERlV01IZ3VUb1hSUFNiU0wxZFJTSThLSlZ4RWV6eWdZCmVKeHdYYnB6VEtteStDT0pHMmgycmszY0t4V1FiY0VESXgyem5zVStOK0lROWw3ZlZIR3R4V0dXVWpZQ0hSUkcKeG5hT1Q3Q3NHM3JEaWVRN1BBR3hoNFZTZ3JnMmlMQStIYklrbHJkdm1mVkNVdGFJa1NYYnpCZGZpYWFSWkJFagowRzRLaGp0bjQ4QT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUgUkVRVUVTVC0tLS0tCg== usages: - digital signature - key encipherment - server auth&lt;/code>&lt;/pre>&lt;p>Next let&amp;rsquo;s submit the request to Kubernetes Certificate Manager.&lt;/p>&lt;pre>&lt;code>minikube:~/credentials$ kubectl create -f signing-request.yaml certificatesigningrequest.certificates.k8s.io/ganesh-csr createdminikube:~/credentials$ kubectl get csrNAME AGE REQUESTOR CONDITIONganesh-csr 1m minikube-user Pendingminikube:~/credentials$&lt;/code>&lt;/pre>&lt;h3 id="approve-the-certificate-as-k8s-admin">Approve the Certificate as K8s Admin&lt;/h3>&lt;p>Next, as a Kubernetes Admin, let&amp;rsquo;s approve the Certificate Signing Request.&lt;/p>&lt;pre>&lt;code>minikube:~/credentials$ kubectl certificate approve ganesh-csrcertificatesigningrequest.certificates.k8s.io/ganesh-csr approvedminikube:~/credentials$minikube:~/credentials$ kubectl get csrNAME AGE REQUESTOR CONDITIONganesh-csr 6m minikube-user Approved,Issuedminikube:~/credentials$&lt;/code>&lt;/pre>&lt;p>Now the user certificated has been successfully approved, let&amp;rsquo;s retrieve the signed Certificate and store it in a local file named &amp;ldquo;ganesh.crt&amp;rdquo;.&lt;/p>&lt;pre>&lt;code>minikube:~/credentials$ kubectl get csr ganesh-csr -o jsonpath='{.status.certificate}' | base64 --decode &amp;gt; ganesh.crtminikube:~/credentials$ lsganesh.crt ganesh.csr ganesh.key signing-request.yamlminikube:~/credentials$ cat ganesh.crt -----BEGIN CERTIFICATE-----MIIDJDCCAgygAwIBAgIUWea2B/T52AUW1yUpfqfDxFnM7FkwDQYJKoZIhvcNAQELBQAwFTETMBEGA1UEAxMKbWluaWt1YmVDQTAeFw0xODExMDEwNjQ0MDBaFw0xOTExMDEwNjQ0MDBaMC0xGjAYBgNVBAoTEUV0aGVybmV0UmVzZWFyY2huMQ8wDQYDVQQDEwZnYW5lc2gwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCoNnKewPY8ABlrPaJi2zQ43HgvgA5jZse2qAk4S2L4HfVGci/A8k+t62zhcQj9uMhwR5yhWX+898nvyDumD8B6my0YIMcfslc4liGcZSlwBnrQDSvin5TtcsRJDXE4fLuOrbuAWpP5RXDFK+AVj2wAkmis4i8dMNL+X63B1Kng9Wfj/eboM6JSl4kLoTrCM6dtHlvwAxcc1u8N2ceYsVof4rmF7Tjzn4YjG1j6+ZKI5dgaxl2sIxtrtEOBO4oadnhTfdFDUkLnDSIb51ZyWnKPp3kfPTno00coXhhM6LFyiTMX0H3a/yH1oQVb/MZONbyo8R2JhUXBHhRuHeCvIlh/AgMBAAGjVDBSMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUEDDAKBggrBgEFBQcDATAMBgNVHRMBAf8EAjAAMB0GA1UdDgQWBBQe/8B6PROMLku2VdNFVvLeQT2PXTANBgkqhkiG9w0BAQsFAAOCAQEAYzjYxH05MlEfC8Oue7szI4pzL/iGRQA+u9BYggbrY5STGHN64yB0sgkzD3d+nmKF1Vl/O4CKzxr1WgEl0ENxuj8ZdfTJy+7RMjJPvc77LSP4378Osw1Y38EEpmgm/g611XA8V5zbXMSJtb+N3aghgdDRmyQxC0WF2aW59MEE0mnd2/t95RxaJhxAV7i2NCz8XI9iW7GigMl6+PYWIl5+g8RBzUnhUMiFuU3vM9ELIbdPVtdz5UPSYQIZR/1o6coB8iDvGZbConJazn8E/4Surh7iKKtXvOGGSTWLpS8Rh5XhIofkYS2XDTBvNCc+C8WrvPFalPKrrFI/Mux2SayY2Q==-----END CERTIFICATE-----&lt;/code>&lt;/pre>&lt;h3 id="create-new-user-in-k8s-using-the-credentials">Create New User in K8s Using the Credentials&lt;/h3>&lt;p>Now, let&amp;rsquo;s create the user &amp;ldquo;ganesh&amp;rdquo; in Kubernetes with the approved credentials we have just generated.&lt;/p>&lt;pre>&lt;code>minikube:~/credentials$ kubectl config set-credentials ganesh --client-certificate=ganesh.crt --client-key=ganesh.keyUser &amp;quot;ganesh&amp;quot; set.minikube:~/credentials$ kubectl config viewapiVersion: v1clusters:- cluster: certificate-authority: /home/gannygans/.minikube/ca.crt server: https://10.148.0.2:8443 name: minikubecontexts:- context: cluster: minikube user: minikube name: minikubecurrent-context: minikubekind: Configpreferences: {}users:- name: ganesh user: client-certificate: /home/gannygans/credentials/ganesh.crt client-key: /home/gannygans/credentials/ganesh.key- name: minikube user: client-certificate: /home/gannygans/.minikube/client.crt client-key: /home/gannygans/.minikube/client.keyminikube:~/credentials$ &lt;/code>&lt;/pre>&lt;h3 id="create-a-new-namespace-in-the-cluster">Create a New Namespace in the Cluster&lt;/h3>&lt;p>Now, let&amp;rsquo;s create a new namespace in the cluster and assign the user &amp;ldquo;ganesh&amp;rdquo; with access rights to that namespace.&lt;/p>&lt;pre>&lt;code>minikube:~$ kubectl create namespace financenamespace/finance createdminikube:~$ kubectl get namespaceNAME STATUS AGEdefault Active 4hfinance Active 6skube-public Active 4hkube-system Active 4hminikube:~$&lt;/code>&lt;/pre>&lt;h3 id="create-new-context">Create New Context&lt;/h3>&lt;p>Next, let&amp;rsquo;s create a new context and associate the user with that context.&lt;/p>&lt;p>Context is very useful if we have many Kubernetes clusters to manage and wanted to switch the context from one Kubernetes Cluster to another, so that you could execute commands related to that cluster. Context makes the job of switching the connection very easy.&lt;/p>&lt;p>Context is also very useful if you want to switch from one namespace in a cluster to another namespace in the same cluster or in a different cluster.&lt;/p>&lt;pre>&lt;code>minikube:~/credentials$ kubectl config set-context finance-context --cluster=minikube --namespace=finance --user=ganeshContext &amp;quot;finance-context&amp;quot; created.minikube:~/credentials$ kubectl config viewapiVersion: v1clusters:- cluster:certificate-authority: /home/gannygans/.minikube/ca.crtserver: https://10.148.0.2:8443name: minikubecontexts:- context:cluster: minikubenamespace: financeuser: ganeshname: finance-context- context:cluster: minikubeuser: minikubename: minikubecurrent-context: minikubekind: Configpreferences: {}users:- name: ganeshuser:client-certificate: /home/gannygans/credentials/ganesh.crtclient-key: /home/gannygans/credentials/ganesh.key- name: minikubeuser:client-certificate: /home/gannygans/.minikube/client.crtclient-key: /home/gannygans/.minikube/client.keyminikube:~/credentials$&lt;/code>&lt;/pre>&lt;h3 id="check-user-previleges">Check User Previleges:&lt;/h3>&lt;p>Now, let&amp;rsquo;s check the access previleges of the user &amp;ldquo;ganesh&amp;rdquo; we have just created.&lt;/p>&lt;pre>&lt;code>minikube:~$ kubectl auth can-i list pods --namespace financeyesminikube:~$ kubectl auth can-i list pods --namespace finance --as ganeshno&lt;/code>&lt;/pre>&lt;p>We see that as an admin we can access pods in &amp;ldquo;finance&amp;rdquo; namespace but not as user &amp;ldquo;ganesh&amp;rdquo;. Let&amp;rsquo;s try accessing more resources in the &amp;ldquo;finance&amp;rdquo; namespace.&lt;/p>&lt;p>Let&amp;rsquo;s try to create a simple helloworld pod, as we did in the previous demos. Let&amp;rsquo;s execute the below commands as an admin because the user &amp;ldquo;ganesh&amp;rdquo; doesn&amp;rsquo;t have the access rights to create resources in the minikube cluster yet.&lt;/p>&lt;pre>&lt;code>minikube-2:~$ cat pod.yamlapiVersion: v1kind: Pod # 1metadata: name: helloworld # 2 namespace: finance labels: app: helloworldspec: # 3 containers: - image: gvelrajan/helloworld:v2.0 # 4 imagePullPolicy: Always name: helloworld # 5 ports: - containerPort: 80 # 6minikube:~$ kubectl create -f pod.yamlpod/helloworld createdminikube:~$ kubectl get pods --namespace=financeNAME READY STATUS RESTARTS AGEhelloworld 1/1 Running 0 10sminikube:~$ kubectl get pods --namespace finance --as ganeshError from server (Forbidden): pods is forbidden: User &amp;quot;ganesh&amp;quot; cannot list pods in the namespace &amp;quot;finance&amp;quot;&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s also check if the user &amp;ldquo;ganesh&amp;rdquo; has access to cluster resources like &amp;ldquo;nodes&amp;rdquo; that are not bound by a namespace.&lt;/p>&lt;pre>&lt;code>minikube:~$ kubectl get nodes --namespace finance NAME STATUS ROLES AGE VERSIONminikube Ready master 1d v1.10.0minikube:~$ kubectl get nodes --namespace finance --as ganeshError from server (Forbidden): nodes is forbidden: User &amp;quot;ganesh&amp;quot; cannot list nodes at the cluster scopeminikube:~$&lt;/code>&lt;/pre>&lt;p>By default, a user doesn&amp;rsquo;t have any access rights to any resources. The admin needs to authorize the user to perform various actions or roles within the cluster or a in a namespace within the cluster.&lt;/p>&lt;h2 id="authorization">Authorization:&lt;/h2>&lt;h3 id="role-based-access-control-rbac">Role Based Access Control (RBAC)&lt;/h3>&lt;p>Next, I&amp;rsquo;ll show you how to authorize the user &amp;ldquo;ganesh&amp;rdquo; to perform various roles in a specific namespace or in the entire cluster using RBAC .&lt;/p>&lt;pre>&lt;code>minikube-2:~$ cat role.yaml kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: namespace: finance name: finance-readerrules:- apiGroups: [&amp;quot;&amp;quot;] # &amp;quot;&amp;quot; indicates the core API group resources: [&amp;quot;pods&amp;quot;, &amp;quot;services&amp;quot;, &amp;quot;nodes&amp;quot;] verbs: [&amp;quot;get&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;list&amp;quot;]&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s create the role using the above yaml file.&lt;/p>&lt;pre>&lt;code>minikube:~$ kubectl create -f role.yaml role.rbac.authorization.k8s.io/finance-reader createdminikube:~$ kubectl get roles --namespace=financeNAME AGEfinance-reader 56s&lt;/code>&lt;/pre>&lt;p>Next, we have to bind the role to the user.&lt;/p>&lt;pre>&lt;code>minikube:~$ cat role-binding.yaml kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: finance-read-access namespace: finance subjects:- kind: User name: ganesh # Name is case sensitive apiGroup: rbac.authorization.k8s.ioroleRef: kind: Role #this must be Role or ClusterRole name: finance-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.iominikube:~$ kubectl create -f role-binding.yaml rolebinding.rbac.authorization.k8s.io/finance-read-access createdminikube:~$ kubectl get rolebindings --namespace=financeNAME AGEfinance-read-access 42sminikube:~$&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s check if the user &amp;ldquo;ganesh&amp;rdquo; has access to pods now.&lt;/p>&lt;pre>&lt;code>minikube:~$ kubectl get pods --namespace=finance --as=ganeshNAME READY STATUS RESTARTS AGEhelloworld 1/1 Running 0 43sminikube:~$ kubectl get pods --as=ganeshError from server (Forbidden): pods is forbidden: User &amp;quot;ganesh&amp;quot; cannot list pods in the namespace &amp;quot;default&amp;quot;minikube:~$&lt;/code>&lt;/pre>&lt;p>User &amp;ldquo;ganesh&amp;rdquo; has access to pods in the &amp;ldquo;finance&amp;rdquo; namespace but not in any other namespace such as the &amp;ldquo;default&amp;rdquo; namespace. This behaviour is as expected.&lt;/p>&lt;p>Next, let&amp;rsquo;s check if the user has access to any cluster resources like &amp;ldquo;nodes'&lt;/p>&lt;pre>&lt;code>minikube:~$ kubectl get nodes --namespace=finance --as=ganeshError from server (Forbidden): nodes is forbidden: User &amp;quot;ganesh&amp;quot; cannot list nodes at the cluster scope&lt;/code>&lt;/pre>&lt;p>User &amp;ldquo;ganesh&amp;rdquo; doesn&amp;rsquo;t have access to &amp;ldquo;nodes&amp;rdquo;, despite granting access to it in the above &amp;ldquo;role.yaml file.&amp;rdquo; Why ?&lt;/p>&lt;p>This is because &amp;ldquo;nodes&amp;rdquo; is a cluster resource and not a resource that belongs to the &amp;ldquo;finance&amp;rdquo; namespace or any namespace. So eventhough we had mentoned &amp;ldquo;nodes&amp;rdquo; as one of the resources in the finance namespace &amp;ldquo;role.yaml&amp;rdquo; file above. It semantically doesn&amp;rsquo;t make any sense. Hence user &amp;ldquo;ganesh&amp;rdquo; doesn&amp;rsquo;t get any permission to read cluster nodes.&lt;/p>&lt;p>How do we solve this problem ?&lt;/p>&lt;p>The answer is: &amp;ldquo;Cluster Role and Cluster Role Binding&amp;rdquo;&lt;/p>&lt;h3 id="create-cluster-role">Create Cluster Role&lt;/h3>&lt;p>We need to create a Kubernetes Cluster Role and then bind the Cluster Role to the user &amp;ldquo;ganesh&amp;rdquo; to access the Kubernetes cluster nodes.&lt;/p>&lt;pre>&lt;code>minikube:~$ cat cluster-role.yaml kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: # &amp;quot;namespace&amp;quot; omitted since ClusterRoles are not namespaced name: cluster-node-readerrules:- apiGroups: [&amp;quot;&amp;quot;] resources: [&amp;quot;nodes&amp;quot;] verbs: [&amp;quot;get&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;list&amp;quot;]&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s create the cluster role using the above yaml file.&lt;/p>&lt;pre>&lt;code>minikube:~$ kubectl create -f cluster-role.yaml clusterrole.rbac.authorization.k8s.io/cluster-node-reader createdminikube:~$ kubectl get clusterrolesNAME AGEadmin 25hcluster-admin 25hcluster-node-reader 19sedit 25hsystem:aggregate-to-admin 25h......system:node-problem-detector 25hsystem:node-proxier 25hsystem:persistent-volume-provisioner 25hsystem:volume-scheduler 25hview 25h&lt;/code>&lt;/pre>&lt;h3 id="cluter-role-binding">Cluter Role Binding&lt;/h3>&lt;p>Next, let&amp;rsquo;s use the following Cluster Role Binding to bind the Cluster Role to the user &amp;ldquo;ganesh&amp;rdquo;.&lt;/p>&lt;pre>&lt;code>minikube:~$ cat cluster-role-binding.yaml kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: read-cluster-nodessubjects:- kind: User name: ganesh # Name is case sensitive apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: cluster-node-reader apiGroup: rbac.authorization.k8s.iominikube:~$ kubectl create -f cluster-role-binding.yaml clusterrolebinding.rbac.authorization.k8s.io/read-cluster-nodes createdminikube:~$ kubectl get clusterrolebindingsNAME AGEcluster-admin 26hkubeadm:kubelet-bootstrap 26hkubeadm:node-autoapprove-bootstrap 26hkubeadm:node-autoapprove-certificate-rotation 26hkubeadm:node-proxier 26hminikube-rbac 26hread-cluster-nodes 10sstorage-provisioner 26hsystem:aws-cloud-provider 26hsystem:basic-user 26h......system:kube-controller-manager 26hsystem:kube-dns 26hsystem:kube-scheduler 26hsystem:node 26hsystem:node-proxier 26hsystem:volume-scheduler 26h&lt;/code>&lt;/pre>&lt;h3 id="verify-users-cluster-roles">Verify User&amp;rsquo;s Cluster Roles&lt;/h3>&lt;p>Now let&amp;rsquo;s check if the user &amp;ldquo;ganesh&amp;rdquo; has access to read the cluster nodes.&lt;/p>&lt;pre>&lt;code>minikube:~$ kubectl get nodes --namespace=finance --as=ganeshNAME STATUS ROLES AGE VERSIONminikube Ready master 1d v1.10.0&lt;/code>&lt;/pre>&lt;p>We don&amp;rsquo;t have to specify the &amp;ldquo;&amp;ndash;namespace&amp;rdquo; option because nodes are cluster resources and not bound to any namespace.&lt;/p>&lt;pre>&lt;code>minikube:~$ kubectl get nodes --as=ganeshNAME STATUS ROLES AGE VERSIONminikube Ready master 1d v1.10.0&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s check if user &amp;ldquo;ganesh&amp;rdquo; has access to any other cluster resource other than &amp;ldquo;nodes&amp;rdquo;.&lt;/p>&lt;pre>&lt;code>minikube:~$ kubectl get secrets --as=ganeshError from server (Forbidden): secrets is forbidden: User &amp;quot;ganesh&amp;quot; cannot list secrets in the namespace &amp;quot;default&amp;quot;&lt;/code>&lt;/pre>&lt;p>As we expected, user &amp;ldquo;ganesh&amp;rdquo; cannot access any other cluster resources except for the &amp;ldquo;nodes&amp;rdquo;.&lt;/p>&lt;p>That&amp;rsquo;s also folks, I have to discuss today, about security and user authentication, authorization using RBAC.&lt;/p></description></item><item><title>Accessing Kubernetes API Server Through REST API's</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/accessing-kubernetes-api-server-through-rest-apis/</link><pubDate>Mon, 29 Oct 2018 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/accessing-kubernetes-api-server-through-rest-apis/</guid><description>&lt;p>Typically, we use Kubectl CLI utility to talk to the Kubernetes API server to create, update, delete, read any Kubernetes objects like Pods, Deployments, Services etc.&lt;/p>&lt;p>In this tutorial, I&amp;rsquo;ll show you how to directly access the Kubernetes API server using the REST API&amp;rsquo;s.&lt;/p>&lt;p>Kubernetes API server can be accessed directly using the REST API&amp;rsquo;s in two different ways.&lt;/p>&lt;h2 id="method-1---kubectl-proxy-mode">Method #1 - Kubectl Proxy Mode&lt;/h2>&lt;p>In the method, we&amp;rsquo;ll configure Kubectl to run in a Proxy Mode, so that it will authenticate with the API Server. We can just provide the API to the Kubectl and it will in turn forward our request to the API Server using the appropriate security tokens.&lt;/p>&lt;pre>&lt;code>minikube:~$ kubectl proxy --port=8080Starting to serve on 127.0.0.1:8080&lt;/code>&lt;/pre>&lt;p>The above CLI is a blocking CLI call. So it will run until your kill it using Ctrl+C&lt;/p>&lt;p>Next point the Curl web tool to this kubectl proxy server. The kubectl inturn will redirect our API requests to the Kubernetes API server.&lt;/p>&lt;pre>&lt;code>minikube:~$ curl localhost:8080/api{ &amp;quot;kind&amp;quot;: &amp;quot;APIVersions&amp;quot;, &amp;quot;versions&amp;quot;: [ &amp;quot;v1&amp;quot; ], &amp;quot;serverAddressByClientCIDRs&amp;quot;: [ { &amp;quot;clientCIDR&amp;quot;: &amp;quot;0.0.0.0/0&amp;quot;, &amp;quot;serverAddress&amp;quot;: &amp;quot;10.160.0.3:8443&amp;quot; } ]}minikube:~$&lt;/code>&lt;/pre>&lt;p>Now, if you want to access any object within the Kubernetes Cluster then you need to get the appropriate REST API information from the &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/">Kubernetes API Reference guide&lt;/a>. They keep changing the location of the Kubernetes API Reference guide , frequently. So if the above link is broken, you need to Google Search to find it again.&lt;/p>&lt;p>In the above guide, I looked at the section on Pod. I&amp;rsquo;m interested in reading the contents of a specific pod that I have created in the Kubernetes Cluster.&lt;/p>&lt;p>&lt;img src="http://www.ethernetresearch.com/wp-content/uploads/2018/10/Kubernetes-API-Reference-Guide.jpg" alt="Kubernetes API Reference Guide">{.aligncenter .wp-image-1216 .size-full width=&amp;ldquo;992&amp;rdquo; height=&amp;ldquo;768&amp;rdquo;}&lt;/p>&lt;p>The API we need to use is the following:&lt;/p>&lt;pre>&lt;code>/api/v1/namespaces/{namespace-name}/pods/{pod-name}&lt;/code>&lt;/pre>&lt;p>Now let&amp;rsquo;s use Curl to access this API.&lt;/p>&lt;pre>&lt;code>minikube:~$ curl localhost:8080/api/v1/namespaces/default/pods/helloworld{ &amp;quot;kind&amp;quot;: &amp;quot;Pod&amp;quot;, &amp;quot;apiVersion&amp;quot;: &amp;quot;v1&amp;quot;, &amp;quot;metadata&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;helloworld&amp;quot;, &amp;quot;namespace&amp;quot;: &amp;quot;default&amp;quot;, &amp;quot;selfLink&amp;quot;: &amp;quot;/api/v1/namespaces/default/pods/helloworld&amp;quot;, &amp;quot;uid&amp;quot;: &amp;quot;f1857174-db41-11e8-ab3f-42010aa00003&amp;quot;, &amp;quot;resourceVersion&amp;quot;: &amp;quot;11847&amp;quot;, &amp;quot;creationTimestamp&amp;quot;: &amp;quot;2018-10-29T06:14:50Z&amp;quot;, &amp;quot;labels&amp;quot;: { &amp;quot;app&amp;quot;: &amp;quot;helloworld&amp;quot; } }, &amp;quot;spec&amp;quot;: { &amp;quot;volumes&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;default-token-htj76&amp;quot;, &amp;quot;secret&amp;quot;: { &amp;quot;secretName&amp;quot;: &amp;quot;default-token-htj76&amp;quot;, &amp;quot;defaultMode&amp;quot;: 420 } } ], &amp;quot;containers&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;helloworld&amp;quot;, &amp;quot;image&amp;quot;: &amp;quot;gvelrajan/helloworld:v2.0&amp;quot;, &amp;quot;ports&amp;quot;: [ { &amp;quot;containerPort&amp;quot;: 80, &amp;quot;protocol&amp;quot;: &amp;quot;TCP&amp;quot; } ], &amp;quot;resources&amp;quot;: { }, &amp;quot;volumeMounts&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;default-token-htj76&amp;quot;, &amp;quot;readOnly&amp;quot;: true, &amp;quot;mountPath&amp;quot;: &amp;quot;/var/run/secrets/kubernetes.io/serviceaccount&amp;quot; } ], &amp;quot;terminationMessagePath&amp;quot;: &amp;quot;/dev/termination-log&amp;quot;, &amp;quot;terminationMessagePolicy&amp;quot;: &amp;quot;File&amp;quot;, &amp;quot;imagePullPolicy&amp;quot;: &amp;quot;Always&amp;quot; } ], &amp;quot;restartPolicy&amp;quot;: &amp;quot;Always&amp;quot;, &amp;quot;terminationGracePeriodSeconds&amp;quot;: 30, &amp;quot;dnsPolicy&amp;quot;: &amp;quot;ClusterFirst&amp;quot;, &amp;quot;serviceAccountName&amp;quot;: &amp;quot;default&amp;quot;, &amp;quot;serviceAccount&amp;quot;: &amp;quot;default&amp;quot;, &amp;quot;nodeName&amp;quot;: &amp;quot;minikube&amp;quot;, &amp;quot;securityContext&amp;quot;: { }, &amp;quot;schedulerName&amp;quot;: &amp;quot;default-scheduler&amp;quot;, &amp;quot;tolerations&amp;quot;: [ { &amp;quot;key&amp;quot;: &amp;quot;node.kubernetes.io/not-ready&amp;quot;, &amp;quot;operator&amp;quot;: &amp;quot;Exists&amp;quot;, &amp;quot;effect&amp;quot;: &amp;quot;NoExecute&amp;quot;, &amp;quot;tolerationSeconds&amp;quot;: 300 }, { &amp;quot;key&amp;quot;: &amp;quot;node.kubernetes.io/unreachable&amp;quot;, &amp;quot;operator&amp;quot;: &amp;quot;Exists&amp;quot;, &amp;quot;effect&amp;quot;: &amp;quot;NoExecute&amp;quot;, &amp;quot;tolerationSeconds&amp;quot;: 300 } ] }, &amp;quot;status&amp;quot;: { &amp;quot;phase&amp;quot;: &amp;quot;Running&amp;quot;, &amp;quot;conditions&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;Initialized&amp;quot;, &amp;quot;status&amp;quot;: &amp;quot;True&amp;quot;, &amp;quot;lastProbeTime&amp;quot;: null, &amp;quot;lastTransitionTime&amp;quot;: &amp;quot;2018-10-29T06:14:50Z&amp;quot; }, { &amp;quot;type&amp;quot;: &amp;quot;Ready&amp;quot;, &amp;quot;status&amp;quot;: &amp;quot;True&amp;quot;, &amp;quot;lastProbeTime&amp;quot;: null, &amp;quot;lastTransitionTime&amp;quot;: &amp;quot;2018-10-29T06:14:55Z&amp;quot; }, { &amp;quot;type&amp;quot;: &amp;quot;PodScheduled&amp;quot;, &amp;quot;status&amp;quot;: &amp;quot;True&amp;quot;, &amp;quot;lastProbeTime&amp;quot;: null, &amp;quot;lastTransitionTime&amp;quot;: &amp;quot;2018-10-29T06:14:50Z&amp;quot; } ], &amp;quot;hostIP&amp;quot;: &amp;quot;10.160.0.3&amp;quot;, &amp;quot;podIP&amp;quot;: &amp;quot;172.17.0.2&amp;quot;, &amp;quot;startTime&amp;quot;: &amp;quot;2018-10-29T06:14:50Z&amp;quot;, &amp;quot;containerStatuses&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;helloworld&amp;quot;, &amp;quot;state&amp;quot;: { &amp;quot;running&amp;quot;: { &amp;quot;startedAt&amp;quot;: &amp;quot;2018-10-29T06:14:55Z&amp;quot; } }, &amp;quot;lastState&amp;quot;: { }, &amp;quot;ready&amp;quot;: true, &amp;quot;restartCount&amp;quot;: 0, &amp;quot;image&amp;quot;: &amp;quot;gvelrajan/helloworld:v2.0&amp;quot;, &amp;quot;imageID&amp;quot;: &amp;quot;docker-pullable://gvelrajan/helloworld@sha256:897f0c9ec9f26b32a258f027bde0d276954dc7bf666d34cc85106cb3430c03de&amp;quot;, &amp;quot;containerID&amp;quot;: &amp;quot;docker://d9751038d20c827eac9bcfa94b71256b494ac442b2822b74a51312fd1ed7653d&amp;quot; } ], &amp;quot;qosClass&amp;quot;: &amp;quot;BestEffort&amp;quot; }}&lt;/code>&lt;/pre>&lt;p>We see that our &amp;ldquo;helloworld&amp;rdquo; Pod, like any Kubernetes object, has a &amp;ldquo;Spec&amp;rdquo; which is the user expected state(intention) of the object and &amp;ldquo;Status&amp;rdquo; which is the current state of the object.&lt;/p>&lt;p>As a developer or a DevOps Engineer, you can use Kubectl as a proxy to access the API server directly via the REST API&amp;rsquo;s.&lt;/p>&lt;h2 id="method-2---direct-access-using-tokens-and-certificates">Method #2 - Direct Access using Tokens and Certificates&lt;/h2>&lt;p>In this method, we&amp;rsquo;ll use the Web Request Tools such as Curl to directly authenticate with the API server.&lt;/p>&lt;p>First get the API server&amp;rsquo;s IP address and TCP port using the following command.&lt;/p>&lt;pre>&lt;code>minikube:~$ kubectl config viewapiVersion: v1clusters:- cluster: certificate-authority: /home/gannygans/.minikube/ca.crt server: https://10.160.0.3:8443 name: minikubecontexts:- context: cluster: minikube user: minikube name: minikubecurrent-context: minikubekind: Configpreferences: {}users:- name: minikube user: client-certificate: /home/gannygans/.minikube/client.crt client-key: /home/gannygans/.minikube/client.keyminikube:~$ &lt;/code>&lt;/pre>&lt;p>Next we need to get the secrete token for authentication. We&amp;rsquo;ll use the following two command to get the information.&lt;/p>&lt;pre>&lt;code>$ kubectl get secrets$ kubectl describe secret &amp;lt;secret-name&amp;gt;&lt;/code>&lt;/pre>&lt;p>We&amp;rsquo;ll use the following single line shell script to retrieve the token from the above commands and set it in an environmental variable named &amp;ldquo;TOKEN&amp;rdquo;.&lt;/p>&lt;pre>&lt;code>minikube:~$ TOKEN=$(kubectl describe secret $(kubectl get secrets | grep ^default | cut -f1 -d ' ') | grep -E '^token' | cut -f2 -d':' | tr -d &amp;quot; &amp;quot;)minikube:~$ echo $TOKENeyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4taHRqNzYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjM0MDU1MTQ1LWQwZjktMTFlOC05NDk0LTQyMDEwYWEwMDAwMyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.JR1mrMxaaW1Ty22bPGh9M1PVY1wRi85ZrDz5DYztkqhO8zaqcmUwdWeOaKtdiA4fHcAx7CRupLRXWxCWsVZ4KK-sRDq6CTNI6knzLVvm1n0fjLLt15mY8bWdTZld1r2hKS5NEcE05XDCFOAnmjFJuHxtUq1dD4fhUM2g_EuZgZL44CKnTtdWWmhTBNK4ejghWQeW3M7XgewS_J3-URqSUzCkWAWMAfrPYsKb_70kecm_tQuukOL72ZWbRL333xLp5GVzXzHjBhBf5CPYOXGT-OXd4IvnkZ7vhE3kf2lE8K-fKnOjIauHl2pLFN5LapqM6IMWGzoV6NmEv_k6E7x-Yw&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s use the above token in the curl command to directly access the Kubernetes API server, as shown below.&lt;/p>&lt;pre>&lt;code>minikube:~$ curl https://10.160.0.3:8443/api/namespaces/default/pods/helloworld --header &amp;quot;Authorization: Bearer $TOKEN&amp;quot; --insecure{ &amp;quot;kind&amp;quot;: &amp;quot;Pod&amp;quot;, &amp;quot;apiVersion&amp;quot;: &amp;quot;v1&amp;quot;, &amp;quot;metadata&amp;quot;: { &amp;quot;name&amp;quot;: &amp;quot;helloworld&amp;quot;, &amp;quot;namespace&amp;quot;: &amp;quot;default&amp;quot;, &amp;quot;selfLink&amp;quot;: &amp;quot;/api/v1/namespaces/default/pods/helloworld&amp;quot;, &amp;quot;uid&amp;quot;: &amp;quot;f1857174-db41-11e8-ab3f-42010aa00003&amp;quot;, &amp;quot;resourceVersion&amp;quot;: &amp;quot;11847&amp;quot;, &amp;quot;creationTimestamp&amp;quot;: &amp;quot;2018-10-29T06:14:50Z&amp;quot;, &amp;quot;labels&amp;quot;: { &amp;quot;app&amp;quot;: &amp;quot;helloworld&amp;quot; } }, &amp;quot;spec&amp;quot;: { &amp;quot;volumes&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;default-token-htj76&amp;quot;, &amp;quot;secret&amp;quot;: { &amp;quot;secretName&amp;quot;: &amp;quot;default-token-htj76&amp;quot;, &amp;quot;defaultMode&amp;quot;: 420 } } ], &amp;quot;containers&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;helloworld&amp;quot;, &amp;quot;image&amp;quot;: &amp;quot;gvelrajan/helloworld:v2.0&amp;quot;, &amp;quot;ports&amp;quot;: [ { &amp;quot;containerPort&amp;quot;: 80, &amp;quot;protocol&amp;quot;: &amp;quot;TCP&amp;quot; } ], &amp;quot;resources&amp;quot;: { }, &amp;quot;volumeMounts&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;default-token-htj76&amp;quot;, &amp;quot;readOnly&amp;quot;: true, &amp;quot;mountPath&amp;quot;: &amp;quot;/var/run/secrets/kubernetes.io/serviceaccount&amp;quot; } ], &amp;quot;terminationMessagePath&amp;quot;: &amp;quot;/dev/termination-log&amp;quot;, &amp;quot;terminationMessagePolicy&amp;quot;: &amp;quot;File&amp;quot;, &amp;quot;imagePullPolicy&amp;quot;: &amp;quot;Always&amp;quot; } ], &amp;quot;restartPolicy&amp;quot;: &amp;quot;Always&amp;quot;, &amp;quot;terminationGracePeriodSeconds&amp;quot;: 30, &amp;quot;dnsPolicy&amp;quot;: &amp;quot;ClusterFirst&amp;quot;, &amp;quot;serviceAccountName&amp;quot;: &amp;quot;default&amp;quot;, &amp;quot;serviceAccount&amp;quot;: &amp;quot;default&amp;quot;, &amp;quot;nodeName&amp;quot;: &amp;quot;minikube&amp;quot;, &amp;quot;securityContext&amp;quot;: { }, &amp;quot;schedulerName&amp;quot;: &amp;quot;default-scheduler&amp;quot;, &amp;quot;tolerations&amp;quot;: [ { &amp;quot;key&amp;quot;: &amp;quot;node.kubernetes.io/not-ready&amp;quot;, &amp;quot;operator&amp;quot;: &amp;quot;Exists&amp;quot;, &amp;quot;effect&amp;quot;: &amp;quot;NoExecute&amp;quot;, &amp;quot;tolerationSeconds&amp;quot;: 300 }, { &amp;quot;key&amp;quot;: &amp;quot;node.kubernetes.io/unreachable&amp;quot;, &amp;quot;operator&amp;quot;: &amp;quot;Exists&amp;quot;, &amp;quot;effect&amp;quot;: &amp;quot;NoExecute&amp;quot;, &amp;quot;tolerationSeconds&amp;quot;: 300 } ] }, &amp;quot;status&amp;quot;: { &amp;quot;phase&amp;quot;: &amp;quot;Running&amp;quot;, &amp;quot;conditions&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;Initialized&amp;quot;, &amp;quot;status&amp;quot;: &amp;quot;True&amp;quot;, &amp;quot;lastProbeTime&amp;quot;: null, &amp;quot;lastTransitionTime&amp;quot;: &amp;quot;2018-10-29T06:14:50Z&amp;quot; }, { &amp;quot;type&amp;quot;: &amp;quot;Ready&amp;quot;, &amp;quot;status&amp;quot;: &amp;quot;True&amp;quot;, &amp;quot;lastProbeTime&amp;quot;: null, &amp;quot;lastTransitionTime&amp;quot;: &amp;quot;2018-10-29T06:14:55Z&amp;quot; }, { &amp;quot;type&amp;quot;: &amp;quot;PodScheduled&amp;quot;, &amp;quot;status&amp;quot;: &amp;quot;True&amp;quot;, &amp;quot;lastProbeTime&amp;quot;: null, &amp;quot;lastTransitionTime&amp;quot;: &amp;quot;2018-10-29T06:14:50Z&amp;quot; } ], &amp;quot;hostIP&amp;quot;: &amp;quot;10.160.0.3&amp;quot;, &amp;quot;podIP&amp;quot;: &amp;quot;172.17.0.2&amp;quot;, &amp;quot;startTime&amp;quot;: &amp;quot;2018-10-29T06:14:50Z&amp;quot;, &amp;quot;containerStatuses&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;helloworld&amp;quot;, &amp;quot;state&amp;quot;: { &amp;quot;running&amp;quot;: { &amp;quot;startedAt&amp;quot;: &amp;quot;2018-10-29T06:14:55Z&amp;quot; } }, &amp;quot;lastState&amp;quot;: { }, &amp;quot;ready&amp;quot;: true, &amp;quot;restartCount&amp;quot;: 0, &amp;quot;image&amp;quot;: &amp;quot;gvelrajan/helloworld:v2.0&amp;quot;, &amp;quot;imageID&amp;quot;: &amp;quot;docker-pullable://gvelrajan/helloworld@sha256:897f0c9ec9f26b32a258f027bde0d276954dc7bf666d34cc85106cb3430c03de&amp;quot;, &amp;quot;containerID&amp;quot;: &amp;quot;docker://d9751038d20c827eac9bcfa94b71256b494ac442b2822b74a51312fd1ed7653d&amp;quot; } ], &amp;quot;qosClass&amp;quot;: &amp;quot;BestEffort&amp;quot; }}&lt;/code>&lt;/pre>&lt;p>Again, we get the same output from the API server.&lt;/p>&lt;p>To setup Authentication, Authorization and Admission Control appropriately for accessing the API Server, refer to this article in Kubernetes Documentation.&lt;/p>&lt;p>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/">Controlling Access to the Kubernetes API Server&lt;/a>&lt;/p>&lt;p>That&amp;rsquo;s all folks ! Hope you got some idea on how to talk to the Kubernetes API Server directly, without using any Kubectl commands.&lt;/p></description></item><item><title>Kubernetes - How to Install Minikube in a VM</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-how-to-install-minikube-in-a-vm/</link><pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-how-to-install-minikube-in-a-vm/</guid><description>&lt;h2 id="what-is-minikube">What is Minikube&lt;/h2>&lt;p>Minikube is a miniaturized version of the Kubernetes Cluster Platform.&lt;/p>&lt;p>You can install minikube in your laptop, for development, testing and customer demo purposes.&lt;/p>&lt;p>Minikube provides a single node cluster, to run and test application containers.&lt;br>It is not a scalable software, like the &amp;ldquo;full&amp;rdquo; Kubernetes Cluster Platform .&lt;/p>&lt;p>You can run only very few pods or containers in them.&lt;/p>&lt;p>Minikube is not used in production. It is just a toy for playing with container images on your laptop.&lt;/p>&lt;h2 id="find-online-courses-from-author-ganesh-velrajan-here">Find online courses from author Ganesh Velrajan here.&lt;/h2>&lt;h3 id="docker-and-kubernetes-courses-in-udemy-at-author-discount-rateshttpwwwethernetresearchcomonline-courses-training">&lt;a href="http://www.ethernetresearch.com/online-courses-training/">Docker and Kubernetes Courses in Udemy at Author Discount Rates.&lt;/a>&lt;/h3>&lt;h3 id="minikube-installation-steps">Minikube Installation Steps&lt;/h3>&lt;p>In this Demo, we will show you, how to install minikube on Ubuntu Virtual Machine running in Google Cloud Platform.&lt;/p>&lt;p>Follow the same procedure to install Minikube anywhere, including installing it in your Lab Server or in your laptop.&lt;/p>&lt;p>This is the script I&amp;rsquo;ll use to download Minikube software.&lt;/p>&lt;pre>&lt;code>gannygans@minikube-cluster:~$ cat minikube-install.sh#Install KubeCTLsudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y apt-transport-https curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -sudo touch /etc/apt/sources.list.d/kubernetes.list echo &amp;quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&amp;quot; | sudo tee -a /etc/apt/sources.list.d/kubernetes.listsudo apt-get updatesudo apt-get install -y kubectl#Install MiniKube:curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.28.2/minikube-linux-amd64 &amp;amp;&amp;amp; chmod +x minikube &amp;amp;&amp;amp; sudo mv minikube /usr/local/bin/#Install Docker:sudo apt-get update &lt;/code>&lt;/pre>&lt;p>&amp;amp;&amp;amp; sudo apt-get install -qy docker.io&lt;/p>&lt;p>Run the installation script.&lt;/p>&lt;pre>&lt;code>gannygans@minikube-cluster:~$ sh minikube-install.sh&lt;/code>&lt;/pre>&lt;p>Check the version of the minikube installed.&lt;/p>&lt;pre>&lt;code>gannygans@minikube-cluster:~$ minikube versionminikube version: v0.28.2&lt;/code>&lt;/pre>&lt;p>The minikube software installation is complete now. Lets start the minikube cluster.&lt;/p>&lt;p>Set the &amp;ldquo;&lt;em>&amp;ndash;vm-driver&amp;rdquo;&lt;/em> option to NONE, when you install and run minikube inside a virtual machine.&lt;/p>&lt;p>&lt;strong>Note&lt;/strong>: If you plan to install minikube directly on your laptop, you should install the oracle virtualbox hypervisor, before you install minikube.&lt;br>Also set the &amp;ldquo;&lt;em>&amp;ndash;vm-driver&lt;/em>&amp;rdquo; option to virtualbox.&lt;/p>&lt;pre>&lt;code>gannygans@minikube-cluster:~$ sudo minikube start --vm-driver=none There is a newer version of minikube available (v0.29.0). Download it here:https://github.com/kubernetes/minikube/releases/tag/v0.29.0To disable this notification, run the following:minikube config set WantUpdateNotification falseStarting local Kubernetes v1.10.0 cluster...Starting VM...Getting VM IP address...Moving files into cluster...Downloading kubeadm v1.10.0Downloading kubelet v1.10.0Finished Downloading kubelet v1.10.0Finished Downloading kubeadm v1.10.0Setting up certs...Connecting to cluster...Setting up kubeconfig...Starting cluster components...Kubectl is now configured to use the cluster.===================WARNING: IT IS RECOMMENDED NOT TO RUN THE NONE DRIVER ON PERSONAL WORKSTATIONS The 'none' driver will run an insecure kubernetes apiserver as root that may leave the host vulnerable to CSRF attacksWhen using the none driver, the kubectl config and credentials generated will be root owned and will appear in the root home directory.You will need to move the files to the appropriate location and then set the correct permissions. An example of this is below: sudo mv /root/.kube $HOME/.kube # this will write over any previous configuration sudo chown -R $USER $HOME/.kube sudo chgrp -R $USER $HOME/.kube sudo mv /root/.minikube $HOME/.minikube # this will write over any previous configuration sudo chown -R $USER $HOME/.minikube sudo chgrp -R $USER $HOME/.minikubeThis can also be done automatically by setting the env var CHANGE_MINIKUBE_NONE_USER=trueLoading cached images from config file.&lt;/code>&lt;/pre>&lt;p>Follow the instructions in the output(highlighted in red above)&lt;br>Copy paste the commands highlighted in blue color in the command output above, as the instruction says.&lt;/p>&lt;pre>&lt;code>gannygans@minikube-cluster:~$ sudo mv /root/.kube $HOME/.kube # this will write over any previous configurationmv: cannot stat '/root/.kube': No such file or directorygannygans@minikube-cluster:~$ sudo chown -R $USER $HOME/.kubegannygans@minikube-cluster:~$ sudo chgrp -R $USER $HOME/.kubegannygans@minikube-cluster:~$ sudo mv /root/.minikube $HOME/.minikube # this will write over any previous configurationmv: cannot stat '/root/.minikube': No such file or directorygannygans@minikube-cluster:~$ sudo chown -R $USER $HOME/.minikubegannygans@minikube-cluster:~$ sudo chgrp -R $USER $HOME/.minikubegannygans@minikube-cluster:~$&lt;/code>&lt;/pre>&lt;p>Now we can run the minikube and kubectl commands as a non-root user. Let&amp;rsquo;s check the status of the minikube node.&lt;/p>&lt;pre>&lt;code>gannygans@minikube-cluster:~$ kubectl get nodesNAME STATUS ROLES AGE VERSIONminikube Ready master 30m v1.10.0gannygans@minikube-cluster:~$&lt;/code>&lt;/pre>&lt;p>To get more information about the node, use the describe node command.&lt;/p>&lt;pre>&lt;code>gannygans@minikube-cluster:~$ kubectl describe nodesName: minikubeRoles: masterLabels: beta.kubernetes.io/arch=amd64beta.kubernetes.io/os=linuxkubernetes.io/hostname=minikubenode-role.kubernetes.io/master=Annotations: node.alpha.kubernetes.io/ttl: 0volumes.kubernetes.io/controller-managed-attach-detach: trueCreationTimestamp: Fri, 28 Sep 2018 07:22:57 +0000Taints: &amp;lt;none&amp;gt;Unschedulable: falseConditions:......KubeletReady kubelet is posting ready status. AppArmor enabledAddresses:InternalIP: 10.160.0.4Hostname: minikubeMinikube is now ready to run pods.......&lt;/code>&lt;/pre>&lt;p>Minikube cluster is now ready to run pods.&lt;/p></description></item><item><title>Kubernetes Dashboard Setup and Login</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-dashboard-setup-and-login/</link><pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-dashboard-setup-and-login/</guid><description>&lt;h2 id="deploy-the-dashboard-ui">Deploy the Dashboard UI:&lt;/h2>&lt;pre>&lt;code>master$ kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml&lt;/code>&lt;/pre>&lt;p>We can either directly talk to the API server to connect to the dashboard or if we would like to use the &amp;ldquo;kubectl&amp;rdquo; command then we should run the following proxy command. This command is a blocking call.&lt;/p>&lt;pre>&lt;code>master$ kubectl proxyStarting to serve on 127.0.0.1:8001&lt;/code>&lt;/pre>&lt;p>&lt;/p>&lt;h2 id="creating-admin-token-for-login">Creating Admin Token for Login:&lt;/h2>&lt;p>Now open another terminal window in the master node and execute the following commands to the create the admin token for login to the Dashboard.&lt;/p>&lt;p>Create the service account first using the following YAML file. You can name the file anything.&lt;/p>&lt;pre>&lt;code>master$ cat service-account.yamlapiVersion: v1kind: ServiceAccountmetadata: name: admin-user namespace: kube-systemmaster$ kubectl create -f service-account.yamlserviceaccount/admin-user created&lt;/code>&lt;/pre>&lt;p>The admin Role already exists in the cluster. We can use it for login. We just need to create only RoleBinding for the ServiceAccount we create above.&lt;/p>&lt;pre>&lt;code>master$ cat role-binding.yamlapiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: admin-userroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: admin-user namespace: kube-systemmaster$ kubectl create -f role-binding.yamlclusterrolebinding.rbac.authorization.k8s.io/admin-user created&lt;/code>&lt;/pre>&lt;p>Now let&amp;rsquo;s find the token we need to use to login.&lt;/p>&lt;pre>&lt;code>master$ kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')Name: admin-user-token-6bphjNamespace: kube-systemLabels: Annotations: kubernetes.io/service-account.name=admin-user kubernetes.io/service-account.uid=b472e305-a92c-11e8-85f8-0800277d1239Type: kubernetes.io/service-account-tokenData====token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLTZicGhqIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJiNDcyZTMwNS1hOTJjLTExZTgtODVmOC0wODAwMjc3ZDEyMzkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.TFkjDjoz5crJ0JTupCFSY-qLmoFgxhiy_Js69JALlzs6Uof7Y2CSHpLo_7F_8xiOPlnVvWibXGh6CslXPTH87L-8uYCDpYfYoQBfl0nJjFqP1860IaoJIEdaRYshEvd4RHtZbRiN82zTEDeXMozuwKK3_wQyFf1eZMHcyWtt94KW3_kuGM6DvsdkDM59aKU1LiWif4cRXghnXy0yiEEwcCREwxRqDShzX5I3ne_hN04AnR7DM_r8Tjw0VgTOil7X3UTEZ0BD13tpdgA2K4xb551lQQC6Jr-p5bf1-fLa7sA7ztzxKRZt7rm4A8tDHNve5WPfesOtODPz4L7IAGx1nQca.crt: 1025 bytesnamespace: 11 bytes&lt;/code>&lt;/pre>&lt;p>Now point the browser to the following URL:&lt;/p>&lt;p>http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/&lt;/p>&lt;p>Input the token we got from the above output into the token field below.&lt;/p>&lt;p>&lt;img src="http://www.thetechzone.in/wp-content/uploads/2018/09/K8-Dashboard1.jpg" alt="Kubernetes Dashboard Login">{.wp-image-48 .size-full width=&amp;ldquo;640&amp;rdquo; height=&amp;ldquo;276&amp;rdquo;}&lt;strong>Kubernetes Dashboard Login Screen&lt;/strong>&lt;/p>&lt;p>After we login, we can access the pods and deployments in the Kubernetes dashboard.&lt;/p>&lt;p>&lt;img src="http://www.thetechzone.in/wp-content/uploads/2018/09/K8-Dashboard2.jpg" alt="Kubernetes Dashboard Setup">&lt;strong>Kubernetes Dashboard&lt;/strong>&lt;/p></description></item><item><title>Kubernetes Horizontal Pod Autoscaler and Metric Server</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-horizontal-pod-autoscaler-and-metric-server/</link><pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-horizontal-pod-autoscaler-and-metric-server/</guid><description>&lt;h2 id="what-is-horizontal-pod-autoscalerhpa">What is Horizontal Pod Autoscaler(HPA)&lt;/h2>&lt;p>The Horizontal Pod Autoscaler automatically scales up or scales down the number of pods in a deployment based on some metric such as CPU usage or some other custom metric.&lt;/p>&lt;p>Here is a &lt;strong>formal definition&lt;/strong> from the Kubernetes Official Documentation:&lt;/p>&lt;p>&amp;ldquo;The Horizontal Pod Autoscaler automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with &lt;a href="https://git.k8s.io/community/contributors/design-proposals/instrumentation/custom-metrics-api.md">custom metrics&lt;/a> support, on some other application-provided metrics). Note that Horizontal Pod Autoscaling does not apply to objects that cant be scaled, for example, DaemonSets.&lt;/p>&lt;p>The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user.&amp;rdquo;&lt;/p>&lt;p>&lt;img src="http://www.thetechzone.in/wp-content/uploads/2018/09/Screen-Shot-2018-09-28-at-8.24.06-AM-300x252.jpg" alt="Kubernetes Horizontal Pod Autoscaler and metric-server">{.alignnone .wp-image-43 width=&amp;ldquo;464&amp;rdquo; height=&amp;ldquo;390&amp;rdquo;}&lt;/p>&lt;p>In this article, we&amp;rsquo;ll configure HPA to use the CPU utilization as a metric to automatically scale up or scale down pods in a deployment using Minikube.&lt;/p>&lt;h2 id="metric-server">Metric-Server:&lt;/h2>&lt;p>Metric-Server collects the resource utilization such as CPU or Memory utilization across the cluster. It provides a metrics API through which other agents, such as the &lt;em>&amp;ldquo;kubectl top&lt;/em> &amp;ldquo;command, HPA, and even the users, could fetch the resource utilization information to make various decisions.&lt;/p>&lt;h3 id="installing-and-configuring-metric-server-and-hpa-in-the-cluster">Installing and Configuring Metric-Server and HPA in the cluster&lt;/h3>&lt;p>In this demo, I&amp;rsquo;ll use a single node Minikube Cluster running in a VM in Google Cloud Platform to demonstrate the power of Horizontal Pod Autoscaler.&lt;/p>&lt;p>Following the instructions in this article to &lt;a href="http://www.ethernetresearch.com/kubernetes/kubernetes-how-to-install-minikube-in-a-vm/">install Minikube in a VM or Laptop&lt;/a>&lt;/p>&lt;p>If you are planning to implement HPA autoscaler using Kubernetes Cluster, then follow the procedure in this &lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/core-metrics-pipeline/">documentation on Core Metric Pipeline&lt;/a> to install the metric-server.&lt;/p>&lt;h4 id="step-1">Step #1:&lt;/h4>&lt;p>Enable the metric-server in the Minikube cluster. You need to install the following command as a root user.&lt;/p>&lt;pre>&lt;code>gannygans@minikube:~$ sudo minikube addons enable metrics-servermetrics-server was successfully enabled&lt;/code>&lt;/pre>&lt;h4 id="step2">Step#2:&lt;/h4>&lt;p>Create a deployment with 1 pod using the below deployment YAML file. We&amp;rsquo;ll use the &lt;em>gvelrajan/hello-world:v2.0&lt;/em> application from my public docker hub registry.&lt;/p>&lt;p>The pod runs a simple nodejs helloworld web server application that prints &amp;ldquo;Hello World!&amp;rdquo; on the browser when connected to the web server.&lt;/p>&lt;pre>&lt;code>gannygans@minikube:~$ cat deployment.yaml apiVersion: extensions/v1beta1kind: Deployment # 1metadata: name: helloworldspec: replicas: 1 # 2 minReadySeconds: 15 strategy: type: RollingUpdate # 3 rollingUpdate: maxUnavailable: 1 # 4 maxSurge: 1 # 5 template: # 6 metadata: labels: app: helloworld # 7 spec: containers: - image: gvelrajan/hello-world:v2.0 imagePullPolicy: Always # 8 name: helloworld ports: - containerPort: 80 resources: requests: cpu: 200mgannygans@minikube:~$ kubectl get podsNo resources found.gannygans@minikube:~$ kubectl get deploymentNo resources found.gannygans@minikube:~$ kubectl create -f deployment.yamldeployment.extensions/helloworld createdgannygans@minikube:~$ kubectl get podsNAME READY STATUS RESTARTS AGEhelloworld-5cd647f57c-z9s87 1/1 Running 0 2m&lt;/code>&lt;/pre>&lt;h4 id="step-3">Step #3:&lt;/h4>&lt;p>Let&amp;rsquo;s create a load balancer service named &amp;ldquo;&lt;em>helloworld-lb&lt;/em>&amp;rdquo; using the below service YAML.&lt;/p>&lt;pre>&lt;code>gannygans@minikube:~$ cat service.yamlapiVersion: v1kind: Service # 1metadata: name: helloworld-lbspec: externalIPs: - 10.160.0.3 type: LoadBalancer # 2 ports: - port: 80 # 3 protocol: TCP # 4 targetPort: 80 # 5 selector: # 6 app: helloworld # 7gannygans@minikube:~$ kubectl get serviceNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEhelloworld-lb LoadBalancer 10.110.45.50 10.160.0.3 80:32741/TCP 3hkubernetes ClusterIP 10.96.0.1 &amp;lt;none&amp;gt; 443/TCP 15d&lt;/code>&lt;/pre>&lt;h3 id="configure-the-autoscaler">Configure the Autoscaler:&lt;/h3>&lt;p>Now, let&amp;rsquo;s configure an Autoscaler for the deployment we have created.&lt;/p>&lt;p>We want the autoscaler to automatically spin-up a new pod in the deployment, when the CPU utilization goes above 2%.&lt;/p>&lt;p>And similarly, if the CPU utilization goes below 2%, we want the Autoscaler to automatically bring down a pod.&lt;/p>&lt;p>We want autoscaler to always run atleast 1 pod even when the CPU utilization is 0%.&lt;/p>&lt;p>At the maximum, it can scale the number of pods to 10.&lt;/p>&lt;p>It is a good security practice to set the max limit on the pods, when configuring autoscaler.&lt;/p>&lt;p>If we don&amp;rsquo;t specify the maximum number, the autoscaler could be attacked by a DoS attacker to spin up unlimited number of pods and consume all the resources available in the cluster, making the other applications running in the same cluster to starve for resources.&lt;/p>&lt;pre>&lt;code>gannygans@minikube:~$ kubectl autoscale deployment helloworld --cpu-percent=2 --min=1 --max=10horizontalpodautoscaler.autoscaling/helloworld autoscaledgannygans@minikube:~$&lt;/code>&lt;/pre>&lt;p>Now check the status of the Horizontal Pod Scaler for our deployment.&lt;/p>&lt;pre>&lt;code>gannygans@minikube:~$ kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEhelloworld Deployment/helloworld &amp;lt;unknown&amp;gt;/2% 1 10 0 18sgannygans@minikube:~$ kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEhelloworld Deployment/helloworld &amp;lt;unknown&amp;gt;/2% 1 10 0 26sgannygans@minikube:~$ kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEhelloworld Deployment/helloworld 0%/2% 1 10 1 1m&lt;/code>&lt;/pre>&lt;p>&lt;strong>Note&lt;/strong>: It may take few minutes for the HPA to collect the metrics from the metric-server. HPA doesn&amp;rsquo;t poll the metric-server too often. It polls metric-server only every minute.&lt;/p>&lt;h3 id="automatic-pod-scale-up-and-scale-down-demo">Automatic Pod Scale Up and Scale Down Demo&lt;/h3>&lt;p>Crank up the CPU usage by repeatedly polling the web-interface in a loop using a shell script.&lt;/p>&lt;pre>&lt;code>$cat test.shfor var in {1..10000} do curl http://35.200.250.14/ echo &amp;quot;\n&amp;quot;done$$sh test.shHello World!Hello World!Hello World!Hello World!Hello World!Hello World!Hello World!Hello World!......&lt;/code>&lt;/pre>&lt;p>&lt;strong>Note&lt;/strong>: It may take few minutes for the HPA to detect and begin taking any action.&lt;/p>&lt;pre>&lt;code>gannygans@minikube:~$ kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEhelloworld Deployment/helloworld 2%/2% 1 10 1 4mgannygans@minikube:~$ kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEhelloworld Deployment/helloworld 12%/2% 1 10 1 4mgannygans@minikube:~$ kubectl get podsNAME READY STATUS RESTARTS AGEhelloworld-5cd647f57c-bstxl 1/1 Running 0 23shelloworld-5cd647f57c-djxn5 1/1 Running 0 23shelloworld-5cd647f57c-l7v94 1/1 Running 0 23shelloworld-5cd647f57c-z9s87 1/1 Running 0 7mgannygans@minikube:~$gannygans@minikube:~$ kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEhelloworld Deployment/helloworld 2%/2% 1 10 4 7m&lt;/code>&lt;/pre>&lt;p>Now, let&amp;rsquo;s kill the script to bring down the CPU utilization.&lt;/p>&lt;pre>&lt;code>......Hello World!Hello World!Hello World!^C$$&lt;/code>&lt;/pre>&lt;p>Check if the autoscaler detects the drop in CPU utilization and starts to bring down the pods one by one, slowly.&lt;/p>&lt;p>&lt;strong>Note&lt;/strong>: It may take few minutes for the HPA to detect and begin taking any action.&lt;/p>&lt;pre>&lt;code>gannygans@minikube:~$ kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEhelloworld Deployment/helloworld 9%/2% 1 10 4 5mgannygans@minikube:~$ kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEhelloworld Deployment/helloworld 3%/2% 1 10 4 6mgannygans@minikube:~$ kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEhelloworld Deployment/helloworld 3%/2% 1 10 4 6mgannygans@minikube:~$ kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEhelloworld Deployment/helloworld 2%/2% 1 10 4 7mgannygans@minikube:~$ kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEhelloworld Deployment/helloworld 2%/2% 1 10 4 7mgannygans@minikube:~$gannygans@minikube:~$ kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEhelloworld Deployment/helloworld 0%/2% 1 10 4 9mgannygans@minikube:~$ kubectl get podsNAME READY STATUS RESTARTS AGEhelloworld-5cd647f57c-bstxl 1/1 Terminating 0 5mhelloworld-5cd647f57c-djxn5 1/1 Terminating 0 5mhelloworld-5cd647f57c-l7v94 1/1 Terminating 0 5mhelloworld-5cd647f57c-z9s87 1/1 Running 0 12mgannygans@minikube:~$ kubectl get podsNAME READY STATUS RESTARTS AGEhelloworld-5cd647f57c-z9s87 1/1 Running 0 13mgannygans@minikube:~$&lt;/code>&lt;/pre>&lt;p>Now to delete the HPA autoscaler, using the delete command.&lt;/p>&lt;pre>&lt;code>gannygans@minikube:~$ kubectl delete hpa helloworldhorizontalpodautoscaler.autoscaling &amp;quot;helloworld&amp;quot; deleted&lt;/code>&lt;/pre>&lt;p>&lt;/p>&lt;p>&lt;/p>&lt;p>&lt;/p>&lt;p>&lt;/p>&lt;p>&lt;/p>&lt;p>&lt;/p></description></item><item><title>Docker Containers Tutorial - Persistent Storage Volumes and Stateful Containers</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/docker-tutorial-persistent-storage-volumes-and-stateful-containers/</link><pubDate>Tue, 23 Oct 2018 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/docker-tutorial-persistent-storage-volumes-and-stateful-containers/</guid><description>&lt;p>In this article, I&amp;rsquo;ll show you how to mount persistent storage volumes inside a Docker container, so that Stateful Applications such as MySQL or MongoDB or PostgreSQL could be run as Docker Containers. We&amp;rsquo;ll use MySQL as an example here.&lt;/p>&lt;h2 id="why-use-persistent-storage-volumes">Why use Persistent Storage Volumes?&lt;/h2>&lt;p>Containers are ephemeral in nature. They have a filesystem of their own. When containers die, the data stored locally in their filesystem is also gone. Stateful applications such as MySQL or MongoDB or PostgreSQL cannot be run as Docker Containers, because the data stored in their database will be lost when the container crashes or dies or gets deleted. Data is critical. We don&amp;rsquo;t want to lose it at any cost.&lt;/p>&lt;h2 id="what-is-a-persistent-storage-volume-">What is a Persistent Storage Volume ?&lt;/h2>&lt;p>A storage device or volume that can persist a container crash or its life cycle is called a Persistent Storage Volume. Persistent storage volumes can be created in a disk mounted to the Host Machine directly or it could be created in a NAS storage device in the Local LAN network mounted as a network storage device on the Host Machine or it could even be created in a a cloud storage device mounted as a storage device in the Host Machine.&lt;/p>&lt;h2 id="how-to-create-persistent-storage-volumes---two-methods">How to create Persistent Storage Volumes - Two Methods:&lt;/h2>&lt;p>There are two different methods to mount a persistent storage volume inside a Docker Container.&lt;/p>&lt;p>&lt;strong>Method #1:&lt;/strong>&lt;/p>&lt;p>You can create a new persistent storage volume in the Host Machine and mount it under a directory or folder inside a Docker Container. The Docker Container gets exclusive access to the storage volume. The data stored in the volume cannot be easily read, manipulated or corrupted from the Host Machine ( although, we could read the data through some means, I&amp;rsquo;ll show you how, later in this article).&lt;/p>&lt;p>&lt;strong>Method #2:&lt;/strong>&lt;/p>&lt;p>You can mount a local directory in the host machine as a persistent storage volume inside a Docker Container, so that data could be shared between the host machine and Docker Container. This method is very useful if the host machine wants to access or periodically backup the data or database written to the folder by the DB server running inside the Docker Container.&lt;/p>&lt;p>&lt;strong>Demo:&lt;/strong>&lt;/p>&lt;h3 id="method-1">Method #1&lt;/h3>&lt;p>Check if there are any existing volumes:&lt;/p>&lt;pre>&lt;code>$ docker volume lsDRIVER VOLUME NAME$&lt;/code>&lt;/pre>&lt;p>Create a new persistent storage volume in the Host Machine&lt;/p>&lt;pre>&lt;code>$ docker volume create mysql-datamysql-data$ docker volume lsDRIVER VOLUME NAMElocal mysql-data$&lt;/code>&lt;/pre>&lt;p>Inspect the storage volume to get more detailed information.&lt;/p>&lt;pre>&lt;code>$ docker volume inspect mysql-data[ { &amp;quot;CreatedAt&amp;quot;: &amp;quot;2018-10-23T03:18:12Z&amp;quot;, &amp;quot;Driver&amp;quot;: &amp;quot;local&amp;quot;, &amp;quot;Labels&amp;quot;: {}, &amp;quot;Mountpoint&amp;quot;: &amp;quot;/var/lib/docker/volumes/mysql-data/_data&amp;quot;, &amp;quot;Name&amp;quot;: &amp;quot;mysql-data&amp;quot;, &amp;quot;Options&amp;quot;: {}, &amp;quot;Scope&amp;quot;: &amp;quot;local&amp;quot; }]&lt;/code>&lt;/pre>&lt;p>Check the data in the storage volume, from where it is mounted locally. You need root privileges to access the folder where it is mounted in the host machine.&lt;/p>&lt;pre>&lt;code>$ ls /var/lib/docker/volumes/mysql-data/_datals: cannot access '/var/lib/docker/volumes/mysql-data/_data': Permission denied$ sudo ls /var/lib/docker/volumes/mysql-data/_data$&lt;/code>&lt;/pre>&lt;p>It is an empty volume with no data, as we expected.&lt;/p>&lt;p>Create a MySQL DB (stateful) Docker Container and make it use the persistent storage volume we just created. So that, MySQL will store its DB and files in this volume. MySQL stores its data at the /var/lib/mysql folder.&lt;/p>&lt;pre>&lt;code>$ docker run --name ganesh-mysql -v mysql-data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=mypasswd -d mysql:latestUnable to find image 'mysql:latest' locallylatest: Pulling from library/mysqlf17d81b4b692: Pull complete c691115e6ae9: Pull complete 41544cb19235: Pull complete 254d04f5f66d: Pull complete 4fe240edfdc9: Pull complete 0cd4fcc94b67: Pull complete 8df36ec4b34a: Pull complete 739800af3a9f: Pull complete 0cbc995daddd: Pull complete 5db5c83b9b9a: Pull complete 9cb56d3f0a7e: Pull complete 448d4de73cac: Pull complete Digest: sha256:8fdc47e9ccb8112a62148032ae70484e3453b628ab6fe02bccf159e2966b750eStatus: Downloaded newer image for mysql:lateste15c04f42ec6e5b0e0ae2256caf0f6ed4760c8f27806f300f9246bd0718dbe37$&lt;/code>&lt;/pre>&lt;p>Now, get into the MySQL Docker Container&amp;rsquo;s bash shell and check the /var/lib/mysql folder.&lt;/p>&lt;pre>&lt;code>$ docker exec -it e15c /bin/bashroot@e15c04f42ec6:/# lsbin docker-entrypoint-initdb.d home media proc sbin tmpboot entrypoint.sh lib mnt root srv usrdev etc lib64 opt run sys varroot@e15c04f42ec6:/# ls /var/lib/mysqlauto.cnf ca.pem ib_logfile1 performance_schema sysbinlog.000001 client-cert.pem ibdata1 private_key.pem undo_001binlog.000002 client-key.pem ibtmp1 public_key.pem undo_002binlog.index ib_buffer_pool mysql server-cert.pemca-key.pem ib_logfile0 mysql.ibd server-key.pemroot@e15c04f42ec6:/# exitexit$&lt;/code>&lt;/pre>&lt;p>The folder has the mysql database and files.&lt;/p>&lt;p>Now let&amp;rsquo;s check the same from the host machine&amp;rsquo;s mount point folder.&lt;/p>&lt;pre>&lt;code>$ sudo ls /var/lib/docker/volumes/mysql-data/_dataauto.cnf ca.pem ib_logfile0 performance_schema sysbinlog.000001 client-cert.pem ib_logfile1 private_key.pem undo_001binlog.000002 client-key.pem ibtmp1 public_key.pem undo_002binlog.index ib_buffer_pool mysql server-cert.pemca-key.pem ibdata1 mysql.ibd server-key.pem$&lt;/code>&lt;/pre>&lt;p>They reflect the same information. This is because it&amp;rsquo;s a volume created in the host namespace and mounted inside the container.&lt;/p>&lt;p>Now, if we kill and create a new MySQL docker container, it can reuse the MySQL database created by the previous incarnation, as it is. That&amp;rsquo;s the power of persistent storage volumes. They persist a container crash or death.&lt;/p>&lt;pre>&lt;code>$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe15c04f42ec6 mysql:latest &amp;quot;docker-entrypoint.s&amp;quot; 10 minutesago Up 10 minutes 3306/tcp, 33060/tcp ganesh-mysql$ docker container stop ganesh-mysqlganesh-mysql$ docker container rm ganesh-mysqlganesh-mysql$ docker container ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES$&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s get the timestamp and snapshot of the files in the mysql-data storage volume we created.&lt;/p>&lt;pre>&lt;code>$ sudo ls -l /var/lib/docker/volumes/mysql-data/_datatotal 179200-rw-r----- 1 999 docker 56 Oct 23 03:31 auto.cnf-rw-r----- 1 999 docker 3071644 Oct 23 03:31 binlog.000001-rw-r----- 1 999 docker 178 Oct 23 03:42 binlog.000002-rw-r----- 1 999 docker 155 Oct 23 03:44 binlog.000003-rw-r----- 1 999 docker 48 Oct 23 03:44 binlog.index-rw------- 1 999 docker 1680 Oct 23 03:31 ca-key.pem-rw-r--r-- 1 999 docker 1112 Oct 23 03:31 ca.pem-rw-r--r-- 1 999 docker 1112 Oct 23 03:31 client-cert.pem-rw------- 1 999 docker 1680 Oct 23 03:31 client-key.pem-rw-r----- 1 999 docker 4953 Oct 23 03:42 ib_buffer_pool-rw-r----- 1 999 docker 12582912 Oct 23 03:44 ibdata1-rw-r----- 1 999 docker 50331648 Oct 23 03:44 ib_logfile0-rw-r----- 1 999 docker 50331648 Oct 23 03:30 ib_logfile1-rw-r----- 1 999 docker 12582912 Oct 23 03:44 ibtmp1drwxr-x--- 2 999 docker 4096 Oct 23 03:31 mysql-rw-r----- 1 999 docker 31457280 Oct 23 03:44 mysql.ibddrwxr-x--- 2 999 docker 4096 Oct 23 03:31 performance_schema-rw------- 1 999 docker 1676 Oct 23 03:31 private_key.pem-rw-r--r-- 1 999 docker 452 Oct 23 03:31 public_key.pem-rw-r--r-- 1 999 docker 1112 Oct 23 03:31 server-cert.pem-rw------- 1 999 docker 1680 Oct 23 03:31 server-key.pemdrwxr-x--- 2 999 docker 4096 Oct 23 03:31 sys-rw-r----- 1 999 docker 12582912 Oct 23 03:44 undo_001-rw-r----- 1 999 docker 10485760 Oct 23 03:44 undo_002&lt;/code>&lt;/pre>&lt;p>Now create a new MySQL Docker Container and make it use the same storage volume where MySQL DB and files already exists.&lt;/p>&lt;pre>&lt;code>$ docker run --name ganesh-mysql-v2 -v mysql-data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=mypasswd -d mysql:latest46973aa2cddb3b65c94ec37bdee3e8b044767aa4ccc0962900e712eefd4710ec$$ sudo ls -l /var/lib/docker/volumes/mysql-data/_datatotal 179200-rw-r----- 1 999 docker 56 Oct 23 03:31 auto.cnf-rw-r----- 1 999 docker 3071644 Oct 23 03:31 binlog.000001-rw-r----- 1 999 docker 178 Oct 23 03:42 binlog.000002-rw-r----- 1 999 docker 155 Oct 23 03:44 binlog.000003-rw-r----- 1 999 docker 48 Oct 23 03:44 binlog.index-rw------- 1 999 docker 1680 Oct 23 03:31 ca-key.pem-rw-r--r-- 1 999 docker 1112 Oct 23 03:31 ca.pem-rw-r--r-- 1 999 docker 1112 Oct 23 03:31 client-cert.pem-rw------- 1 999 docker 1680 Oct 23 03:31 client-key.pem-rw-r----- 1 999 docker 4953 Oct 23 03:42 ib_buffer_pool-rw-r----- 1 999 docker 12582912 Oct 23 03:44 ibdata1-rw-r----- 1 999 docker 50331648 Oct 23 03:44 ib_logfile0-rw-r----- 1 999 docker 50331648 Oct 23 03:30 ib_logfile1-rw-r----- 1 999 docker 12582912 Oct 23 03:44 ibtmp1drwxr-x--- 2 999 docker 4096 Oct 23 03:31 mysql-rw-r----- 1 999 docker 31457280 Oct 23 03:44 mysql.ibddrwxr-x--- 2 999 docker 4096 Oct 23 03:31 performance_schema-rw------- 1 999 docker 1676 Oct 23 03:31 private_key.pem-rw-r--r-- 1 999 docker 452 Oct 23 03:31 public_key.pem-rw-r--r-- 1 999 docker 1112 Oct 23 03:31 server-cert.pem-rw------- 1 999 docker 1680 Oct 23 03:31 server-key.pemdrwxr-x--- 2 999 docker 4096 Oct 23 03:31 sys-rw-r----- 1 999 docker 12582912 Oct 23 03:44 undo_001-rw-r----- 1 999 docker 10485760 Oct 23 03:44 undo_002&lt;/code>&lt;/pre>&lt;p>We see that the files and their timestamp has not changed. This proves that the new MySQL instances reused the database and files at /var/lib/mysql. It also proves that the data is persistent across a container crash.&lt;/p>&lt;p>&lt;/p>&lt;h3 id="method-2">Method #2:&lt;/h3>&lt;p>First, let&amp;rsquo;s create a directory in the host machine.&lt;/p>&lt;pre>&lt;code>$ mkdir mysql-data-dir$ ls mysql-data-dir/$&lt;/code>&lt;/pre>&lt;p>Now share the directory in the host-machine with the MySQL Docker Container.&lt;/p>&lt;pre>&lt;code>$ docker run --name ganesh-mysql-v3 &lt;/code>&lt;/pre>&lt;p>-v /home/user/mysql-data-dir:/var/lib/mysql&lt;br>-e MYSQL_ROOT_PASSWORD=mypasswd -d mysql:latesta809f1fa7608714641148a47889d53894a60e9ead2f21b177d411dc7197da21a&lt;/p>&lt;pre>&lt;code>$ ls /home/user/mysql-data-dirauto.cnf client-cert.pem ib_logfile1 private_key.pem undo_001binlog.000001 client-key.pem ibtmp1 public_key.pem undo_002binlog.index ib_buffer_pool mysql server-cert.pemca-key.pem ibdata1 mysql.ibd server-key.pemca.pem ib_logfile0 performance_schema sys$&lt;/code>&lt;/pre>&lt;p>We see that the mysql container has written its files in the folder we shared with it from the host machine.&lt;/p>&lt;p>That&amp;rsquo;s all folks !&lt;/p>&lt;p>You can see this tutorial in action in these video lectures below.&lt;/p>&lt;h3 id="tutorial">Tutorial:&lt;/h3>&lt;p>&lt;a href="https://youtu.be/rIxOLB2k-Xc">https://youtu.be/rIxOLB2k-Xc&lt;/a>&lt;/p>&lt;h3 id="demo">Demo:&lt;/h3>&lt;p>&lt;a href="https://youtu.be/YTNpSxY_icw">https://youtu.be/YTNpSxY_icw&lt;/a>&lt;/p></description></item><item><title>Kubernetes Tutorial - Hands On For Beginners</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-tutorial-hands-on/</link><pubDate>Sat, 11 Aug 2018 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-tutorial-hands-on/</guid><description>&lt;p>In the previous two Kubernetes tutorials, we saw how to install and setup a Kubernetes cluster and we discussed the fundamental building blocks of Kubernetes.&lt;/p>&lt;p>In this tutorial, we&amp;rsquo;ll do a hands-on session on how to run a web application as a docker container in the &lt;a href="http://35.238.255.76/geekzone/kubernetes-tutorial-how-to-install-kubernetes-on-ubuntu/">Kubernetes Cluster we created&lt;/a> previously.&lt;/p>&lt;h2 id="sample-web-application">Sample Web Application&lt;/h2>&lt;p>Here is the sample &amp;ldquo;Hello, World&amp;rdquo; Node.js web application that we&amp;rsquo;ll use for this exercise.&lt;/p>&lt;pre>&lt;code>$ cat myapp.jsvar http = require('http');var rand = Math.floor(Math.random() * 100);//create a server object:http.createServer(function (req, res) {res.writeHead(200, {'Content-Type': 'text/html'});res.write('Hello World!'); //write a response to the clientres.write('My number is: ' + rand); //write a response to the clientres.end(); //end the response}).listen(80); //the server object listens on port 80&lt;/code>&lt;/pre>&lt;p>Here is the Dockerfile we&amp;rsquo;ll use to package this JavaScript application into a docker container image.&lt;/p>&lt;pre>&lt;code>$ cat DockerfileFROM alpine:latestRUN apk update &amp;amp;&amp;amp; apk add nodejsRUN mkdir -p /usr/src/appCOPY ./myapp.js /usr/src/appWORKDIR /usr/src/appEXPOSE 80 CMD [&amp;quot;node&amp;quot;,&amp;quot;myapp.js&amp;quot;]&lt;/code>&lt;/pre>&lt;p>Refer to the Docker Tutorial on &lt;a href="http://35.238.255.76/geekzone/docker-container-tutorial-creating-docker-container-images/">How to build a Docker Image&lt;/a>, that we covered earlier.&lt;/p>&lt;h2 id="build-docker-image">Build Docker Image&lt;/h2>&lt;p>Build the docker image using the below command. Replace the registry name &amp;ldquo;gvelrajan&amp;rdquo; with your own registry in DockerHub.&lt;/p>&lt;pre>&lt;code>$ docker image build -t gvelrajan/hello-world:v1.0 .&lt;/code>&lt;/pre>&lt;p>Once the image is built, push the image to the DockerHub, so that you can access it from anywhere.&lt;/p>&lt;pre>&lt;code>$ docker push gvelrajan/hello-world:v1.0&lt;/code>&lt;/pre>&lt;h2 id="run-the-docker-image-in-kubernetes-cluster">Run the Docker Image in Kubernetes Cluster&lt;/h2>&lt;p>Kubernetes is so popular because it makes the job of managing containers very easy. All you need to do is describe your intent on &amp;ldquo;How to launch your web application&amp;rdquo; in a YAML file and Kubernetes will manifest that intent for you. Kubernetes hides all the implementation details from the user.&lt;/p>&lt;p>For example, to run our &amp;ldquo;Hello, World&amp;rdquo; docker image we just built, we need to create a Kubernetes Pod YAML file and provide it as an input to Kubernetes.&lt;/p>&lt;p>Kubernetes will take care of scheduling the pod in one of the available nodes in the cluster.&lt;/p>&lt;h2 id="create-a-pod">Create a Pod&lt;/h2>&lt;p>Here is our Pod YAML file for our &amp;ldquo;Hello, World&amp;rdquo; webapp.&lt;/p>&lt;pre>&lt;code>$ cat pod.yamlapiVersion: v1kind: Pod # 1metadata: name: helloworld # 2 labels: app: helloworldspec: # 3 containers: - image: gvelrajan/hello-world:v1.0 # 4 imagePullPolicy: Always name: helloworld # 5 ports: - containerPort: 80 # 6&lt;/code>&lt;/pre>&lt;p>&lt;strong>Description of various fields in the YAML file:&lt;/strong>&lt;/p>&lt;p>#1 kind:&lt;/p>&lt;p>#2 name &amp;amp; labels&lt;/p>&lt;p>#3 spec&lt;/p>&lt;p>#4 image&lt;/p>&lt;p>#5 name&lt;/p>&lt;p>#6 containerPort&lt;/p>&lt;p>Request the Kubernetes master node to create a Kubernetes Pod by executing the following command in the master node.&lt;/p>&lt;pre>&lt;code>master$ kubectl create -f pod.yamlpod/helloworld created&lt;/code>&lt;/pre>&lt;p>Check if the pod has been created.&lt;/p>&lt;pre>&lt;code>master$ kubectl get podNAME READY STATUS RESTARTS AGEhelloworld 1/1 Running 0 10s&lt;/code>&lt;/pre>&lt;p>To know more about the pod, use the &amp;ldquo;Kubectl describe pod&amp;rdquo; command as shown below.&lt;/p>&lt;pre>&lt;code>master$ kubectl describe pod helloworldName: helloworldNamespace: defaultNode: instance-2/10.160.0.3Start Time: Sat, 11 Aug 2018 05:14:09 +0000Labels: app=helloworldAnnotations: Status: RunningIP: 192.168.56.18Containers: helloworld: Container ID: docker://91458eb52daaff0eec5df25fd9da004e721e9c56e3b06f1715b31821178ed155 Image: gvelrajan/hello-world:v2.0 Image ID: docker-pullable://gvelrajan/hello-world@sha256:6d15f54fce820eb2628a943734a60cbc042eda1e696d97b63f159f88b0269f91 Port: 80/TCP Host Port: 0/TCP State: Running Started: Sat, 11 Aug 2018 05:14:14 +0000 Ready: True Restart Count: 0 Environment: Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-n8l4x (ro)Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-n8l4x: Type: Secret (a volume populated by a Secret) SecretName: default-token-n8l4x Optional: falseQoS Class: BestEffortNode-Selectors: Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300sEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 24s default-scheduler Successfully assigned default/helloworld to instance-2 Normal Pulling 23s kubelet, instance-2 pulling image &amp;quot;gvelrajan/hello-world:v2.0&amp;quot; Normal Pulled 19s kubelet, instance-2 Successfully pulled image &amp;quot;gvelrajan/hello-world:v2.0&amp;quot; Normal Created 19s kubelet, instance-2 Created container Normal Started 19s kubelet, instance-2 Started containermaster$&lt;/code>&lt;/pre>&lt;p>Each pod gets its own local IP address in Kubernetes. The pod above has been assigned an IP address of192.168.56.18.&lt;/p>&lt;p>What if we need to scale up the webapp we created ? Do we execute the above pod creation command again to create one more instance of the same pod?&lt;/p>&lt;pre>&lt;code>master$ kubectl create -f pod.yamlError from server (AlreadyExists): error when creating &amp;quot;pod.yaml&amp;quot;: pods &amp;quot;helloworld&amp;quot; already exists&lt;/code>&lt;/pre>&lt;p>This is not the right way to scale up pod or containers in Kubernetes.&lt;/p>&lt;p>This will serve as a nice segway to our next concept, which is Deployment.&lt;/p>&lt;h2 id="create-a-deployment">Create a Deployment&lt;/h2>&lt;p>Kubernetes uses a Deployment YAML file to specify how many replicas of a pod we need and it will take care of spawning that many number of pods across different nodes in the cluster.&lt;/p>&lt;p>We don&amp;rsquo;t have to worry about how and where the pods needs to be instantiated. Kubernetes will take care of it on our behalf.&lt;/p>&lt;p>Kubernetes will even spawn a new pod if an existing pod dies.&lt;/p>&lt;p>Here is our Kubernetes Deployment file to spawn two pods for our webapp.&lt;/p>&lt;pre>&lt;code>master$ cat deployment.yaml apiVersion: extensions/v1beta1kind: Deployment # 1metadata: name: helloworldspec: replicas: 2 # 2 minReadySeconds: 15 strategy: type: RollingUpdate # 3 rollingUpdate: maxUnavailable: 1 # 4 maxSurge: 1 # 5 template: # 6 metadata: labels: app: helloworld # 7 spec: containers: - image: gvelrajan/hello-world:v2.0 imagePullPolicy: Always # 8 name: helloworld ports: - containerPort: 80&lt;/code>&lt;/pre>&lt;p>Request Kubernetes Master to deploy our application.&lt;/p>&lt;pre>&lt;code>master$ kubectl create -f deployment.yaml deployment.extensions/helloworld created&lt;/code>&lt;/pre>&lt;p>check the status of the deployment.&lt;/p>&lt;pre>&lt;code>master$ kubectl get deploymentNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEhelloworld 2 2 2 1 24s&lt;/code>&lt;/pre>&lt;p>To know more about the Deployment, execute the &amp;ldquo;kubectl describe deployment&amp;rdquo; command.&lt;/p>&lt;pre>&lt;code>master$ kubectl get podsNAME READY STATUS RESTARTS AGEhelloworld 1/1 Running 0 3hhelloworld-59d46f94c5-ddmvr 1/1 Running 0 2hhelloworld-59d46f94c5-w8gcx 1/1 Running 0 2h&lt;/code>&lt;/pre>&lt;p>We have 3 pods of the &amp;ldquo;Hello world&amp;rdquo; application running now, including the one we created manually in the previous section.&lt;/p>&lt;p>Let&amp;rsquo;s delete that pod now.&lt;/p>&lt;pre>&lt;code>master$ kubectl delete pod helloworldpod &amp;quot;helloworld&amp;quot; deletedmaster$ kubectl get podsNAME READY STATUS RESTARTS AGEhelloworld-59d46f94c5-ddmvr 1/1 Running 0 2hhelloworld-59d46f94c5-w8gcx 1/1 Running 0 2h&lt;/code>&lt;/pre>&lt;p>Get detailed info on these pods.&lt;/p>&lt;pre>&lt;code>master$ kubectl describe podsName: helloworld-59d46f94c5-ddmvrNamespace: defaultNode: instance-2/10.160.0.3Start Time: Sat, 11 Aug 2018 05:50:01 +0000Labels: app=helloworld pod-template-hash=1580295071Annotations: Status: RunningIP: 192.168.56.23Controlled By: ReplicaSet/helloworld-59d46f94c5Containers: helloworld: Container ID: docker://7068f2006645d21824a29fc25122195f96f8b926fb6c251c3ec9b0454f1c835b Image: gvelrajan/hello-world:v4.0 Image ID: docker-pullable://gvelrajan/hello-world@sha256:a499383b22475f7b2ac805003a1c432f6d4a4ca7196e66a4dabb94f10dd061c2 Port: 80/TCP Host Port: 0/TCP State: Running Started: Sat, 11 Aug 2018 05:50:11 +0000 Ready: True Restart Count: 0 Environment: Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-n8l4x (ro)Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-n8l4x: Type: Secret (a volume populated by a Secret) SecretName: default-token-n8l4x Optional: falseQoS Class: BestEffortNode-Selectors: Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300sEvents: Name: helloworld-59d46f94c5-w8gcxNamespace: defaultNode: instance-2/10.160.0.3Start Time: Sat, 11 Aug 2018 05:50:01 +0000Labels: app=helloworld pod-template-hash=1580295071Annotations: Status: RunningIP: 192.168.56.24Controlled By: ReplicaSet/helloworld-59d46f94c5Containers: helloworld: Container ID: docker://a21882e7dd55d831fe965dfe7dfeb325165bcc13326b4c31e9c1b11dabbbd75e Image: gvelrajan/hello-world:v4.0 Image ID: docker-pullable://gvelrajan/hello-world@sha256:a499383b22475f7b2ac805003a1c432f6d4a4ca7196e66a4dabb94f10dd061c2 Port: 80/TCP Host Port: 0/TCP State: Running Started: Sat, 11 Aug 2018 05:50:08 +0000 Ready: True Restart Count: 0 Environment: Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-n8l4x (ro)Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-n8l4x: Type: Secret (a volume populated by a Secret) SecretName: default-token-n8l4x Optional: falseQoS Class: BestEffortNode-Selectors: Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300sEvents:&lt;/code>&lt;/pre>&lt;p>Kubernetes assigns each pod its own unique local IP address.&lt;/p>&lt;p>Should the client remember the IP address of each of these pods to communicate with them? Moreover the IP address is a local IP address and not a public IP address.&lt;/p>&lt;p>So how do we make clients talk to each of these pods?&lt;/p>&lt;p>That&amp;rsquo;s where the Kubernetes Service comes in handy.&lt;/p>&lt;p>A Kubernetes Services creates an abstraction on top of all the pods that provides the same functionality, so that the clients are not aware of the multiple instances of the pod. Kubernetes Service exposes a single interface for clients to communicate with the pods.&lt;/p>&lt;p>Kubernetes Service provides a load-balancers as well as a NAT service. The load-balancer helps in load-balancing the traffic equally between the pods. The NAT service provides translation between the public IP and Port and cluster local IP and port.&lt;/p>&lt;h2 id="create-a-service">Create a Service&lt;/h2>&lt;p>Next create a load-balancer and NAT service.&lt;/p>&lt;pre>&lt;code>master$ cat service.yaml apiVersion: v1kind: Service # 1metadata: name: helloworld-lbspec: externalIPs: - 10.160.0.3 type: LoadBalancer # 2 ports: - port: 80 # 3 protocol: TCP # 4 targetPort: 80 # 5 selector: # 6 app: helloworld # 7&lt;/code>&lt;/pre>&lt;p>Request Kubernetes to create the service.&lt;/p>&lt;pre>&lt;code>master$ kubectl create -f service.yaml service/helloworld-lb created&lt;/code>&lt;/pre>&lt;p>Check the status of the service.&lt;/p>&lt;pre>&lt;code>master$ kubectl get serviceNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEhelloworld-lb LoadBalancer 10.98.193.112 10.160.0.3 80:31870/TCP 9skubernetes ClusterIP 10.96.0.1 443/TCP 34d&lt;/code>&lt;/pre>&lt;p>To know the details use the describe command.&lt;/p>&lt;pre>&lt;code>master$ kubectl describe service helloworld-lbName: helloworld-lbNamespace: defaultLabels: Annotations: Selector: app=helloworldType: LoadBalancerIP: 10.98.193.112External IPs: 10.160.0.3Port: 80/TCPTargetPort: 80/TCPNodePort: 31870/TCPEndpoints: 192.168.56.23:80,192.168.56.24:80Session Affinity: NoneExternal Traffic Policy: ClusterEvents:&lt;/code>&lt;/pre>&lt;h2 id="wrap-it-up">Wrap it up&lt;/h2>&lt;p>Check the browser:&lt;/p>&lt;pre>&lt;code>$ cat test.shfor var in {1..100} do curl http://10.160.0.3 echo &amp;quot;\n&amp;quot;done$ sh test.sh \Hello World!My number is: 23Hello World!My number is: 26Hello World!My number is: 26Hello World!My number is: 23Hello World!My number is: 23Hello World!My number is: 26Hello World!My number is: 26Hello World!My number is: 23Hello World!My number is: 23Hello World!My number is: 23Hello World!My number is: 23Hello World!My number is: 23Hello World!My number is: 26Hello World!My number is: 26Hello World!My number is: 26......&lt;/code>&lt;/pre>&lt;p>In the above output, we see that the &amp;ldquo;Curl&amp;rdquo; output alternates between two different random numbers - each representing a pod in the cluster.&lt;/p>&lt;p>This proves that the Kubernetes Service is load-balancing our requests to the 2 different Pods running in the cluster.&lt;/p></description></item><item><title>Kubernetes Tutorial - Getting Started on the Basics</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-tutorial-getting-started-on-the-basics/</link><pubDate>Sun, 22 Jul 2018 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-tutorial-getting-started-on-the-basics/</guid><description>&lt;p>The goal of this Kubernetes Tutorial is to get started on the basics. We&amp;rsquo;ll explore the various building blocks of Kubernetes and the terminologies used.&lt;/p>&lt;h2 id="what-is-kubernetes" class="western">What is Kubernetes:&lt;/h2>&lt;p>Kubernetes is a container orchestration or management platform. Kubernetes is not an alternative to Docker container. Kubernetes actually used Docker to spawn containers in Kubernetes cluster nodes.&lt;/p>&lt;p>Kubernetes is an alternative to Docker Swarm or Apache Mesos.&lt;/p>&lt;h3 id="what-is-a-kubernetes-cluster" class="western">What is a Kubernetes Cluster:&lt;/h3>&lt;p>&lt;img src="http://35.238.255.76/wp-content/uploads/2018/07/module_01_cluster-300x243.jpg" alt="">{.size-medium .wp-image-1031 .aligncenter width=&amp;ldquo;300&amp;rdquo; height=&amp;ldquo;243&amp;rdquo;}&lt;/p>&lt;p>A Kubernetes Cluster is a group of nodes that are connected together to run Kubernetes Pods in them. The nodes of a cluster, share the load of the pods running in them.&lt;/p>&lt;h3 id="what-is-a-kubernetes-node">What is a Kubernetes Node:&lt;/h3>&lt;p>&lt;img src="http://35.238.255.76/wp-content/uploads/2018/07/module_03_nodes-300x257.jpg" alt="">{.size-medium .wp-image-1032 .aligncenter width=&amp;ldquo;300&amp;rdquo; height=&amp;ldquo;257&amp;rdquo;}&lt;/p>&lt;p>A Kubernetes Node is a physical or Virtual Machine, depending on the clusters.&lt;/p>&lt;p>There are two types of nodes  master nodes and worker nodes. Master nodes run the Kubernetes controller and management software. Worker nodes run Kubelet, the Kubernetes agent software. Worker nodes also run multiple Kubernetes Pods in them. Master nodes generally don&amp;rsquo;t run pods in them, unless explicitly configured to do so.&lt;/p>&lt;h3 id="what-is-a-kubernetes-pod" class="western">What is a Kubernetes Pod:&lt;/h3>&lt;p>&lt;img src="http://35.238.255.76/wp-content/uploads/2018/07/module_03_pods-300x124.jpg" alt="">{.size-medium .wp-image-1033 .aligncenter width=&amp;ldquo;300&amp;rdquo; height=&amp;ldquo;124&amp;rdquo;}&lt;/p>&lt;p>A Kubernetes Pod is a group of one or more application containers that share jstorage volumes, IP address, and information about how to run them.&lt;/p>&lt;h3 id="what-is-a-kubernetes-deployment" class="western">What is a Kubernetes Deployment&lt;/h3>&lt;p>A Kubernetes Deployment is a group of replicas of a pod. Deployment has a deployment configuration file in YAML format. The Deployment configuration instructs Kubernetes how to create, run and update instances of your applications or pods.&lt;/p>&lt;p>For example, if an application requires 5 instances of an NGINX pod, the user creates a Deployment config file and specifies the NGINX app, the version of the app, and the replicas count as 5. Kubernetes Deployment Controller will then spawn 5 instances of the pod in various worker nodes of the Kubernetes cluster.&lt;/p>&lt;pre>&lt;code>apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 5 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80&lt;/code>&lt;/pre>&lt;p>The controller will monitor the Deployment and if a pod goes down, it will respawn a new one.&lt;/p>&lt;h3 id="what-is-a-kubernetes-service" class="western">What is a Kubernetes Service&lt;/h3>&lt;p>When a Deployment has more than one replica of a pod, each pod will have its own local IP address and the port number on which the application runs. In order for the Deployment to expose a single interface to other applications that depend on the service provided by the Deployment, it needs a service abstraction. This service abstraction layer is called the Kubernetes Service. The service abstraction layer can contain a port, IP address, and/or a load-balancer.&lt;/p>&lt;p>The Kubernetes Service is configured using a configuration file, again an YAML file. The service config file specifies the load-balancer and the IP address of the service.&lt;/p>&lt;p>This is the basic principle of microservices architecture.&lt;/p>&lt;p>A sample Service configuration file is shown below.&lt;/p>&lt;pre>&lt;code>kind: ServiceapiVersion: v1metadata: name: my-servicespec: selector: app: MyApp ports: - protocol: TCP port: 80 targetPort: 9376 clusterIP: 10.0.171.239 loadBalancerIP: 78.11.24.19 type: LoadBalancerstatus: loadBalancer: ingress: - ip: 146.148.47.155&lt;/code>&lt;/pre>&lt;p>&lt;/p>&lt;p>That&amp;rsquo;s all folks about the basics on Kubernetes terminologies.&lt;/p>&lt;p>In the next tutorial, we&amp;rsquo;ll show you &lt;a href="http://35.238.255.76/index.php/2018/07/19/kubernetes-tutorial-how-to-install-kubernetes-on-ubuntu/">how to install Kubernetes on Ubuntu&lt;/a>.&lt;/p></description></item><item><title>Microservices Architecture Pros and Cons</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/microservices-architecture-pros-cons/</link><pubDate>Sun, 22 Jul 2018 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/microservices-architecture-pros-cons/</guid><description>&lt;p>Microservices Architecture has become a buzz word in the industry creating so much hype around it.&lt;/p>&lt;p>Many organizations are just puzzled and in fact, feel guilty that they have been following a completely wrong architecture all along.&lt;/p>&lt;p>Kubernetes, LXC, Docker, Docker Swarm, and Apache Mesos are few of the technologies behind this Microservice Architecture hype wave.&lt;/p>&lt;p>Netflix, Amazon, Google, Microsoft, LinkedIn, and Facebook are some of companies driving this hype wave in the industry: &amp;ldquo;It is the best architecture in the world. It works for us! It must work for you too!&amp;rdquo;&lt;/p>&lt;p>It&amp;rsquo;s high time we analyze the pros and cons of the Microservices Architecture to help those helpless souls left out wondering &amp;ldquo;Should we adapt Microservices Architecture or Not?&amp;rdquo;&lt;/p>&lt;p>In this article, we&amp;rsquo;ll analyze and argue why Microservices Architecture is not ideal for many organizations and what are some of the Microservices architecture pros and cons&lt;/p>&lt;h2 id="what-is-a-microservices-architecture" class="western">What is a Microservices Architecture&lt;/h2>&lt;p>Microservices Architecture is a modern way of building applications by breaking down a huge monolithic application into many small micro blocks such that each microblock focusses on providing just one functionality or service through well-defined interfaces such as REST, API, or a web service.&lt;/p>&lt;h2 id="where-was-microservices-architecture-born" class="western">Where was Microservices Architecture born&lt;/h2>&lt;p>Microservices Architecture was born in hyperscale web2.0 companies such as Amazon, Google, Facebook, LinkedIn, eBay, Netflix and so on, which focusses on providing online services to almost the entire world.&lt;/p>&lt;h2 id="why-did-hyperscalers-choose-microservices-architecture" class="western">Why did hyperscalers choose Microservices Architecture&lt;/h2>&lt;p>Hypersclaers chose Microservices Architecture over Monolithic Architecture for the following reasons:&lt;/p>&lt;ul>&lt;li>&lt;p>Maintaining a single large monolithic application with so many tightly coupled software modules was very difficult.&lt;/p>&lt;/li>&lt;li>&lt;p>Debugging problems in a monolithic application became a nightmare, when the sequence of function calls spanned more than one module of the application.&lt;/p>&lt;/li>&lt;li>&lt;p>With more than 1000 engineers working on a single application, the hyperscalers found it very hard to prevent the affect of a software change done by one team in one module on the rest of the modules in the application.&lt;/p>&lt;/li>&lt;li>&lt;p>Moreover, predicting the release date of an application software became almost impossible. Critical bugs found in just one module, close to a release date (release stopper or gating bugs), affect the release date of the entire application.&lt;/p>&lt;/li>&lt;li>&lt;p>Couldn&amp;rsquo;t scale up or scale down the monolithic application based on demand. A new instance of the entire application stack needs to be created even though the demand is high only for just one of the many functionalities provided by the application. Scaling up or scaling down a specific module that provides a specific functionality in an monolithic application was not possible.&lt;/p>&lt;/li>&lt;/ul>&lt;h2 id="microservices-architecture-pros-and-cons">Microservices Architecture Pros and Cons&lt;/h2>&lt;h3 id="what-are-the-pros-of-using-microservices-architecture" class="western">What are the pros of using Microservices Architecture&lt;/h3>&lt;ul>&lt;li>&lt;p>It is very easy to design, develop, maintain and debug a very small, completely independent application module that has just one service to provide to rest of the application using a well-defined interface.&lt;/p>&lt;/li>&lt;li>&lt;p>With the boundaries and interfaces between microservices well-defined, organizations with a large development team, say in 1000&amp;rsquo;s, can now divide the one big team into small teams that develop, test and own just their microservice module alone.&lt;/p>&lt;/li>&lt;li>&lt;p>Each microservice module of an appliation can be released completely independent of the rest of the application modules.&lt;/p>&lt;/li>&lt;li>&lt;p>Each microservice module can be upgraded in the production network completely independent of the rest of the application modules.&lt;/p>&lt;/li>&lt;li>&lt;p>And more importantly, each microservice module can be scaled up or scaled down completely independent of the rest of the application modules.&lt;/p>&lt;/li>&lt;/ul>&lt;h3 id="what-are-the-cons-of-using-microservices-architecture" class="western">What are the cons of using Microservices Architecture&lt;/h3>&lt;ul>&lt;li>&lt;p>The biggest disadvantage or overhead of Microservices Architecture is the requirement to define a well-defined interface for communication between different microservices of an application. Creating a well-defined interface between one microservice to another involves a overhead and costs some performance impact. For example, using a REST API to talk to a microservice is much slower than invoking a direct function call to a module or a service. This is because the microservices need to convert data back-and-forth between the REST format and the app native format, to communicate with each other.&lt;/p>&lt;/li>&lt;li>&lt;p>Organizations with small development team cannot afford to have such a small resource grouped on a per microservice module basis. Moreover, such small development teams cannot afford to operate in independent silos.&lt;/p>&lt;/li>&lt;li>&lt;p>It doesn&amp;rsquo;t make sense to independently release or upgrade an application that doesn&amp;rsquo;t have such a requirement. For instance, not all applications undergo frequent changes. Many organizations don&amp;rsquo;t touch a stable application for years, if it works just fine.&lt;/p>&lt;/li>&lt;/ul>&lt;h2 id="why-microservices-architecture-is-not-ideal-for-many-organization" class="western">Why Microservices Architecture is not ideal for many organization&lt;/h2>&lt;p>The functional requirments of various applications developed and used by many organizations do not resemble those of the applications developed and used by hyperscalers. Microservices Architecture is not a panacea for all application problems. For many organizations building a monolithic application still makes sense. The cons of the Microservices architecture listed above may overshadow any pros of the architecture realized by such applications.&lt;/p>&lt;blockquote>&lt;p>Nevertheless, applications built on monolithic architecture, could still benefit from advances in server virtualization technologies such as containerization, and advances in devops best practices such as continuous integration and continuous delivery.&lt;/p>&lt;/blockquote>&lt;h2 id="heading">&lt;/h2>&lt;h2 id="conclusion" class="western">Conclusion:&lt;/h2>&lt;p>In conclusion, Microservice architecture shouldn&amp;rsquo;t be blindly inherited into all applications developed in an organization. You need to analyze them on a case-by-case basis, weigh the pros and cons, and make a decision on the architecture. The pros and cons of the Microservices architecture listed in this article could guide your organization in that process.&lt;/p></description></item><item><title>Docker Container Tutorial - Creating Docker Container Images</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/docker-container-tutorial-creating-docker-container-images/</link><pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/docker-container-tutorial-creating-docker-container-images/</guid><description>&lt;h2 id="how-to-create-docker-images">How to create Docker images:&lt;/h2>&lt;p>In this tutorial we&amp;rsquo;ll see how to create Docker container images. There are two methods to create a docker image:&lt;/p>&lt;ul>&lt;li>Manual method&lt;/li>&lt;li>Dockerfile method&lt;/li>&lt;/ul>&lt;h3 id="manual-method">Manual Method:&lt;/h3>&lt;p>First let&amp;rsquo;s pull and run a latest version of ubuntu container.&lt;/p>&lt;pre>&lt;code>$ docker container run -it ubuntuUnable to find image 'ubuntu:latest' locallylatest: Pulling from library/ubuntu6b98dfc16071: Pull complete 4001a1209541: Pull complete 6319fc68c576: Pull complete b24603670dc3: Pull complete 97f170c87c6f: Pull complete Digest: sha256:5f4bdc3467537cbbe563e80db2c3ec95d548a9145d64453b06939c4592d67b6dStatus: Downloaded newer image for ubuntu:latestroot@b549dd9ccb9e:/# &lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s modify the contents of the container root file system by creating a new file named &amp;ldquo;testfile&amp;rdquo;.&lt;/p>&lt;pre>&lt;code>root@b549dd9ccb9e:/# echo &amp;quot;It's fun to learn Docker containers&amp;quot; &amp;gt; testfile root@b549dd9ccb9e:/# cat testfile It's fun to learn Docker containers root@b549dd9ccb9e:/# exit exit $ docker ps -a | grep ubuntu b549dd9ccb9e ubuntu &amp;quot;/bin/bash&amp;quot; 3 minutes ago Exited (0) 3 minutes ago agitated_kepler $&lt;/code>&lt;/pre>&lt;p>Now let&amp;rsquo;s check what changes we have made in the ubuntu container using the following diff command.&lt;/p>&lt;pre>&lt;code>$ docker container diff b549C /rootA /root/.bash_historyA /testfile$&lt;/code>&lt;/pre>&lt;p>We could commit the changes we have made in this ubuntu container and make it an image.&lt;/p>&lt;pre>&lt;code>$ docker container commit b549sha256:ce1fc66062791dbda9056d74ad5873a53dffd558ac4aa23bd44e51af8fd8b850$ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE ce1fc6606279 23 seconds ago 81.2MBubuntu latest 113a43faa138 2 weeks ago 81.2MB&lt;/code>&lt;/pre>&lt;p>The commit assigned a unique image id(ce1fc6606279) to the modified ubuntu image. Let&amp;rsquo;s associate a name &amp;ldquo;&lt;em>funtolearn&lt;/em>&amp;rdquo; with the image.&lt;/p>&lt;pre>&lt;code>$ docker image tag ce1f funtolearn:latest$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEfuntolearn latest ce1fc6606279 10 minutes ago 81.2MBubuntu latest 113a43faa138 2 weeks ago 81.2MB&lt;/code>&lt;/pre>&lt;p>We could now distribute this image to others so that they could spawn a container using this image and it would contain the &lt;em>testfile&lt;/em> we created.&lt;/p>&lt;pre>&lt;code>$ docker container run -it funtolearnroot@665193b64648:/# lsbin dev home lib64 mnt proc run srv testfile usrboot etc lib media opt root sbin sys tmp varroot@665193b64648:/# cat testfile It's fun to learn Docker containersroot@665193b64648:/# exitexit$&lt;/code>&lt;/pre>&lt;p>To summarize, what we did in this exercise was we pulled a base version of ubuntu container and then manually added a file. We then created a new docker image by committing this modified container workspace and gave it a name.&lt;/p>&lt;h3 id="dockerfile-method">Dockerfile Method:&lt;/h3>&lt;p>First let&amp;rsquo;s create our &lt;em>testfile&lt;/em> and add some contents to it.&lt;/p>&lt;pre>&lt;code>$ echo &amp;quot;It's SO MUCH FUN to learn Docker containers&amp;quot; &amp;gt; testfile&lt;/code>&lt;/pre>&lt;p>Next let&amp;rsquo;s create a &lt;em>Dockerfile&lt;/em> containing the following instructions to build a Docker image.&lt;/p>&lt;pre>&lt;code>$ cat Dockerfile FROM ubuntuCOPY ./testfile /var/localWORKDIR /var/localCMD [&amp;quot;cat&amp;quot;, &amp;quot;testfile&amp;quot;]&lt;/code>&lt;/pre>&lt;p>Here is what the above Dockerfile asks Docker to do: Take the latest version of ubuntu image as the base, copy the &lt;em>testfile&lt;/em> in the host directory to the container&amp;rsquo;s /var/local directory, change the working directory of the container to /var/local and finally execute the command &amp;ldquo;cat testfile&amp;rdquo;.&lt;/p>&lt;p>Let&amp;rsquo;s create the docker image using the below command.&lt;/p>&lt;pre>&lt;code>$ docker image build -t muchfuntolearn .Sending build context to Docker daemon 17.92kBStep 1/4 : FROM ubuntu ---&amp;gt; 113a43faa138Step 2/4 : COPY ./testfile /var/local ---&amp;gt; ed771f3dc583Step 3/4 : WORKDIR /var/localRemoving intermediate container b7ba093f07d0 ---&amp;gt; df81918abdcaStep 4/4 : CMD [&amp;quot;cat&amp;quot;, &amp;quot;testfile&amp;quot;] ---&amp;gt; Running in 7e782fecd2cbRemoving intermediate container 7e782fecd2cb ---&amp;gt; b192a036aa94Successfully built b192a036aa94Successfully tagged muchfuntolearn:latest&lt;/code>&lt;/pre>&lt;p>The &amp;ldquo;.&amp;rdquo; in the above command specifies the PATH to find the &amp;ldquo;Dockerfile&amp;rdquo;. Let&amp;rsquo;s check if the image has been created in the name.&lt;/p>&lt;pre>&lt;code>$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEmuchfuntolearn latest b192a036aa94 15 seconds ago 81.2MBfuntolearn latest ce1fc6606279 About an hour ago 81.2MBubuntu latest 113a43faa138 2 weeks ago 81.2MB&lt;/code>&lt;/pre>&lt;p>If we run the container using the newly created image we&amp;rsquo;ll see the contents of the &lt;em>testfile&lt;/em> getting displayed.&lt;/p>&lt;pre>&lt;code>$ docker container run muchfuntolearnIt's SO MUCH FUN to learn Docker containers&lt;/code>&lt;/pre>&lt;p>The beauty of the Dockerfile method is that it enables us to build Docker images with many variations quickly by just editing the Dockerfile.&lt;/p>&lt;p>An interesting fact about Docker images is that it is made of many layers. We could display the various layers in an image, for eg: the base ubuntu image, using the following command.&lt;/p>&lt;pre>&lt;code>$ docker image history ubuntuIMAGE CREATED CREATED BY SIZE COMMENT113a43faa138 2 weeks ago /bin/sh -c #(nop) CMD [&amp;quot;/bin/bash&amp;quot;] 0B 2 weeks ago /bin/sh -c mkdir -p /run/systemd &amp;amp;&amp;amp; echo 'do 7B 2 weeks ago /bin/sh -c sed -i 's/^#\s*\(deb.*universe\)$ 2.76kB 2 weeks ago /bin/sh -c rm -rf /var/lib/apt/lists/* 0B 2 weeks ago /bin/sh -c set -xe &amp;amp;&amp;amp; echo '#!/bin/sh' &amp;gt; / 745B 2 weeks ago /bin/sh -c #(nop) ADD file:28c0771e44ff530db 81.1MB&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s check if the image we created has more layers added on top of the ubuntu image layers.&lt;/p>&lt;pre>&lt;code>$ docker image history muchfuntolearnIMAGE CREATED CREATED BY SIZE COMMENTb192a036aa94 32 minutes ago /bin/sh -c #(nop) CMD [&amp;quot;cat&amp;quot; &amp;quot;testfile&amp;quot;] 0B df81918abdca 32 minutes ago /bin/sh -c #(nop) WORKDIR /var/local 0B ed771f3dc583 32 minutes ago /bin/sh -c #(nop) COPY file:41c23f7a0a3b38c1 44B 113a43faa138 2 weeks ago /bin/sh -c #(nop) CMD [&amp;quot;/bin/bash&amp;quot;] 0B 2 weeks ago /bin/sh -c mkdir -p /run/systemd &amp;amp;&amp;amp; echo 'do 7B 2 weeks ago /bin/sh -c sed -i 's/^#\s*\(deb.*universe\)$ 2.76kB 2 weeks ago /bin/sh -c rm -rf /var/lib/apt/lists/* 0B 2 weeks ago /bin/sh -c set -xe &amp;amp;&amp;amp; echo '#!/bin/sh' &amp;gt; / 745B 2 weeks ago /bin/sh -c #(nop) ADD file:28c0771e44ff530db 81.1MB&lt;/code>&lt;/pre>&lt;p>Each layer has an unique id associated with it and will be cached locally. The reason we see the tag in the above output is because those layers are not locally present but exists in the DockerHub repository.&lt;/p>&lt;p>If we try to rebuild the same image and give it a new version, we&amp;rsquo;ll see that Docker builds the new image using various layers of the image already present in the local cache.&lt;/p>&lt;pre>&lt;code>$ docker image build -t muchfuntolearn:v1.0 .Sending build context to Docker daemon 17.92kBStep 1/4 : FROM ubuntu ---&amp;gt; 113a43faa138Step 2/4 : COPY ./testfile /var/local ---&amp;gt; Using cache ---&amp;gt; f6500304a128Step 3/4 : WORKDIR /var/local ---&amp;gt; Using cache ---&amp;gt; cf189f932f95Step 4/4 : CMD [&amp;quot;cat&amp;quot;, &amp;quot;testfile&amp;quot;] ---&amp;gt; Using cache ---&amp;gt; 912fe16e4752Successfully built 912fe16e4752Successfully tagged muchfuntolearn:v1.0&lt;/code>&lt;/pre>&lt;p>Suppose, we would like to modify the contents of the &lt;em>testfile&lt;/em>, just like any real world application that gets changed frequently, and rebuild a new image version, we follow the same steps.&lt;/p>&lt;pre>&lt;code>$ echo &amp;quot;I want to learn more about Docker containers&amp;quot; &amp;gt;&amp;gt; testfile$ docker image build -t muchfuntolearn:v2.0 .Sending build context to Docker daemon 17.92kBStep 1/4 : FROM ubuntu ---&amp;gt; 113a43faa138Step 2/4 : COPY ./testfile /var/local ---&amp;gt; f6500304a128Step 3/4 : WORKDIR /var/localRemoving intermediate container 0bf6987cef00 ---&amp;gt; cf189f932f95Step 4/4 : CMD [&amp;quot;cat&amp;quot;, &amp;quot;testfile&amp;quot;] ---&amp;gt; Running in 3b958d65350dRemoving intermediate container 3b958d65350d ---&amp;gt; 912fe16e4752Successfully built 912fe16e4752Successfully tagged muchfuntolearn:v2.0$$ docker container run muchfuntolearn:v2.0It's SO MUCH FUN to learn Docker containersI want to learn more about Docker containers&lt;/code>&lt;/pre>&lt;h2 id="heading">&lt;/h2>&lt;h2 id="wrap-it-up">Wrap it up:&lt;/h2>&lt;p>Finally, let&amp;rsquo;s wrap up this exercise by creating a docker image for a simple real world web application.&lt;/p>&lt;p>Here is a simple nodejs hello-world app that we&amp;rsquo;ll use. All it does is prints &amp;ldquo;Hello World!&amp;rdquo; on the webpage.&lt;/p>&lt;pre>&lt;code>$ cat app.js var http = require('http');//create a server object:http.createServer(function (req, res) { res.writeHead(200, {'Content-Type': 'text/html'}); res.write('Hello World!'); //write a response to the client res.end(); //end the response}).listen(3000); //the server object listens on port 8080$&lt;/code>&lt;/pre>&lt;p>Here is the Dockerfile we&amp;rsquo;ll use to build the app container image:&lt;/p>&lt;pre>&lt;code>$ cat Dockerfile FROM alpine:latestRUN apk update &amp;amp;&amp;amp; apk add nodejsRUN mkdir -p /usr/src/appCOPY . /usr/src/appWORKDIR /usr/src/appEXPOSE 3000 CMD [&amp;quot;node&amp;quot;,&amp;quot;app.js&amp;quot;]&lt;/code>&lt;/pre>&lt;p>The Dockerfile tells Docker to install nodejs on an alpine base container image and then copy the app from the host directory to the container image work directory /usr/src/app and finally ask it to run the command &amp;ldquo;node app.js&amp;rdquo;. The EXPOSE command tells Docker to allow traffic to port 3000.&lt;/p>&lt;p>Let&amp;rsquo;s build the version 0.1 of the &lt;em>hello-world&lt;/em> container image now:&lt;/p>&lt;pre>&lt;code>$ docker image build -t hello-world:v0.1 .Sending build context to Docker daemon 17.92kBStep 1/7 : FROM alpine:latest ---&amp;gt; 3fd9065eaf02Step 2/7 : RUN apk update &amp;amp;&amp;amp; apk add nodejs ---&amp;gt; Using cache ---&amp;gt; 928e995e38f4Step 3/7 : RUN mkdir -p /usr/src/app ---&amp;gt; Using cache ---&amp;gt; 5c3b1d4fa148Step 4/7 : COPY . /usr/src/app ---&amp;gt; Using cache ---&amp;gt; dab04cdb33d6Step 5/7 : WORKDIR /usr/src/app ---&amp;gt; Using cache ---&amp;gt; c3fdc62b6f66Step 6/7 : EXPOSE 3000 ---&amp;gt; Using cache ---&amp;gt; dbe68495bccaStep 7/7 : CMD [&amp;quot;node&amp;quot;,&amp;quot;app.js&amp;quot;] ---&amp;gt; Using cache ---&amp;gt; 74a062a6a3c3Successfully built 74a062a6a3c3Successfully tagged hello-world:v0.1&lt;/code>&lt;/pre>&lt;p>Use the following command to run the &lt;em>hello-world&lt;/em>container app. The command requests Docker to run the container in the background as a daemon [-d] and map the host port [8000] to the docker port [3000]. The nodejs HTTP server app will listen on TCP port 3000 in the container&amp;rsquo;s network namespace.&lt;/p>&lt;pre>&lt;code>$ docker run -d -p 8000:3000 hello-world:v0.1 b6bbc0790c69ee95d9e977caef329f5b166c28de9f883614769058b781419bc7&lt;/code>&lt;/pre>&lt;p>The option [-p] in the above command is the port mapping option. It requests Docker daemon to add a port NAT entry in the host&amp;rsquo;s NAT table as shown below:&lt;/p>&lt;pre>&lt;code>$ sudo iptables -t nat --list Chain PREROUTING (policy ACCEPT)target prot opt source destination DOCKER all -- anywhere anywhere ADDRTYPE match dst-type LOCALChain INPUT (policy ACCEPT)target prot opt source destinationChain OUTPUT (policy ACCEPT)target prot opt source destination DOCKER all -- anywhere !localhost/8 ADDRTYPE match dst-type LOCALChain POSTROUTING (policy ACCEPT)target prot opt source destination MASQUERADE all -- 172.17.0.0/16 anywhere MASQUERADE tcp -- 172.17.0.2 172.17.0.2 tcp dpt:3000Chain DOCKER (2 references)target prot opt source destination RETURN all -- anywhere anywhere DNAT tcp -- anywhere anywhere tcp dpt:8000 to:172.17.0.2:3000&lt;/code>&lt;/pre>&lt;p>Check if the container is running on the specified port.&lt;/p>&lt;pre>&lt;code>$ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb6bbc0790c69 hello-world:v0.1 &amp;quot;node app.js&amp;quot; 2 minutes ago Up 2 minutes 0.0.0.0:8000-&amp;gt;3000/tcp agitated_zhukovsky&lt;/code>&lt;/pre>&lt;p>Now if we open a browser in the host machine and point it to the url: http://localhost:8000 it will print the message &amp;ldquo;Hello World!&amp;rdquo;&lt;/p>&lt;p>&lt;img src="https://ganeshtechblog.files.wordpress.com/2018/06/hello-world1.png" alt="Hello-World">{.alignnone .size-full .wp-image-903 width=&amp;ldquo;2564&amp;rdquo; height=&amp;ldquo;926&amp;rdquo;}&lt;/p>&lt;p>That&amp;rsquo;s all folks ! Wasn&amp;rsquo;t it so much fun to learn how to build Docker images? Pat yourselves on the back! You have completed part-2 of the Docker container tutorial.&lt;/p>&lt;p>&lt;/p></description></item><item><title>Docker Container Tutorial - Docker Swarms</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/docker-container-tutorial-docker-swarms/</link><pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/docker-container-tutorial-docker-swarms/</guid><description>&lt;p>Docker Swarm is a very useful management tool when we have more than one container and more than one host to manage.&lt;/p>&lt;p>In other words, when we have a &amp;ldquo;swarm of containers&amp;rdquo; it is nearly impossible to manage each one of them individually. We need a management software or tool to manage a swarm of containers. Docker Swarm is a software tool that does the job.&lt;/p>&lt;p>In this tutorial, we&amp;rsquo;ll discuss how to work with Docker Swarms to manage Docker Containers.&lt;/p>&lt;h3 id="initializing-docker-swarm">Initializing Docker Swarm:&lt;/h3>&lt;p>A Docker swarm consists of manager and worker nodes. There can be multiple manager nodes to provide redundancy. One of the manager nodes will become a leader node and all others will be in backup mode. Manager nodes are used for configuring and managing the swarm nodes. Similarly, there can be multiple worker nodes to load-balance the container workloads between them.&lt;/p>&lt;p>Let&amp;rsquo;s initialize a Docker Swarm and create a manager node. In this tutorial, we will have only one manager node and one worker node. Each node can be a physical host or a virtual machine host.&lt;/p>&lt;pre>&lt;code>manager$ docker swarm init --advertise-addr $(hostname -i)Swarm initialized: current node (w69ubdlip5mou1dluuvcb76qx) is now a manager.To add a worker to this swarm, run the following command:docker swarm join --token SWMTKN-1-3kdognqu7fy07fxkp5jhl30752dhmxtdevg3bu3cfm64c185fm-8vtuu2y46dgtgxb8apajzeh0z 192.168.0.13:2377To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.&lt;/code>&lt;/pre>&lt;p>Next join the Docker swarm from the worker node using the token provided by the manager node.&lt;/p>&lt;pre>&lt;code>worker$ docker swarm join --token SWMTKN-1-3kdognqu7fy07fxkp5jhl30752dhmxtdevg3bu3cfm64c185fm-8vtuu2y46dgtgxb8apajzeh0z 192.168.0.13:2377This node joined a swarm as a worker.&lt;/code>&lt;/pre>&lt;p>List the nodes in the swarm by running the following command in the master node only.&lt;/p>&lt;pre>&lt;code>manager$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONw69ubdlip5mou1dluuvcb76qx * node1 Ready Active Leader 18.03.1-ce2ojpw5poh6s33ep9e4o6q509v node2 Ready Active 18.03.1-ce&lt;/code>&lt;/pre>&lt;p>The above output shows that the manager node is also the leader node because there is only one manager node configured in the swarm. Typically production environment would have multiple manager nodes for redundancy.&lt;/p>&lt;p>If we execute the same command in the worker node it will complain that it is not a master to execute any swarm management commands.&lt;/p>&lt;pre>&lt;code>worker$ docker node lsError response from daemon: This node is not a swarm manager. Worker nodes can't be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager.&lt;/code>&lt;/pre>&lt;p>&lt;/p>&lt;h3 id="clone-the-voting-app">Clone the Voting App:&lt;/h3>&lt;p>Let&amp;rsquo;s download the sample voting app from the Github. Please make sure you run this command in the master node.&lt;/p>&lt;pre>&lt;code>manager$ git clone https://github.com/docker/example-voting-appCloning into 'example-voting-app'...remote: Counting objects: 482, done.remote: Total 482 (delta 0), reused 0 (delta 0), pack-reused 482Receiving objects: 100% (482/482), 229.43 KiB | 13.50 MiB/s, done.Resolving deltas: 100% (179/179), done.manager$ cd example-voting-appmanager$ lsLICENSE docker-stack.ymlMAINTAINERS dockercloud.ymlREADME.md k8s-specificationsarchitecture.png resultdocker-compose-javaworker.yml votedocker-compose-simple.yml workerdocker-compose.yml&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s check the contents of the docker-stack.yml file. It will have instructions on how each services in a stack should be instantiated and configured.&lt;/p>&lt;pre>&lt;code>manager$ cat docker-stack.ymlversion: &amp;quot;3&amp;quot;services:redis: image: redis:alpine ports: - &amp;quot;6379&amp;quot; networks: - frontend deploy: replicas: 1 update_config: parallelism: 2 delay: 10s restart_policy: condition: on-failure db: image: postgres:9.4 volumes: - db-data:/var/lib/postgresql/data networks: - backend deploy: placement: constraints: [node.role == manager] vote: image: dockersamples/examplevotingapp_vote:before ports: - 5000:80 networks: - frontend depends_on: - redis deploy: replicas: 2 update_config: parallelism: 2 restart_policy: condition: on-failure result: image: dockersamples/examplevotingapp_result:before ports: - 5001:80 networks: - backend depends_on: - db deploy: replicas: 1 update_config: parallelism: 2 delay: 10s restart_policy: condition: on-failureworker: image: dockersamples/examplevotingapp_worker networks: - frontend - backend deploy: mode: replicated replicas: 1 labels: [APP=VOTING] restart_policy: condition: on-failure delay: 10s max_attempts: 3 window: 120s placement: constraints: [node.role == manager]visualizer: image: dockersamples/visualizer:stable ports: - &amp;quot;8080:8080&amp;quot; stop_grace_period: 1m30s volumes: - &amp;quot;/var/run/docker.sock:/var/run/docker.sock&amp;quot; deploy: placement: constraints: [node.role == manager]networks: frontend: backend:volumes: db-data:&lt;/code>&lt;/pre>&lt;h3 id="deploy-a-stack">Deploy a Stack:&lt;/h3>&lt;p>A stack is a group of services that are usually deployed together, such as a frontend service and a backend service that together forms an application stack. Each service can be made up of one or more container instances called tasks. For example, there can be multiple instances of the frontend container spawned to handle more traffic. Each such container instance is a task of the frontend service.&lt;/p>&lt;p>Let&amp;rsquo;s deploy the stack from the manager console using the docker-stack.yml file.&lt;/p>&lt;pre>&lt;code>manager$ docker stack deploy --compose-file=docker-stack.yml voting_stackCreating network voting_stack_backendCreating network voting_stack_defaultCreating network voting_stack_frontendCreating service voting_stack_voteCreating service voting_stack_resultCreating service voting_stack_workerCreating service voting_stack_visualizerCreating service voting_stack_redisCreating service voting_stack_dbmanager$ docker stack lsNAME SERVICESvoting_stack 6&lt;/code>&lt;/pre>&lt;p>Here is the command to list the services spawned for a given stack. Note in the below output that the &amp;ldquo;voting_stack_vote&amp;rdquo; service has 2 replicas, as we have instructed in the docker-stack.yml file. These two replicas are the 2 tasks of the &amp;ldquo;voting_stack_vote&amp;rdquo; service.&lt;/p>&lt;pre>&lt;code>manager$ docker stack services voting_stackID NAME MODE REPLICAS IMAGE PORTS070sq2hxkv5i voting_stack_db replicated 1/1 postgres:9.47p20869dlwdq voting_stack_vote replicated 2/2 dockersamples/examplevotingapp_vote:before *:5000-&amp;gt;80/tcplo0sdggwktfh voting_stack_redis replicated 1/1 redis:alpine *:30000-&amp;gt;6379/tcpm02xt0qcqyvd voting_stack_visualizer replicated 1/1 dockersamples/visualizer:stable *:8080-&amp;gt;8080/tcpmaxc947eok0l voting_stack_result replicated 1/1 dockersamples/examplevotingapp_result:before *:5001-&amp;gt;80/tcpsmhw70zh0qb1 voting_stack_worker replicated 1/1 dockersamples/examplevotingapp_worker:latest&lt;/code>&lt;/pre>&lt;p>To display the replicas ( or tasks) associated with a service, execute the following command.&lt;/p>&lt;pre>&lt;code>manager$ docker service ps voting_stack_voteID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSaabui7rdmc4v voting_stack_vote.1 dockersamples/examplevotingapp_vote:before node1 Running Running 7 minutes agojd1wzdyq3bw5 voting_stack_vote.2 dockersamples/examplevotingapp_vote:before node2 Running Running 7 minutes ago&lt;/code>&lt;/pre>&lt;p>If for some reason, after casting votes, people throng the result website &amp;ldquo;voting_stack_result&amp;rdquo; in the voting app above, we may have to scale up that service. To scale up a service, increase the number of replicas or tasks or containers spawned for the service, using the following command.&lt;/p>&lt;pre>&lt;code>manager$ docker service scale voting_stack_result=5voting_stack_result scaled to 5overall progress: 5 out of 5 tasks1/5: running2/5: running3/5: running4/5: running5/5: runningverify: Service converged&lt;/code>&lt;/pre>&lt;p>Similarly, you can scaled down a service by reduce the count in the above command.&lt;/p>&lt;pre>&lt;code>manager$ docker service scale voting_stack_result=3voting_stack_result scaled to 3overall progress: 3 out of 3 tasks1/3: running2/3: running3/3: runningverify: Service convergedmanager$ docker stack services voting_stackID NAME MODE REPLICAS IMAGE PORTS070sq2hxkv5i voting_stack_db replicated 1/1 postgres:9.47p20869dlwdq voting_stack_vote replicated 2/2 dockersamples/examplevotingapp_vote:before *:5000-&amp;gt;80/tcplo0sdggwktfh voting_stack_redis replicated 1/1 redis:alpine *:30000-&amp;gt;6379/tcpm02xt0qcqyvd voting_stack_visualizer replicated 1/1 dockersamples/visualizer:stable *:8080-&amp;gt;8080/tcpmaxc947eok0l voting_stack_result replicated 3/3 dockersamples/examplevotingapp_result:before *:5001-&amp;gt;80/tcpsmhw70zh0qb1 voting_stack_worker replicated 1/1 dockersamples/examplevotingapp_worker:latestmanager$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES791115ff9838 dockersamples/examplevotingapp_worker:latest &amp;quot;/bin/sh -c 'dotnet &amp;quot; About an hour ago Up About an hour voting_stack_worker.1.ijtteerugzkrzf99mjcpqnwqm2e5d745cc699 postgres:9.4 &amp;quot;docker-entrypoint.s&amp;quot; About an hour ago Up About an hour 5432/tcp voting_stack_db.1.yhby06qsc53ryks4at9xkhzsu9cb6543aa5ae dockersamples/visualizer:stable &amp;quot;npm start&amp;quot; About an hour ago Up About an hour 8080/tcp voting_stack_visualizer.1.z7937i0rkwi81p35zsn67s6fo7a57ad0a16a1 dockersamples/examplevotingapp_result:before &amp;quot;node server.js&amp;quot; About an hour ago Up About an hour 80/tcp voting_stack_result.1.ibhhdgrgqjfmsi4fgk4d2tnpe4c729d9cf487 dockersamples/examplevotingapp_vote:before &amp;quot;gunicorn app:app -b&amp;quot; About an hour ago Up About an hour 80/tcp voting_stack_vote.1.aabui7rdmc4viz9t76x6k93dnworker$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES079a5a4506dd dockersamples/examplevotingapp_result:before &amp;quot;node server.js&amp;quot; 9 minutes ago Up 9 minutes 80/tcp voting_stack_result.3.ddozkt7a528ek69479jl5woud8f25b2d0512f dockersamples/examplevotingapp_result:before &amp;quot;node server.js&amp;quot; 9 minutes ago Up 9 minutes 80/tcp voting_stack_result.2.5tmjvecjtlx8pnzy52y98dxx55811c9b7ac84 redis:alpine &amp;quot;docker-entrypoint.s&amp;quot; About an hour ago Up About an hour 6379/tcp voting_stack_redis.1.s6ky4lq93m3moa2oc7i5m612a7aa7718315d9 dockersamples/examplevotingapp_vote:before &amp;quot;gunicorn app:app -b&amp;quot; About an hour ago Up About an hour 80/tcp voting_stack_vote.2.jd1wzdyq3bw5x26aq3jqlrvrb&lt;/code>&lt;/pre>&lt;h3 id="heading">&lt;/h3>&lt;h3 id="drain-a-node-and-pull-it-out-of-service">Drain a node and pull it out of service:&lt;/h3>&lt;p>In the real world, we need to shutdown nodes for upgrade and periodic maintenance. Docker allows users to pull out nodes from a swarm and shut the node down. It involves two steps: first, we need to move the services away from the node to other nodes in the swarm; second, we need to pull out the node from the swarm.&lt;/p>&lt;p>To drain the services out of a node execute the following command.&lt;/p>&lt;pre>&lt;code>manager$ docker node update --availability drain 2ojpw5poh6s33ep9e4o6q509v2ojpw5poh6s33ep9e4o6q509v&lt;/code>&lt;/pre>&lt;p>check the worker node and see if the services have been drained out of it.&lt;/p>&lt;pre>&lt;code>worker$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES&lt;/code>&lt;/pre>&lt;p>Check the manager node to see if all the services from the worker node have been moved to it.&lt;/p>&lt;pre>&lt;code>manager$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1c8a7935a8dc dockersamples/examplevotingapp_worker:latest &amp;quot;/bin/sh -c 'dotnet &amp;quot; 2 minutes ago Up 2 minutes voting_stack_worker.1.v2a312sb73eur5ncf5b4g60073882b7a45c1c redis:alpine &amp;quot;docker-entrypoint.s&amp;quot; 2 minutes ago Up 2 minutes 6379/tcp voting_stack_redis.1.gvys2udrcgcnm2eqfnbzj0jhg83f2126a17b6 dockersamples/examplevotingapp_vote:before &amp;quot;gunicorn app:app -b&amp;quot; 2 minutes ago Up 2 minutes 80/tcp voting_stack_vote.2.qfwsop38s23iqfwiz762vbd91b17ba659bb3e dockersamples/examplevotingapp_result:before &amp;quot;node server.js&amp;quot; 2 minutes ago Up 2 minutes 80/tcp voting_stack_result.3.ot6qinxotkwxt9522ywg7swds75d2c009233f dockersamples/examplevotingapp_result:before &amp;quot;node server.js&amp;quot; 2 minutes ago Up 2 minutes 80/tcp voting_stack_result.2.pnzjtjjcrjvuvad5dg4dsfdvf2e5d745cc699 postgres:9.4 &amp;quot;docker-entrypoint.s&amp;quot; About an hour ago Up About an hour 5432/tcp voting_stack_db.1.yhby06qsc53ryks4at9xkhzsu9cb6543aa5ae dockersamples/visualizer:stable &amp;quot;npm start&amp;quot; About an hour ago Up About an hour 8080/tcp voting_stack_visualizer.1.z7937i0rkwi81p35zsn67s6fo7a57ad0a16a1 dockersamples/examplevotingapp_result:before &amp;quot;node server.js&amp;quot; About an hour ago Up About an hour 80/tcp voting_stack_result.1.ibhhdgrgqjfmsi4fgk4d2tnpe4c729d9cf487 dockersamples/examplevotingapp_vote:before &amp;quot;gunicorn app:app -b&amp;quot; About an hour ago Up About an hour 80/tcp voting_stack_vote.1.aabui7rdmc4viz9t76x6k93dn&lt;/code>&lt;/pre>&lt;p>Next we could pull out the worker node from the swarm.&lt;/p>&lt;pre>&lt;code>worker$ docker swarm leaveNode left the swarm.&lt;/code>&lt;/pre>&lt;p>We can shutdown the worker node now for maintenance.&lt;/p>&lt;p>That&amp;rsquo;s all folks ! You have become a Docker pro. Now you know how to create and manage Docker Swarm. Congratulations !!&lt;/p></description></item><item><title>Docker Container Tutorial - Running A Simple Docker Container</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/docker-container-tutorial-installing-docker/</link><pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/docker-container-tutorial-installing-docker/</guid><description>&lt;h2 id="how-to-install-docker-cecommunity-edition">How to install Docker CE(Community Edition)&lt;/h2>&lt;p>Docker CE is the mini version of Docker available to download for free for beginners to learn and experiment with Docker containers on a laptop.&lt;/p>&lt;p>For Linux platform, you could install Docker CE by running a simple script provided by Docker. Visit the official Docker website to find out how to install a version of Docker CE for your OS.&lt;/p>&lt;pre>&lt;code>$ curl -fsSL get.docker.com -o get-docker.sh$ sudo sh get-docker.sh&lt;/code>&lt;/pre>&lt;p>If you would like to use Docker as a non-root user, you should now consideradding your user to the docker group with something like:&lt;/p>&lt;pre>&lt;code>sudo usermod -aG docker &amp;lt;your-user-name&amp;gt;&lt;/code>&lt;/pre>&lt;p>Remember to log out and back in for this to take effect!&lt;/p>&lt;pre>&lt;code>WARNING: Adding a user to the &amp;quot;docker&amp;quot; group grants the ability to runcontainers which can be used to obtain root privileges on thedocker host. Refer to https://docs.docker.com/engine/security/security/#docker-daemon-attack-surfacefor more information.&lt;/code>&lt;/pre>&lt;p>&lt;/p>&lt;h2 id="getting-started">Getting Started:&lt;/h2>&lt;p>First check the version of docker installed on your OS:&lt;/p>&lt;pre>&lt;code>$ docker --versionDocker version 18.05.0-ce, build f150324&lt;/code>&lt;/pre>&lt;p>Now check if we have any container images locally on our installation.&lt;/p>&lt;pre>&lt;code>$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZE&lt;/code>&lt;/pre>&lt;p>Now to run a basic hello world container using docker execute the following command:&lt;/p>&lt;pre>&lt;code>$ docker run hello-worldUnable to find image 'hello-world:latest' locallylatest: Pulling from library/hello-world9bb5a5d4561a: Pull completeDigest: sha256:f5233545e43561214ca4891fd1157e1c3c563316ed8e237750d59bde73361e77Status: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps:1. The Docker client contacted the Docker daemon.2. The Docker daemon pulled the &amp;quot;hello-world&amp;quot; image from the Docker Hub.(amd64)3. The Docker daemon created a new container from that image which runs theexecutable that produces the output you are currently reading.4. The Docker daemon streamed that output to the Docker client, which sent itto your terminal.To try something more ambitious, you can run an Ubuntu container with:$ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID:https://hub.docker.com/For more examples and ideas, visit:https://docs.docker.com/engine/userguide/$&lt;/code>&lt;/pre>&lt;p>The output was verbose and self explanatory. Now check your docker images again.&lt;/p>&lt;pre>&lt;code>$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest e38bc07ac18e 2 months ago 1.85kB&lt;/code>&lt;/pre>&lt;p>Now check the status of the hello world container.&lt;/p>&lt;pre>&lt;code>$ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES&lt;/code>&lt;/pre>&lt;p>There is no output because the hello-world container finished executing after printing the message. If you wanted to still see the container that has finished execution, include the all option in the command.&lt;/p>&lt;pre>&lt;code>$ docker container ls --allCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES695c82c1b0f9 hello-world &amp;quot;/hello&amp;quot; 2 minutes ago Exited (0) 2 minutes ago jovial_bartik&lt;/code>&lt;/pre>&lt;p>Rerun the hello-world container again.&lt;/p>&lt;pre>&lt;code>$ docker run hello-worldHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps:1. The Docker client contacted the Docker daemon.2. The Docker daemon pulled the &amp;quot;hello-world&amp;quot; image from the Docker Hub.(amd64)3. The Docker daemon created a new container from that image which runs theexecutable that produces the output you are currently reading.4. The Docker daemon streamed that output to the Docker client, which sent itto your terminal.To try something more ambitious, you can run an Ubuntu container with:$ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID:https://hub.docker.com/For more examples and ideas, visit:https://docs.docker.com/engine/userguide/$&lt;/code>&lt;/pre>&lt;p>If you carefully note, this time docker was able to find the hello-world image locally on our machine and didnt try to pull it from the dockerhub again. The execution was much faster.&lt;/p>&lt;p>Now lets try something more serious. Lets run an Ubuntu container, interactively. This time lets pull the image first and then run it using the run command.&lt;/p>&lt;pre>&lt;code>$ docker pull ubuntuUsing default tag: latestlatest: Pulling from library/ubuntu6b98dfc16071: Pull complete 4001a1209541: Pull complete 6319fc68c576: Pull complete b24603670dc3: Pull complete 97f170c87c6f: Pull complete Digest: sha256:5f4bdc3467537cbbe563e80db2c3ec95d548a9145d64453b06939c4592d67b6dStatus: Downloaded newer image for ubuntu:latest&lt;/code>&lt;/pre>&lt;p>Check if we have the image in our local repo.&lt;/p>&lt;pre>&lt;code>$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEubuntu latest 113a43faa138 2 weeks ago 81.2MBhello-world latest e38bc07ac18e 2 months ago 1.85kB&lt;/code>&lt;/pre>&lt;p>Now run the container in interactive mode.&lt;/p>&lt;pre>&lt;code>$ docker run -it ubuntu bashroot@1636ea923f40:/# pwd/root@1636ea923f40:/# lsbin dev home lib64 mnt proc run srv tmp varboot etc lib media opt root sbin sys usrroot@1636ea923f40:/# hostname1636ea923f40root@1636ea923f40:/# whoamirootroot@1636ea923f40:/# &amp;gt;testfileroot@1636ea923f40:/# lsbin dev home lib64 mnt proc run srv testfile usrboot etc lib media opt root sbin sys tmp varroot@1636ea923f40:/# exitexit$&lt;/code>&lt;/pre>&lt;p>As expected, docker didnt find the ubuntu container image locally and hence it downloaded the image from the dockerhub. It started the container, executed the bash command and provided us the access to the shell. You can see that the shell prompt changed from $ to # when inside the container. Docker provided the container a unique hostname 1636ea923f40.&lt;/p>&lt;p>When we exited the bash shell, the container finished its execution.&lt;/p>&lt;pre>&lt;code>$ docker container ls --allCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1636ea923f40 ubuntu &amp;quot;bash&amp;quot; 8 minutes ago Exited (0) 8 minutes ago cocky_stallman167b2e58aa67 hello-world &amp;quot;/hello&amp;quot; 21 minutes ago Exited (0) 21 minutes ago vigorous_euler695c82c1b0f9 hello-world &amp;quot;/hello&amp;quot; 37 minutes ago Exited (0) 37 minutes ago jovial_bartik&lt;/code>&lt;/pre>&lt;p>From the output of the above command we can see that the hostname of the ubuntu container is same as its container id.&lt;/p>&lt;pre>&lt;code>$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEubuntu latest 113a43faa138 2 weeks ago 81.2MBhello-world latest e38bc07ac18e 2 months ago 1.85kB&lt;/code>&lt;/pre>&lt;p>If you noted carefully, I did something notorious inside the ubuntu container. I created a file named testfile in the root directory before I exited from it. This is just to see what happens if I restart the stopped container.&lt;/p>&lt;pre>&lt;code>$ docker container start 1636 -iroot@1636ea923f40:/# pwd/root@1636ea923f40:/# lsbin dev home lib64 mnt proc run srv testfile usrboot etc lib media opt root sbin sys tmp varroot@1636ea923f40:/# exitexit$&lt;/code>&lt;/pre>&lt;p>It is enough to just specify the unique first few digits(1636 in this case) of a container id in the command line. We dont have to specify the complete container id(1636ea923f40).&lt;/p>&lt;p>To our surprise, the testfile still exists in the root directory of the container. What does this tell us ? When a container finishes its execution, its root file system is not cleaned and removed. It gets removed only when the user makes an explicit request to remove it, by executing docker container rm 1636.&lt;/p>&lt;p>Now lets try to create one more ubuntu container, using the ubuntu image that was downloaded earlier.&lt;/p>&lt;pre>&lt;code>$ docker run -it ubuntu bashroot@1affae2c7305:/# pwd/root@1affae2c7305:/# lsbin dev home lib64 mnt proc run srv tmp varboot etc lib media opt root sbin sys usrroot@1affae2c7305:/# &amp;gt;magicfileroot@1affae2c7305:/# lsbin dev home lib64 media opt root sbin sys usrboot etc lib magicfile mnt proc run srv tmp varroot@1affae2c7305:/# hostname1affae2c7305root@1affae2c7305:/# whoamirootroot@1affae2c7305:/# exitexit$$ docker container ls --allCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1affae2c7305 ubuntu &amp;quot;bash&amp;quot; About a minute ago Exited (0) About a minute ago friendly_davinci1636ea923f40 ubuntu &amp;quot;bash&amp;quot; 34 minutes ago Exited (0) 11 minutes ago cocky_stallman167b2e58aa67 hello-world &amp;quot;/hello&amp;quot; About an hour ago Exited (0) About an hour ago vigorous_euler695c82c1b0f9 hello-world &amp;quot;/hello&amp;quot; About an hour ago Exited (0) About an hour ago jovial_bartik&lt;/code>&lt;/pre>&lt;p>I created a new ubuntu container and docker assigned it a new id, which is 1affae2c7305. As we expected docker created a new root file system for the container and hence we didnt see the testfile that we created in the first Ubuntu containers root file system. Now, I created a magicfile under the root directory.&lt;/p>&lt;p>This is just to prove the point that docker creates a new root file system when we ask it to create a new container. The file system is saved and not destroyed when the container exits. We could restart the container using the root file system whenever we want.&lt;/p>&lt;pre>&lt;code>$ docker container start 1aff -iroot@1affae2c7305:/# pwd/root@1affae2c7305:/# lsbin dev home lib64 media opt root sbin sys usrboot etc lib magicfile mnt proc run srv tmp varroot@1affae2c7305:/# hostname1affae2c7305root@1affae2c7305:/# exitexit$&lt;/code>&lt;/pre>&lt;p>Now lets remove the container with id 1affae2c7305.&lt;/p>&lt;pre>&lt;code>$ docker container rm 1aff1aff$ docker container ls --all | grep ubuntu1636ea923f40 ubuntu &amp;quot;bash&amp;quot; About an hour ago Exited (0) 28 minutes ago cocky_stallman&lt;/code>&lt;/pre>&lt;p>If we try to start the container now, it will fail.&lt;/p>&lt;pre>&lt;code>$ docker container start 1aff -iError: No such container: 1aff&lt;/code>&lt;/pre>&lt;p>Now, lets try removing the docker image present locally in our machine.&lt;/p>&lt;pre>&lt;code>$ docker image rm ubuntuError response from daemon: conflict: unable to remove repository reference &amp;quot;ubuntu&amp;quot; (must force) - container 1636ea923f40 is using its referenced image 113a43faa138&lt;/code>&lt;/pre>&lt;p>It throws an error saying container 1636 is still referencing the image. We should delete all the containers spawned using the ubuntu image before we could delete it. So lets delete the container 1636 also.&lt;/p>&lt;pre>&lt;code>$ docker container rm 16361636$ docker container ls --all | grep ubuntu$ $ docker image rm ubuntuUntagged: ubuntu:latestUntagged: ubuntu@sha256:5f4bdc3467537cbbe563e80db2c3ec95d548a9145d64453b06939c4592d67b6dDeleted: sha256:113a43faa1382a7404681f1b9af2f0d70b182c569aab71db497e33fa59ed87e6Deleted: sha256:a9fa410a3f1704cd9061a802b6ca6e50a0df183cb10644a3ec4cac9f6421677aDeleted: sha256:b21f75f60422609fa79f241bf80044e6e133dd0662851afb12dacd22d199233aDeleted: sha256:038d2d2aa4fb988c06f04e3af208cc0c1dbd9703aa04905ade206d783e7bc06aDeleted: sha256:b904d425ea85240d6af5a6c6f145e05d5e0127f547f8eb4f68552962df846e81Deleted: sha256:db9476e6d963ed2b6042abef1c354223148cdcdbd6c7416c71a019ebcaea0edb$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest e38bc07ac18e 2 months ago 1.85kB$&lt;/code>&lt;/pre>&lt;p>Now the ubuntu image is successfully removed from our local repository.&lt;/p>&lt;p>Thats all the basic Docker container stuff you should be aware of folks.&lt;/p>&lt;p>Congratulations! You have completed the basic Docker tutorial.&lt;/p>&lt;p>&lt;/p></description></item><item><title>How to Install SSL Certificate in WordPress VM</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/how-to-install-ssl-certificate-in-wordpress/</link><pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/how-to-install-ssl-certificate-in-wordpress/</guid><description>&lt;p>In this article, we&amp;rsquo;ll discuss how to install SSL Certificate in any Linux based VM, including the WordPress VM running in Google Cloud Platform(GCP)&lt;/p>&lt;p>It doesn&amp;rsquo;t matter whether the VM is running in GCP or in your local server or anywhere.&lt;/p>&lt;p>The procedure is the same.&lt;/p>&lt;p>&lt;strong>Quick Trivia:&lt;/strong>&lt;/p>&lt;p>The WordPress VM available in GCP Click to Deploy has the following artefacts in them.&lt;/p>&lt;pre>&lt;code>Debian 8Apache HTTP Server 2.4.10MySQL 5.5.58PHP 5.6.30&lt;/code>&lt;/pre>&lt;p>Anyway, that was just an FYI. Let&amp;rsquo;s move ahead with our demo.&lt;/p>&lt;h2 id="installing-ssl-certificate-from-linux-bash-shell">Installing SSL Certificate from Linux Bash Shell&lt;/h2>&lt;p>[ &lt;strong>Note&lt;/strong>: You can skip steps #1 and #2 below for GCP WordPress VM because it already comes with Openssl and Apache2 installed in it. ]&lt;/p>&lt;p>First install Open SSL package in your Linux machine&lt;/p>&lt;h3 id="step-1">Step #1&lt;/h3>&lt;pre>&lt;code>sudo apt-get updatesudo apt-get upgrade openssl&lt;/code>&lt;/pre>&lt;p>Next Install Apache. Apache by default will run the HTTP service. We&amp;rsquo;ll configure it to run the HTTPS service.&lt;/p>&lt;h3 id="step-2">Step #2&lt;/h3>&lt;pre>&lt;code>sudo apt-get install apache2&lt;/code>&lt;/pre>&lt;p>Enable the SSL module in Apache.&lt;/p>&lt;pre>&lt;code>sudo a2enmod ssl&lt;/code>&lt;/pre>&lt;h3 id="step-3">Step #3&lt;/h3>&lt;p>Apache server comes with a default template for enabling SSL. Let&amp;rsquo;s leverage it.&lt;/p>&lt;pre>&lt;code>sudo a2ensite default-ssl&lt;/code>&lt;/pre>&lt;p>The above command will create a &amp;ldquo;default-ssl.conf&amp;rdquo; in the /etc/apache2/sites-enabled/ folder.&lt;/p>&lt;pre>&lt;code>$ lsdefault-ssl.conf wordpress.conf&lt;/code>&lt;/pre>&lt;h3 id="step-4">Step #4&lt;/h3>&lt;p>Reload the Apache server for these changes to take effect.&lt;/p>&lt;pre>&lt;code>$ sudo service apache2 reload&lt;/code>&lt;/pre>&lt;p>Why do you want your website to be SSL Certified or https enabled?&lt;/p>&lt;p>If your goal is to get &amp;ldquo;https&amp;rdquo; in your WordPress or any website so that it gives a good impression about your site to your users or for SEO reasons, then just stop buying SSL Certificates from those third-party private SSL Vendors who basically rob you of your money. Instead get it for absolutely free, no obligation attached, using the below options.&lt;/p>&lt;h3 id="step-5">Step #5&lt;/h3>&lt;p>You Have Two Option:&lt;/p>&lt;p>a) You can generate a self signed SSL Certificate in your machine locally&lt;/p>&lt;p>b) Get a free SSL certificate from &lt;a href="https://www.sslforfree.com/">https://www.sslforfree.com/&lt;/a> which is a non-profit organization supporting this. Follow the procedures in the website to get your SSL Certificate Files.&lt;/p>&lt;p>The site will give you an option to download the files in a zipped format. Unzip them and bring it to your Linux VM and place them under /etc/apache2/ssl/ folder.  You&amp;rsquo;ll have to create the ssl folder under /etc/apache2 before you can copy the files to this location.&lt;/p>&lt;p>After you copy the files. Make the files not accessible by other users but only by the root. Use the following command.&lt;/p>&lt;pre>&lt;code>$ chmod 600 /etc/apache2/ssh/*$ ls -l /etc/apache2/ssltotal 12-rw------- 1 root root 1646 Jul 11 11:09 ca_bundle.crt-rw------- 1 root root 2174 Jul 11 11:09 certificate.crt-rw------- 1 root root 1703 Jul 11 11:09 private.key$&lt;/code>&lt;/pre>&lt;h3 id="step-6">Step #6&lt;/h3>&lt;p>Now open the default-ssl.conf file that we generated earlier in step #3.&lt;/p>&lt;pre>&lt;code>$ sudo vim /etc/apache2/sites-enabled/default-ssl.conf&amp;lt;IfModule mod_ssl.c&amp;gt;    &amp;lt;VirtualHost _default_:443&amp;gt;        ServerAdmin webmaster@localhost        DocumentRoot /var/www/html        # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,        # error, crit, alert, emerg.        # It is also possible to configure the loglevel for particular        # modules, e.g.        #LogLevel info ssl:warn        ErrorLog ${APACHE_LOG_DIR}/error.log        CustomLog ${APACHE_LOG_DIR}/access.log combined        # For most configuration files from conf-available/, which are        # enabled or disabled at a global level, it is possible to        # include a line for only one particular virtual host. For example the        # following line enables the CGI configuration for this host only        # after it has been globally disabled with &amp;quot;a2disconf&amp;quot;.        #Include conf-available/serve-cgi-bin.conf        #  SSL Engine Switch:        #  Enable/Disable SSL for this virtual host.        SSLEngine on        #  A self-signed (snakeoil) certificate can be created by installing        #  the ssl-cert package. See        #  /usr/share/doc/apache2/README.Debian.gz for more info.        #  If both key and certificate are stored in the same file, only the        #  SSLCertificateFile directive is needed.        SSLCertificateFile /etc/apache2/ssl/certificate.crt        SSLCertificateKeyFile /etc/apache2/ssl/private.key...:wq$&lt;/code>&lt;/pre>&lt;p>Save the file and exit vim.&lt;/p>&lt;h3 id="heading">&lt;/h3>&lt;h3 id="step-7-optional">Step #7 (Optional)&lt;/h3>&lt;p>If you want you can rename the default-ssl.conf file to wordpress-ssl.conf file. Already the folder would have a wordpress.conf file for the non-secure &amp;ldquo;http&amp;rdquo; version of the Apache instance. This is not a mandatory step, but optional.&lt;/p>&lt;pre>&lt;code>$ sudo mv default-ssl.conf wordpress-ssl.conf$ ls /etc/apache2/sites-enabledwordpress.conf wordpress-ssl.conf&lt;/code>&lt;/pre>&lt;h3 id="heading-1">&lt;/h3>&lt;h3 id="step-8">Step #8&lt;/h3>&lt;p>Restart Apache so that the changes made to the config file will take effect.&lt;/p>&lt;pre>&lt;code>$ sudo service apache2 reload&lt;/code>&lt;/pre>&lt;p>&lt;/p>&lt;h3 id="step-9">Step #9&lt;/h3>&lt;p>Verify that the HTTPS service runs on port 443.&lt;/p>&lt;pre>&lt;code>$ openssl s_client -connect 'your-VM-IP-Address-Here:443'&lt;/code>&lt;/pre>&lt;p>You&amp;rsquo;ll see some output like this&amp;hellip;&lt;/p>&lt;pre>&lt;code>-----BEGIN CERTIFICATE-----.........-----END CERTIFICATE-----......---No client certificate CA names sent---SSL handshake has read 2257 bytes and written 415 bytes---......---^cread:errno=0$&lt;/code>&lt;/pre>&lt;p>If you see the above output then your SSL Certificate is installed and Apache HTTPS service is working just fine. You can press CTRL+C and exit.&lt;/p>&lt;p>You can go to your web browser and make it point to the &amp;ldquo;https://your-website-name-here&amp;rdquo; and press enter. It will show your website. More importantly, it will show the lock symbol next the URL, meaning it is a secure connection.&lt;/p>&lt;p>We are done with successful installation and configuration on the Apache server side is done.&lt;/p>&lt;h2 id="changes-required-in-wordpress">Changes Required in WordPress:&lt;/h2>&lt;p>Create a new or edit an already existing &amp;ldquo;.htaccess&amp;rdquo; file under /var/www/html/&lt;br>This is the root folder of your website.&lt;/p>&lt;p>Add the following content to the &amp;ldquo;.htaccess&amp;rdquo; file.&lt;/p>&lt;h3 id="step-10">Step #10&lt;/h3>&lt;p>You can do this by adding the following code in your .htaccess file:&lt;/p>&lt;pre>&lt;code>&amp;lt;IfModule mod_rewrite.c&amp;gt;RewriteEngine OnRewriteCond %{SERVER_PORT} 80RewriteRule ^(.*)$ https://www.yoursite.com/$1 [R,L]&amp;lt;/IfModule&amp;gt;&lt;/code>&lt;/pre>&lt;p>Replace &amp;lsquo;&lt;a href="http://www.yoursite.com">www.yoursite.com&lt;/a>&amp;rsquo; in the above configuration file with your site URL.&lt;/p>&lt;h3 id="step-11">Step #11&lt;/h3>&lt;p>Also edit wp-config.php file in the same root directory of your website and add the following line as shown below.&lt;/p>&lt;pre>&lt;code>define('WP_DEBUG', false);define('FORCE_SSL_ADMIN', true);/* That's all, stop editing! Happy blogging. */...&lt;/code>&lt;/pre>&lt;h3 id="step-12">Step #12&lt;/h3>&lt;p>Finally, login to your WordPress admin console and go to the settings page. Edit the URL of your website. Change &amp;ldquo;http&amp;rdquo; to &amp;ldquo;https&amp;rdquo; as shown below.&lt;/p>&lt;p>Now if you point any web-browser to the &amp;ldquo;http&amp;rdquo; version of your website, it will redirect automatically to the &amp;ldquo;https&amp;rdquo; version of your website.&lt;/p>&lt;p>That&amp;rsquo;s all folks !&lt;/p></description></item><item><title>Kubernetes Tutorial - How to Install Kubernetes on Ubuntu Baremetal</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-tutorial-how-to-install-kubernetes-on-ubuntu/</link><pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-tutorial-how-to-install-kubernetes-on-ubuntu/</guid><description>&lt;p>The primary goal of this tutorial is teach how to install Kubernetes on Ubuntu and configure a Kubernetes Cluster using a Master and Worker node.&lt;/p>&lt;p>[This is the the first lab in the Kubernetes Tutorial.]&lt;/p>&lt;p>The Ubuntu server can be a baremetal Ubuntu server or an Ubuntu VM. The steps involved are exactly the same.&lt;/p>&lt;p>Here is the agenda for in this lab.&lt;/p>&lt;ol>&lt;li>Bring up two Ubuntu 16.4 VMs.&lt;/li>&lt;li>Install and bring up Kubernetes cluster master node,&lt;/li>&lt;li>Install and bring up Kubernetes cluster worker node,&lt;/li>&lt;li>Install and bring up Calico CNI.&lt;/li>&lt;li>Download and run NGINX container.&lt;/li>&lt;li>Verify NGINX pod runs in the cluster.&lt;/li>&lt;/ol>&lt;h2 id="google-cloud-platformgcp">Google Cloud Platform(GCP):&lt;/h2>&lt;p>I signed up for Google Cloud Platform as it provides a $300 worth of cloud usage for 1 year free. I you prefer you can also sign up and run this tutorial in GCP.&lt;/p>&lt;p>Alternatively, you could run the VMs on your laptop.&lt;/p>&lt;p>I spawned two Ubuntu 16.4 VM&amp;rsquo;s.&lt;/p>&lt;p>[&lt;em>&lt;strong>Please ensure that you use Ubuntu 16.4 for this tutorial. I tried on 18.4 but I faced several challenges and open bugs in Kubernetes.&lt;/strong>&lt;/em>]{style=&amp;ldquo;text-decoration: underline; color: #ff0000;&amp;quot;}&lt;/p>&lt;p>[&lt;em>&lt;strong>Please ensure that each VM has at least 2 Cores and 4GB RAM.&lt;/strong>&lt;/em>]{style=&amp;ldquo;text-decoration: underline; color: #ff0000;&amp;quot;}&lt;/p>&lt;h2 id="installing-kubernetes-on-ubuntu">Installing Kubernetes on Ubuntu:&lt;/h2>&lt;p>The installation procedure is common for both master and worker nodes in a Kubernetes cluster. Follow the procedure below to install Kubernetes in both the Ubuntu VM&amp;rsquo;s you have spawned.&lt;/p>&lt;pre>&lt;code>Welcome to Ubuntu 16.04.4 LTS (GNU/Linux 4.13.0-1019-gcp x86_64)* Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantageGet cloud support with Ubuntu Advantage Cloud Guest: http://www.ubuntu.com/business/services/cloud0 packages can be updated.0 updates are security updates.Last login: Sun Jul 8 04:42:10 2018 from 173.194.93.35master:~$ ifconfigens4 Link encap:Ethernet HWaddr 42:01:0a:a0:00:02 inet addr:10.160.0.2 Bcast:10.160.0.2 Mask:255.255.255.255 inet6 addr: fe80::4001:aff:fea0:2/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1460 Metric:1 RX packets:547 errors:0 dropped:0 overruns:0 frame:0 TX packets:499 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:413630 (413.6 KB) TX bytes:61585 (61.5 KB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)&lt;/code>&lt;/pre>&lt;p>Check if the worker node can be pinged from the master node. This is key to forming the Kubernetes cluster.&lt;/p>&lt;pre>&lt;code>master:~$ ping 10.160.0.3PING 10.160.0.3 (10.160.0.3) 56(84) bytes of data.64 bytes from 10.160.0.3: icmp_seq=1 ttl=64 time=1.48 ms64 bytes from 10.160.0.3: icmp_seq=2 ttl=64 time=0.273 ms^C--- 10.160.0.3 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1001msrtt min/avg/max/mdev = 0.273/0.877/1.481/0.604 ms&lt;/code>&lt;/pre>&lt;h3 id="docker-installation">Docker Installation:&lt;/h3>&lt;p>We need to install Docker daemon if we need to use Docker images to run Docker containers. Remember from previous tutorials that Kubernetes is just an orchestration platform. It manages Docker and LXC containers.&lt;/p>&lt;pre>&lt;code>master:~$ sudo apt-get update &lt;/code>&lt;/pre>&lt;p>&amp;amp;&amp;amp; sudo apt-get install -qy docker.io&lt;/p>&lt;h3 id="setup-kubernetes-apt-repository">Setup Kubernetes apt repository:&lt;/h3>&lt;p>First we need to download the Kubernetes apt packages from the Kubernetes web site to a local repository.&lt;/p>&lt;pre>&lt;code>master:~$ sudo apt-get update &lt;/code>&lt;/pre>&lt;p>&amp;amp;&amp;amp; sudo apt-get install -y apt-transport-https&lt;br>&amp;amp;&amp;amp; curl -s &lt;a href="https://packages.cloud.google.com/apt/doc/apt-key.gpg">https://packages.cloud.google.com/apt/doc/apt-key.gpg&lt;/a> | sudo apt-key add -&lt;/p>&lt;pre>&lt;code>master-1:~$ echo &amp;quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&amp;quot; &lt;/code>&lt;/pre>&lt;p>| sudo tee -a /etc/apt/sources.list.d/kubernetes.list&lt;br>&amp;amp;&amp;amp; sudo apt-get update&lt;/p>&lt;h3 id="install-kubernetes">Install Kubernetes&lt;/h3>&lt;p>Next install the required Kubernetes packages from the local repo.&lt;/p>&lt;pre>&lt;code>master:~$ sudo apt-get update &lt;/code>&lt;/pre>&lt;p>&amp;amp;&amp;amp; sudo apt-get install -y&lt;br>kubelet&lt;br>kubeadm&lt;br>kubernetes-cni&lt;/p>&lt;p>This completes the installation of all the softwares required to setup Kubernetes Cluster and run Docker containers in Kubernetes.&lt;/p>&lt;h3 id="simple-installation-script">Simple Installation Script&lt;/h3>&lt;p>For ease of use, I have captured all the commands used so far in a shell script file named &lt;strong>&lt;a href="http://ethernetresearch.com/ganesh/kube-install.sh">kube-install.sh&lt;/a>&lt;/strong>. All you need to do is simply download the shell script and execute &amp;ldquo;&lt;strong>sh kube-install.sh&lt;/strong>&amp;rdquo; as a non-root user. It will install all the software for you in one shot, without requiring any user intervention.&lt;/p>&lt;p>Please make sure that the Linux swap is turn off. Kubernetes requires that all pods/containers always stays in memory and not swapped to the disk.&lt;/p>&lt;pre>&lt;code>master:~$ sudo apt install mount #install swapoff in mount pkgmaster:~$ sudo swapoff --allmaster:~$ cat /proc/swapsFilename Type Size Used Priority&lt;/code>&lt;/pre>&lt;blockquote>&lt;p>&lt;em>I hope you followed all the above set of steps to install Docker and Kubernetes on the worker node also. If not, please complete the Kubernetes installation in the worker node before we move on the the next steps.&lt;/em>&lt;/p>&lt;/blockquote>&lt;h2 id="inialize-the-master-node">Inialize the master node:&lt;/h2>&lt;p>From now on, we execute and manage the clusters and pods from the master node only. Worker nodes don&amp;rsquo;t can&amp;rsquo;t be used for cluster and pod management.&lt;/p>&lt;p>Initialalize the master node with the following command.The default subnet for Calico CNI is192.168.0.0/16 and Flannel CNI is 10.244.0.0/16. We&amp;rsquo;ll be using Calico CNI plugin for this example. Hence we&amp;rsquo;ll set &amp;ldquo;pod-network-cidr&amp;rdquo; to the Calico subnet.&lt;/p>&lt;p>Set the &amp;ldquo;apiserver-advertise-address&amp;rdquo; with the ip address of the master node.&lt;/p>&lt;pre>&lt;code>master:~$ sudo kubeadm init pod-network-cidr=192.168.0.0/16 apiserver-advertise-address=10.160.0.2...Your Kubernetes master has initialized successfully!To start using your cluster, you need to run the following as a regular user:mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root:kubeadm join 10.160.0.2:6443 --token dg6zhb.naxuxb5wd78tdsqz --discovery-token-ca-cert-hash sha256:af9e87508dc6c2c52b6f51dd88635f2dc9f061dce27368f3ac9913b52522cac3&lt;/code>&lt;/pre>&lt;h3 id="heading">&lt;/h3>&lt;h2 id="setup-container-networking-interfacecni">Setup Container Networking Interface(CNI)&lt;/h2>&lt;p>As the init logs above says, we need to install and setup the CNI. We&amp;rsquo;ll use the Calico CNI plugin in this example. We should provide the calico.yaml file from the calico&amp;rsquo;s project website.&lt;/p>&lt;pre>&lt;code>master:~$ kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml&lt;/code>&lt;/pre>&lt;p>Usually Pods or Containers are not run in the master node. because master nodes are used for controlling purposes and not for executing the jobs. But, there is an option to override this to run a single node cluster with just the master node. The pods will run in the master node itself. Here is the command to make master node to run the Pods.&lt;/p>&lt;pre>&lt;code>$ kubectl taint nodes --all node-role.kubernetes.io/master-&lt;/code>&lt;/pre>&lt;p>However, if you want to build a real world cluster, then skip the above command and go add the worker nodes as explained below.&lt;/p>&lt;h2 id="configure-the-worker">Configure the Worker:&lt;/h2>&lt;p>We need to use the token generated by the master during &amp;ldquo;kubeadm init&amp;rdquo; to join from the worker node.&lt;/p>&lt;p>In case you lost the token generated by the master node, execute the below command in the master node to make it print again.&lt;/p>&lt;pre>&lt;code>master $ kubeadm token create --print-join-commandkubeadm join 10.148.0.2:6443 --token wdyfoz.k1wr3dcdc7l9bd2n --discovery-token-ca-cert-hash sha256:d1fa4f7a5d3a5d9b2ba9d32807906fc16fbc16fad5faf81748289faf800add12&lt;/code>&lt;/pre>&lt;p>Now join the worker node to the cluster.&lt;/p>&lt;pre>&lt;code>worker:~$ sudo kubeadm join 10.160.0.2:6443 --token dg6zhb.naxuxb5wd78tdsqz --discovery-token-ca-cert-hash sha256:af9e87508dc6c2c52b6f51dd88635f2dc9f061dce27368f3ac9913b52522cac3[preflight] running pre-flight checksI0708 05:23:27.564762 18132 kernel_validator.go:81] Validating kernel versionI0708 05:23:27.564859 18132 kernel_validator.go:96] Validating kernel config[discovery] Trying to connect to API Server &amp;quot;10.160.0.2:6443&amp;quot;[discovery] Created cluster-info discovery client, requesting info from &amp;quot;https://10.160.0.2:6443&amp;quot;[discovery] Requesting info from &amp;quot;https://10.160.0.2:6443&amp;quot; again to validate TLS against the pinned public key[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &amp;quot;10.160.0.2:6443&amp;quot;[discovery] Successfully established connection with API Server &amp;quot;10.160.0.2:6443&amp;quot;[kubelet] Downloading configuration for the kubelet from the &amp;quot;kubelet-config-1.11&amp;quot; ConfigMap in the kube-system namespace[kubelet] Writing kubelet configuration to file &amp;quot;/var/lib/kubelet/config.yaml&amp;quot;[kubelet] Writing kubelet environment file with flags to file &amp;quot;/var/lib/kubelet/kubeadm-flags.env&amp;quot;[preflight] Activating the kubelet service[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...[patchnode] Uploading the CRI Socket information &amp;quot;/var/run/dockershim.sock&amp;quot; to the Node API object &amp;quot;instance-2&amp;quot; as an annotationThis node has joined the cluster:* Certificate signing request was sent to master and a response was received.* The Kubelet was informed of the new secure connection details.Run 'kubectl get nodes' on the master to see this node join the cluster.worker:~$&lt;/code>&lt;/pre>&lt;p>Sometime when you run the above command, it may complain that the &amp;ldquo;ip_vs&amp;rdquo; kernel module is not loaded. To solve this issue, you may need to do download and the missing kernel modules.&lt;/p>&lt;pre>&lt;code>worker:~$ sudo apt install ip_vsworker:~$ modprobe ip_vs&lt;/code>&lt;/pre>&lt;p>Also, as mentioned earlier, turn off the Linux swap service before running the above join command, using &amp;ldquo;&lt;em>swapoff -a&lt;/em>&amp;rdquo; command.&lt;/p>&lt;h2 id="verify-if-the-cluster-nodes-are-up-and-running">Verify if the cluster nodes are up and running:&lt;/h2>&lt;p>Use the &amp;ldquo;kubectl get nodes&amp;rdquo; command to check if worker node was able to talk to the master node and join the cluster. Verify if both master and worker are ready to run pods.&lt;/p>&lt;pre>&lt;code>master:~$ kubectl get nodesNAME STATUS ROLES AGE VERSIONinstance-1 Ready master 14m v1.11.0instance-2 NotReady 47s v1.11.0master:~$ kubectl get nodesNAME STATUS ROLES AGE VERSIONinstance-1 Ready master 15m v1.11.0instance-2 Ready 1m v1.11.0&lt;/code>&lt;/pre>&lt;p>That&amp;rsquo;s all folks. We are done with installing Kubernetes on a bare metal server.&lt;/p>&lt;h3 id="spin-an-nginx-pod-in-the-cluster">Spin an NGINX pod in the cluster&lt;/h3>&lt;p>We should use the master node to execute any cluster/pod management commands.&lt;/p>&lt;pre>&lt;code>master:~$ kubectl run nginx --image=nginx --port=80deployment.apps/nginx created&lt;/code>&lt;/pre>&lt;p>Verify that the pod is up and running.&lt;/p>&lt;pre>&lt;code>master:~$ kubectl get podsNAME READY STATUS RESTARTS AGEnginx-6f858d4d45-b7nk5 0/1 ContainerCreating 0 7smaster:~$ kubectl get podsNAME READY STATUS RESTARTS AGEnginx-6f858d4d45-b7nk5 1/1 Running 0 31s&lt;/code>&lt;/pre>&lt;p>To get more details about the pod, use the &amp;lsquo;kubectl describe&amp;rsquo; command.&lt;/p>&lt;pre>&lt;code>master:~$ kubectl describe podsName: nginx-6f858d4d45-b7nk5Namespace: defaultNode: instance-2/10.160.0.3Start Time: Sun, 08 Jul 2018 05:25:21 +0000Labels: pod-template-hash=2941480801 run=nginxAnnotations: Status: RunningIP: 192.168.56.1Controlled By: ReplicaSet/nginx-6f858d4d45Containers: nginx: Container ID: docker://90431cb74d4f3e3abf77f3344277ba5679137cfad54a473f56d381d1ad0f3a6d Image: nginx Image ID: docker-pullable://nginx@sha256:a65beb8c90a08b22a9ff6a219c2f363e16c477b6d610da28fe9cba37c2c3a2ac Port: 80/TCP Host Port: 0/TCP State: Running Started: Sun, 08 Jul 2018 05:25:35 +0000 Ready: True Restart Count: 0 Environment: Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-n8l4x (ro)Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-n8l4x: Type: Secret (a volume populated by a Secret) SecretName: default-token-n8l4x Optional: falseQoS Class: BestEffortNode-Selectors: Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300sEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 38s default-scheduler Successfully assigned default/nginx-6f858d4d45-b7nk5 to instance-2 Normal Pulling 37s kubelet, instance-2 pulling image &amp;quot;nginx&amp;quot; Normal Pulled 25s kubelet, instance-2 Successfully pulled image &amp;quot;nginx&amp;quot; Normal Created 24s kubelet, instance-2 Created container Normal Started 24s kubelet, instance-2 Started container&lt;/code>&lt;/pre></description></item><item><title>Kubernetes vs Docker Swarm - 5 Major Differences</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-vs-docker/</link><pubDate>Sun, 08 Jul 2018 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/kubernetes-vs-docker/</guid><description>&lt;p>Here are the 6 differences between Docker Swarm and Kubernetes. But before that let&amp;rsquo;s refresh some basics on Docker and Kubernetes. If you are in a haste, you can jump straight to &lt;strong>Kubernetes vs Docker Swarm&lt;/strong> debate down below.&lt;/p>&lt;h2 id="what-is-docker">What is Docker?&lt;/h2>&lt;p>Docker is a containerization platform. It helps create Docker images and Docker containers from an application binary and configuration files.&lt;/p>&lt;p>Docker leverages the containerization primitives such as namespace, cgroups provided by the Linux Kernel to run docker containers.&lt;/p>&lt;h2 id="what-is-docker-swarm">What is Docker Swarm&lt;/h2>&lt;p>Docker Swarm is a proprietary management software from Docker to manage a swarm of containers running in a Docker cluster.&lt;/p>&lt;p>Docker Swarm is used to create and manage the cluster nodes as well schedule container jobs to run in the cluster nodes.&lt;/p>&lt;h2 id="what-is-kubernetes-or-k8s">What is Kubernetes or K8s?&lt;/h2>&lt;p>Kubernetes (also known as K8s) is an open source &amp;ldquo;orchestration&amp;rdquo; or &amp;ldquo;management&amp;rdquo; platform for managing containers.&lt;/p>&lt;blockquote>&lt;p>Kubernetes is &amp;ldquo;NOT&amp;rdquo; a containerization platform that competes with Docker container engine(Docker CE/EE) .&lt;/p>&lt;/blockquote>&lt;p>It actually leverages Docker to instantiate containers in a cluster. For example, Kubernetes requires you to install Docker CE/EE to run and manage your applications that are packaged as Docker images.&lt;/p>&lt;h3 id="what-kubernetes-does">What Kubernetes does?&lt;/h3>&lt;p>Kubernetes creates and manages cluster nodes. Kubernetes creates and manages pods containing containerised applications within those nodes in a cluster.&lt;/p>&lt;blockquote>&lt;p>In very simple terms, Kubernetes is more like OpenStack equivalent in the container space.&lt;/p>&lt;/blockquote>&lt;p>Kubernetes competes with Docker Swarm and Apache Mesos in the container cluster orchestration market.&lt;/p>&lt;h2 id="kubernetes-vs-docker-swarm---differences">Kubernetes vs Docker Swarm - Differences&lt;/h2>&lt;p>Here are a few primary differences between Kubernetes and Docker Swarm:&lt;/p>&lt;table>&lt;thead>&lt;tr>&lt;th>Kubernetes&lt;/th>&lt;th>Docker Swarm&lt;/th>&lt;/tr>&lt;/thead>&lt;tbody>&lt;tr>&lt;td>It is a free open source technology, originally developed at Google&lt;/td>&lt;td>It is a proprietary solution developed and sold by Docker Inc.&lt;/td>&lt;/tr>&lt;tr>&lt;td>It is a highly scalable platform. Kubernetes can manage upto 5000 nodes in a cluster and 150,000 pods within those cluster nodes.&lt;/td>&lt;td>Docker Swarm is relatively less scalable. Docker Swarm can support only 1000 nodes in a cluster and 30,000 pods with those cluster nodes.&lt;/td>&lt;/tr>&lt;tr>&lt;td>Kubernetes cluster consists of Master nodes and Worker nodes&lt;/td>&lt;td>Docker Swarm consists of Manager nodes and Worker nodes&lt;/td>&lt;/tr>&lt;tr>&lt;td>Supports High Availability(HA). Multiple Master nodes and Worker nodes can be configured.&lt;/td>&lt;td>Supports High Availability(HA). Multiple Manager nodes and Worker nodes can be configured.&lt;/td>&lt;/tr>&lt;tr>&lt;td>Kubernetes terminologies: Pods, Deployments, Services&lt;/td>&lt;td>Docker Swarm terminologies: Stack, Services, Tasks&lt;/td>&lt;/tr>&lt;tr>&lt;td>In Kubernetes terminologies, a deployment is a collection or multiple replicas of the same pod.&lt;/td>&lt;td>In Docker Swarm terminologies, a Stack is a group of services that are usually deployed together, such as a frontend service and a backend service that together forms an application stack.&lt;/td>&lt;/tr>&lt;/tbody>&lt;/table>&lt;h3 id="which-one-is-popular-kubernetes-or-docker-swarm">Which one is popular Kubernetes or Docker Swarm?&lt;/h3>&lt;p>Kubernetes is more powerful, robust and has wider industry adoption than Docker Swarm. Moreover, it is a free open source software.&lt;/p>&lt;p>Kubernetes is supported by all the three major cloud service provides - AWS, MS Azure and Google Cloud Platform.&lt;/p>&lt;p>Kubernetes is providing a though competition to Docker. The flurry of organizational changes happening within Docker company is due to these reasons.&lt;/p>&lt;p>&lt;/p></description></item><item><title>Linux Networking Commands to Debug Troubleshoot IP / UDP / TCP Packet Loss</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/linux-networking-commands-check-debug-ip-udp-tcp-packet-loss/</link><pubDate>Fri, 13 Oct 2017 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/linux-networking-commands-check-debug-ip-udp-tcp-packet-loss/</guid><description>&lt;p>Recently, I happened to debug an UDP flow packet loss issue happening within an ubuntu LXC container. It was not obviously evident why the UDP packets were getting dropped. However, the &lt;em>tcpdump&lt;/em> utility showed that the packets are indeed being received by the ingress interface (eth0) of the container. I executed a bunch of Linux networking related commands and finally figured out why and where the packets were getting dropped.&lt;/p>&lt;p>In this article, I&amp;rsquo;ll share a comprehensive list of the Linux networking commands and debugging tools I used to debug the UDP packet loss problem.&lt;/p>&lt;h2 id="how-to-check-debug-troubleshoot-packet-loss">How to check debug troubleshoot packet loss&lt;/h2>&lt;p>Here is the step by step instructions on how to debug check troubleshoot a network packet loss in linux.&lt;/p>&lt;h3 id="tcpdump-and-wireshark">tcpdump and wireshark:&lt;/h3>&lt;p>The very first step in any packet debugging process is to capture the packet entering or leaving the NIC using the &amp;ldquo;&lt;em>tcpdump&lt;/em>&amp;rdquo; utility. You could potentially save the &lt;em>tcpdump&lt;/em> captured packets in a file and open it using the &amp;ldquo;&lt;em>wireshark&lt;/em>&amp;rdquo; GUI tool. In the GUI, glance over the various packet fields such as Source MAC, Destination MAC, Source IP, Destination IP, TTL, Checksum etc. and ensure they are all correct.&lt;/p>&lt;pre>&lt;code>$ tcpdump -i eth0 -c 1 -w mypacket.pcaptcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes1 packet captured87 packets received by filter0 packets dropped by kernel$ wireshark mypacket.pcap # this will open the GUI&lt;/code>&lt;/pre>&lt;h2 id="interface-statistics">Interface Statistics:&lt;/h2>&lt;p>Once you confirmed that the packets have entered through the NIC and they are not malformed packets, move on to the next step, which is to look at the network interface statistics and ensure they don&amp;rsquo;t have any drops or errors.&lt;/p>&lt;p>Here are some of the commands you could use to do so.&lt;/p>&lt;pre>&lt;code>user@ubuntu:~$ ifconfig -aeth0 Link encap:Ethernet HWaddr 00:0c:29:e7:b1:87 inet addr:192.168.85.134 Bcast:192.168.85.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:fee7:b187/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8236 errors:0 dropped:0 overruns:0 frame:0 TX packets:3390 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:9677592 (9.6 MB) TX bytes:294562 (294.5 KB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:4043 errors:0 dropped:0 overruns:0 frame:0 TX packets:4043 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:769351 (769.3 KB) TX bytes:769351 (769.3 KB)user@ubuntu:~$&lt;/code>&lt;/pre>&lt;p>You could also look at the interface statistics file in the &lt;em>/sys&lt;/em> filesystem.&lt;/p>&lt;pre>&lt;code>user@ubuntu:~$ grep '.*' /sys/class/net/eth0/statistics/* /sys/class/net/eth0/statistics/collisions:0/sys/class/net/eth0/statistics/multicast:0/sys/class/net/eth0/statistics/rx_bytes:9380575/sys/class/net/eth0/statistics/rx_compressed:0/sys/class/net/eth0/statistics/rx_crc_errors:0/sys/class/net/eth0/statistics/rx_dropped:0/sys/class/net/eth0/statistics/rx_errors:0/sys/class/net/eth0/statistics/rx_fifo_errors:0/sys/class/net/eth0/statistics/rx_frame_errors:0/sys/class/net/eth0/statistics/rx_length_errors:0/sys/class/net/eth0/statistics/rx_missed_errors:0/sys/class/net/eth0/statistics/rx_over_errors:0/sys/class/net/eth0/statistics/rx_packets:7954/sys/class/net/eth0/statistics/tx_aborted_errors:0/sys/class/net/eth0/statistics/tx_bytes:278911/sys/class/net/eth0/statistics/tx_carrier_errors:0/sys/class/net/eth0/statistics/tx_compressed:0/sys/class/net/eth0/statistics/tx_dropped:0/sys/class/net/eth0/statistics/tx_errors:0/sys/class/net/eth0/statistics/tx_fifo_errors:0/sys/class/net/eth0/statistics/tx_heartbeat_errors:0/sys/class/net/eth0/statistics/tx_packets:3166/sys/class/net/eth0/statistics/tx_window_errors:0&lt;/code>&lt;/pre>&lt;p>There is a tool named &amp;ldquo;&lt;em>ethtool&lt;/em>&amp;rdquo; which you could download and install. It also would display the above information more elegantly.&lt;/p>&lt;pre>&lt;code>user@ubuntu:~$ ethtool -S eth0NIC statistics: rx_packets: 7958 tx_packets: 3168 rx_bytes: 9413211 tx_bytes: 279313 rx_broadcast: 0 tx_broadcast: 0 rx_multicast: 0 tx_multicast: 0 rx_errors: 0 tx_errors: 0 tx_dropped: 0 multicast: 0 collisions: 0 rx_length_errors: 0 rx_over_errors: 0 rx_crc_errors: 0 rx_frame_errors: 0 rx_no_buffer_count: 0 rx_missed_errors: 0 tx_aborted_errors: 0 tx_carrier_errors: 0 tx_fifo_errors: 0 tx_heartbeat_errors: 0 tx_window_errors: 0 tx_abort_late_coll: 0 tx_deferred_ok: 0 tx_single_coll_ok: 0 tx_multi_coll_ok: 0 tx_timeout_count: 0 tx_restart_queue: 0 rx_long_length_errors: 0 rx_short_length_errors: 0 rx_align_errors: 0 tx_tcp_seg_good: 0 tx_tcp_seg_failed: 0 rx_flow_control_xon: 0 rx_flow_control_xoff: 0 tx_flow_control_xon: 0 tx_flow_control_xoff: 0 rx_long_byte_count: 9413211 rx_csum_offload_good: 7085 rx_csum_offload_errors: 0 alloc_rx_buff_failed: 0 tx_smbus: 0 rx_smbus: 0 dropped_smbus: 0user@ubuntu:~$&lt;/code>&lt;/pre>&lt;p>You could also use the &amp;ldquo;&lt;em>netstat&lt;/em>&amp;rdquo; command with &amp;ldquo;-i&amp;rdquo; option to display the interface statistics.&lt;/p>&lt;pre>&lt;code>user@ubuntu:~$ netstat -iKernel Interface tableIface MTU Met RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgeth0 1500 0 7954 0 0 0 3166 0 0 0 BMRUlo 65536 0 3807 0 0 0 3807 0 0 0 LRUuser@ubuntu:~$&lt;/code>&lt;/pre>&lt;h2 id="try-raspberry-pi-remote-ssh-access-for-free-from-socketxp">Try Raspberry Pi Remote SSH Access for Free From SocketXP&lt;/h2>&lt;h3 id="iot-remote-ssh-or-raspberry-pi-remote-ssh-accesshttpswwwsocketxpcomiotraspberry-pi-remote-ssh-access">&lt;a href="https://www.socketxp.com/iot/raspberry-pi-remote-ssh-access/">IoT Remote SSH or Raspberry Pi Remote SSH Access&lt;/a>&lt;/h3>&lt;h2 id="ip-filters">IP Filters:&lt;/h2>&lt;p>After the packet enters the box and doesn&amp;rsquo;t get dropped at the NIC layer, they are subject to IP ingress filters. We can dump the IP filters table to see if there are any filters configured to drop(REJECT) the packet and the corresponding dropped packet count for each filter configured in the table.&lt;/p>&lt;pre>&lt;code>user@ubuntu:~$ sudo iptables -vL [sudo] password for user: Chain INPUT (policy ACCEPT 7408 packets, 9828K bytes) pkts bytes target prot opt in out source destination 0 0 ACCEPT udp -- virbr0 any anywhere anywhere udp dpt:domain 0 0 ACCEPT tcp -- virbr0 any anywhere anywhere tcp dpt:domain 0 0 ACCEPT udp -- virbr0 any anywhere anywhere udp dpt:bootps 0 0 ACCEPT tcp -- virbr0 any anywhere anywhere tcp dpt:bootps Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 0 0 ACCEPT all -- virbr0 virbr0 anywhere anywhere 0 0 REJECT all -- any virbr0 anywhere anywhere reject-with icmp-port-unreachable 0 0 REJECT all -- virbr0 any anywhere anywhere reject-with icmp-port-unreachable Chain OUTPUT (policy ACCEPT 6671 packets, 920K bytes) pkts bytes target prot opt in out source destination 0 0 ACCEPT udp -- any virbr0 anywhere anywhere udp dpt:bootpc user@ubuntu:~$&lt;/code>&lt;/pre>&lt;h2 id="ethernet-header-and-ip-header-processing">Ethernet Header and IP Header Processing:&lt;/h2>&lt;p>Any packet that is not dropped by the filters would go through the different layers in the TCP/IP stack for further processing. First, it goes through the L2 layer which is the Ethernet layer for Ethernet Header processing. Primarily it checks if the destination MAC is its own MAC address or someone else and makes a decision to forward or consume the packet or pass on to the next layer in the TCP/IP stack. It is in this layer that the source MAC learning happens. Meaning, the source MAC can be reached through the incoming interface and it maps to the Source IP in the packet (ARP resolution).&lt;/p>&lt;p>In the L3 layer processing, IP header fields such as &lt;em>destination&lt;/em> &lt;em>IP&lt;/em>, &lt;em>TTL&lt;/em> etc. will be looked at. If the destination IP is its own IP, then the packet would be consumed or sent to the upper layers(UDP/TCP) for further processing. If the destination IP is not the box&amp;rsquo;s IP address, then a routing table lookup will be done to find the gateway or the next-hop to send the packet to.&lt;/p>&lt;h3 id="displaying-routing-table">Displaying Routing Table:&lt;/h3>&lt;p>Check the &lt;em>routing table&lt;/em> to see if the routes are present. In the example below, we show an IPv6 routing table ( &lt;em>&amp;ndash;inet6&lt;/em> signifies that ).&lt;/p>&lt;pre>&lt;code>user@ubuntu:/# netstat -r --inet6Kernel IPv6 routing tableDestination Next Hop Flag Met Ref Use Iffe80::/64 :: U 256 0 0 eth0::/0 :: !n -1 1 37 lo::1/128 :: Un 0 1 2 lofe80::216:3eff:fe88:c1d3/128 :: Un 0 1 0 loff00::/8 :: U 256 0 0 eth0::/0 :: !n -1 1 37 louser@ubuntu:/#&lt;/code>&lt;/pre>&lt;h3 id="displaying-arp-table">&lt;strong>Displaying Arp Table:&lt;/strong>&lt;/h3>&lt;p>Make sure the arp for the next hop router is resolved and is learnt via correct interface.&lt;/p>&lt;pre>&lt;code>user@ubuntu:~$ arpAddress HWtype HWaddress Flags Mask Iface192.168.85.2 ether 00:50:56:f5:0c:c4 C eth0192.168.85.254 ether 00:50:56:e3:b6:fc C eth0user@ubuntu:~$&lt;/code>&lt;/pre>&lt;h2 id="socket-connection-state">Socket Connection State:&lt;/h2>&lt;p>You should next check if the UDP/TCP socket is in the correct state. You could again use the &amp;ldquo;&lt;em>netstat&lt;/em>&amp;rdquo; command with appropriate options to display the same, as shown below.&lt;/p>&lt;pre>&lt;code>user@ubuntu:~$ netstat --all --udp --tcpActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 0 ubuntu:domain *:* LISTENtcp 0 0 *:ssh *:* LISTENtcp 0 0 localhost:ipp *:* LISTENtcp 0 0 *:git *:* LISTENtcp6 0 0 [::]:ssh [::]:* LISTENtcp6 0 0 ip6-localhost:ipp [::]:* LISTENudp 0 0 *:52707 *:*udp 0 0 *:ipp *:* udp6 0 0 [::]:53788 [::]:*udp6 0 0 [::]:36751 [::]:*&lt;/code>&lt;/pre>&lt;p>After you ensure that the socket state is correct, you can list the IP/UDP/TCP packet statistics using the following netstat statistics ( -s option ) command.&lt;/p>&lt;h2 id="netstat-statistics">Netstat Statistics:&lt;/h2>&lt;pre>&lt;code>user@ubuntu:~$ netstat -sIp: 7499 total packets received 3 with invalid addresses 0 forwarded 0 incoming packets discarded 7463 incoming packets delivered 6731 requests sent out 8 outgoing packets droppedIcmp: 22 ICMP messages received 0 input ICMP message failed. ICMP input histogram: destination unreachable: 20 echo requests: 2 22 ICMP messages sent 0 ICMP messages failed ICMP output histogram: destination unreachable: 20 echo replies: 2IcmpMsg: InType3: 20 InType8: 2 OutType0: 2 OutType3: 20Tcp: 155 active connections openings 77 passive connection openings 60 failed connection attempts 0 connection resets received 0 connections established 6691 segments received 6114 segments send out 0 segments retransmited 0 bad segments received. 60 resets sentUdp: 785 packets received 17 packets to unknown port received. 0 packet receive errors 666 packets sent IgnoredMulti: 216UdpLite:TcpExt: 91 TCP sockets finished time wait in fast timer 192 delayed acks sent Quick ack mode was activated 1 times 315 packets directly queued to recvmsg prequeue. 48452 bytes directly in process context from backlog 939769 bytes directly received in process context from prequeue 3004 packet headers predicted 360 packets header predicted and directly queued to user 604 acknowledgments not containing data payload received 792 predicted acknowledgments TCPLossProbes: 1 IPReversePathFilter: 10 TCPRcvCoalesce: 1012 TCPOrigDataSent: 2429IpExt: InNoRoutes: 23 InMcastPkts: 266 OutMcastPkts: 243 InBcastPkts: 216 InOctets: 9844085 OutOctets: 926822 InMcastOctets: 35621 OutMcastOctets: 30950 InBcastOctets: 24635 InNoECTPkts: 11099user@ubuntu:~$&lt;/code>&lt;/pre>&lt;h2 id="dumping-snmp-counters">Dumping SNMP Counters:&lt;/h2>&lt;p>You could also dump the SNMP Counters to where exactly the packet is being dropped and for what exact reason.&lt;/p>&lt;pre>&lt;code>user@ubuntu:~$ cat /proc/net/snmp6Ip6InReceives  221Ip6InHdrErrors  0Ip6InTooBigErrors  0Ip6InNoRoutes  0Ip6InAddrErrors  0Ip6InUnknownProtos  0Ip6InTruncatedPkts  0Ip6InDiscards  0Ip6InDelivers  221Ip6OutForwDatagrams  0Ip6OutRequests  277Ip6OutDiscards  2Ip6OutNoRoutes  39Ip6ReasmTimeout  0Ip6ReasmReqds  0Ip6ReasmOKs  0Ip6ReasmFails  0Ip6FragOKs  0Ip6FragFails  0Ip6FragCreates  0Ip6InMcastPkts  144Ip6OutMcastPkts  239Ip6InOctets  29087Ip6OutOctets  33147Ip6InMcastOctets  22382Ip6OutMcastOctets  29666Ip6InBcastOctets  0Ip6OutBcastOctets  0Ip6InNoECTPkts  221Ip6InECT1Pkts  0Ip6InECT0Pkts  0Ip6InCEPkts  0Icmp6InMsgs  2Icmp6InErrors  0Icmp6OutMsgs  61Icmp6OutErrors  0Icmp6InCsumErrors  0Icmp6InDestUnreachs  0Icmp6InPktTooBigs  0Icmp6InTimeExcds  0Icmp6InParmProblems  0Icmp6InEchos  0Icmp6InEchoReplies  0Icmp6InGroupMembQueries  0Icmp6InGroupMembResponses  0Icmp6InGroupMembReductions  0Icmp6InRouterSolicits  0Icmp6InRouterAdvertisements  0Icmp6InNeighborSolicits  0Icmp6InNeighborAdvertisements  2Icmp6InRedirects  0Icmp6InMLDv2Reports  0Icmp6OutDestUnreachs  0Icmp6OutPktTooBigs  0Icmp6OutTimeExcds  0Icmp6OutParmProblems  0Icmp6OutEchos  0Icmp6OutEchoReplies  0Icmp6OutGroupMembQueries  0Icmp6OutGroupMembResponses  0Icmp6OutGroupMembReductions  0Icmp6OutRouterSolicits  15Icmp6OutRouterAdvertisements  0Icmp6OutNeighborSolicits  6Icmp6OutNeighborAdvertisements  0Icmp6OutRedirects  0Icmp6OutMLDv2Reports  40Icmp6InType136  2Icmp6OutType133  15Icmp6OutType135  6Icmp6OutType143  40Udp6InDatagrams  245Udp6NoPorts  0Udp6InErrors  0Udp6OutDatagrams  136Udp6RcvbufErrors  0Udp6SndbufErrors  0Udp6InCsumErrors  0Udp6IgnoredMulti  0UdpLite6InDatagrams  0UdpLite6NoPorts  0UdpLite6InErrors  0UdpLite6OutDatagrams  0UdpLite6RcvbufErrors  0UdpLite6SndbufErrors  0UdpLite6InCsumErrors  0user@ubuntu:~$&lt;/code>&lt;/pre>&lt;h2 id="miscellaneous-items">Miscellaneous Items:&lt;/h2>&lt;h3 id="displaying-bridges">Displaying Bridges:&lt;/h3>&lt;p>Make sure the bridges are configured and connected to the interfaces correctly. If not the packets may not be bridged out on the correct interface.&lt;/p>&lt;pre>&lt;code>user@ubuntu:~$ brctl showbridge name bridge id STP enabled interfaceslxcbr0 8000.000000000000 novethI6Q4U6virbr0 8000.000000000000 yesuser@ubuntu:~$&lt;/code>&lt;/pre>&lt;p>&lt;/p>&lt;p>&lt;em>&lt;strong>Sometimes an entire&lt;/strong>&lt;/em> &lt;strong>&lt;em>networking functionality or feature may be disabled in the Linux networking stack&lt;/em>.&lt;/strong> For example, the following command shows you how to check if IPv6 functionality is enabled or disabled in the Linux kernel. If disabled, IPv6 packet may be dropped silently.These settings can be enabled or disabled on a global basis, as shown below:&lt;/p>&lt;pre>&lt;code>user@ubuntu:~$ sysctl net.ipv6.conf.all.disable_ipv6net.ipv6.conf.all.disable_ipv6 = 0user@ubuntu:~$ sysctl net.ipv6.conf.default.disable_ipv6net.ipv6.conf.default.disable_ipv6 = 0&lt;/code>&lt;/pre>&lt;p>or on a per interface basis, as show below:&lt;/p>&lt;pre>&lt;code>user@ubuntu:~$ sysctl net.ipv6.conf.lo.disable_ipv6net.ipv6.conf.lo.disable_ipv6 = 0user@ubuntu:~$ sysctl net.ipv6.conf.eth0.disable_ipv6 net.ipv6.conf.eth0.disable_ipv6 = 0user@ubuntu:~$&lt;/code>&lt;/pre>&lt;p>Alternatively, you could refer to the &lt;em>/etc/sysctl.conf&lt;/em> file for these settings. In the below example, the settings are commented out.&lt;/p>&lt;pre>&lt;code>user@ubuntu:~$ cat /etc/sysctl.conf | grep ipv6#net.ipv6.conf.all.forwarding=1#net.ipv6.conf.all.accept_redirects = 0#net.ipv6.conf.all.accept_source_route = 0user@ubuntu:~$&lt;/code>&lt;/pre>&lt;p>That&amp;rsquo;s all folks !&lt;/p>&lt;p>Interested in learning Docker Containers and Kubernetes Devops from a professional and experienced instructor?&lt;/p>&lt;h2 id="find-online-courses-from-author-ganesh-velrajan-here">Find online courses from author Ganesh Velrajan here.&lt;/h2>&lt;h3 id="docker-and-kubernetes-courses-in-udemy-at-author-discount-ratesonline-courses-training">&lt;a href="https://gvelrajan.github.io/ethernetresearch/online-courses-training/">Docker and Kubernetes Courses in Udemy at Author Discount Rates.&lt;/a>&lt;/h3></description></item><item><title>Linux Container Internals - Fundamentals of Linux Containers</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/linux-containers-internals-lxc/</link><pubDate>Thu, 08 Jun 2017 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/linux-containers-internals-lxc/</guid><description>&lt;h2 id="what-are-containers-">What are containers ?&lt;/h2>&lt;p>Containers are groups of userspace processes running in an isolated or contained virtual environment of their own, created within a single host or a VM. The isolated (contained) environment for each of the containers is created using the virtualization primitives provided by the modern Linux Kernel such as namespace, cgroups, etc. Linux container management tools such as LXC and Docker are nothing but wrappers written on top these underlying Linux Kernel primitives to create and run containers.&lt;/p>&lt;p>Lets get our hands dirty by playing with all these Linux Kernel primitives using some of the tools available in Linux.&lt;/p>&lt;p>To create a container we need a Linux container image first.&lt;/p>&lt;h2 id="what-is-a-container-image-">What is a container image ?&lt;/h2>&lt;p>A Linux container image is not any magic. Its just a plain rootfs tarball. As long as you have a rootfs tarball you could spawn a container.&lt;/p>&lt;p>Please refer to this previous article on &lt;a href="http://35.238.255.76/geekzone/building-linux-rootfs-from-scratch/">how to build a rootfs tarball&lt;/a>. Alternatively, you could download the rootfs tarball from my git repo. You could also potentially use &amp;ldquo;debootstap&amp;rdquo; to install a debian rootfs into a local directory. It doesn&amp;rsquo;t matter which method you choose to get a rootfs locally.&lt;/p>&lt;p>Once you have the rootfs tarball, you could untar it into a local directory as shown below.&lt;/p>&lt;pre>&lt;code>$ mktemp d/tmp/tmp.3nfwC5jpyB$ mkdir /tmp/tmp.3nfwC5jpyB/rootfs$ sudo tar -xvf rootfs.tar -C /tmp/tmp.3nfwC5jpyB/rootfs/ # this command needs root permission&lt;/code>&lt;/pre>&lt;h2 id="chroot">Chroot:&lt;/h2>&lt;p>The word &amp;ldquo;chroot&amp;rdquo; is a short form for &amp;ldquo;change the root filesystem&amp;rdquo; of the process. Chroot command is a wrapper built on top of the chroot() system call. By executing the command &amp;ldquo;chroot&amp;rdquo; we could change the rootfs of the current shell to start using the newly created rootfs tree as its root filesystem. In other words, the chroot command changes the rootfs of the shell. This command may require root privilege to execute.&lt;/p>&lt;pre>&lt;code>$ cd /tmp/ tmp.3nfwC5jpyB$ chroot rootfs/ /bin/shchroot: cannot change root directory to rootfs/: Operation not permitted$ sudo chroot rootfs/ /bin/sh/ #/ # pwd/&lt;/code>&lt;/pre>&lt;p>Using the chroot command and the rootfs tarball we have created a new filesystem. From now on, any command that you execute in the shell will be looked at in the new rootfs tree&amp;rsquo;s bin or sbin folders and not from the parent rootfs tree&amp;rsquo;s bin or sbin.&lt;/p>&lt;pre>&lt;code>/ # which ls/bin/ls&lt;/code>&lt;/pre>&lt;p>This basically says the &amp;ldquo;ls&amp;rdquo; command was picked up from /tmp/tmp.3nfwC5jpyB/rootfs/bin and not from the parent rootfs /bin&lt;/p>&lt;pre>&lt;code>/# lsbin lib media proc sbin usrdev lib64 mnt root sys varetc linuxrc opt run tmp&lt;/code>&lt;/pre>&lt;p>Now let&amp;rsquo;s display the networking stuff from the shell using the &amp;ldquo;ifconfig&amp;rdquo; command.&lt;/p>&lt;pre>&lt;code>/ # ifconfigifconfig: /proc/net/dev: No such file or directoryeth0 Link encap:Ethernet HWaddr 00:0C:29:E7:B1:87inet addr:10.1.1.1 Bcast:10.1.1.255 Mask:255.255.255.0UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1lo Link encap:Local Loopbackinet addr:127.0.0.1 Mask:255.0.0.0UP LOOPBACK RUNNING MTU:65536 Metric:1&lt;/code>&lt;/pre>&lt;p>Why is the shell displaying the networking information from the parent networking namespace?&lt;/p>&lt;p>This is because the shell is still sharing the parent networking namespace. The chroot command just creates a new root filesystem. It doesnt change anything else for the shell, including the process namespace or the networking namespace.  Well discuss below in this article on how to create a new process namespace and a new networking namespace.&lt;/p>&lt;p>Now lets exit from this new root filesystem and go back to the parent root filesystem.&lt;/p>&lt;pre>&lt;code>/ # exit$$ pwd/tmp/tmp.3nfwC5jpyB$ ls /bin dev initrd.img lost+found opt run sys varboot etc lib media proc sbin tmp vmlinuzcdrom home lib64 mnt root srv usr$$ ifconfigeth0 Link encap:Ethernet HWaddr 00:0c:29:e7:b1:87inet addr:10.1.1.1 Bcast:10.1.1.255 Mask:255.255.255.0inet6 addr: fd00::1:1/64 Scope:LinkUP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1RX packets:19698 errors:0 dropped:0 overruns:0 frame:0TX packets:5089 errors:0 dropped:0 overruns:0 carrier:0collisions:0 txqueuelen:1000RX bytes:18775511 (18.7 MB) TX bytes:379475 (379.4 KB)lo Link encap:Local Loopbackinet addr:127.0.0.1 Mask:255.0.0.0inet6 addr: ::1/128 Scope:HostUP LOOPBACK RUNNING MTU:65536 Metric:1RX packets:2252 errors:0 dropped:0 overruns:0 frame:0TX packets:2252 errors:0 dropped:0 overruns:0 carrier:0collisions:0 txqueuelen:0RX bytes:433390 (433.3 KB) TX bytes:433390 (433.3 KB)&lt;/code>&lt;/pre>&lt;h2 id="heading">&lt;/h2>&lt;h2 id="creating-process-namespace">Creating Process Namespace:&lt;/h2>&lt;p>Containers are isolated groups of processes. For process isolation to work, we need to create a separate process namespace for each of the containers.&lt;/p>&lt;p>Let&amp;rsquo;s get our hands dirty further. From the parent rootfs, create a new process and fetch its PID.&lt;/p>&lt;pre>&lt;code>$ top &amp;amp;[2] 7358$ ps | grep top7358 pts/15 00:00:00 top&lt;/code>&lt;/pre>&lt;p>Lets chroot once again into the new rootfs.&lt;/p>&lt;pre>&lt;code>$ sudo chroot rootfs/ /bin/sh&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s check if the top process is visible in the chrooted shell context. We need to mount the proc filesystem to /proc directory for ps command to work.&lt;/p>&lt;pre>&lt;code>/ # mount -t proc proc /proc  / # ps | grep top7358 1000 top&lt;/code>&lt;/pre>&lt;p>Yes the process is visible in the chrooted shell context also. This is because the shell is still sharing the process namespace of the parent. To create a new process namespace for the shell, we need to use the command &lt;strong>unshare&lt;/strong>.&lt;/p>&lt;p>Unshare command is a wrapper around the &lt;strong>unshare()&lt;/strong> system call.&lt;/p>&lt;p>&lt;strong>[Note]{style=&amp;ldquo;color: #0000ff;&amp;quot;}&lt;/strong>: &lt;strong>You need to have util-linux package version 2.23 or greater installed on your system for the unshares PID namespace feature to work&lt;/strong>.&lt;/p>&lt;pre>&lt;code>$ unshare fp /bin/bash&lt;/code>&lt;/pre>&lt;p>In the above command-f option specifies unshare to fork a new process to run /bin/bash. And the -p option specifies unshare to create a new PID (process) namespace and a new process filesystem (&lt;strong>procfs&lt;/strong>).&lt;/p>&lt;p>Now if you execute the ps command, as shown below, it will still display the processes running in the parent PID namespace. This is because the /proc mount point from where the ps utility reads was mounted with the parent proc filesystem.&lt;/p>&lt;pre>&lt;code>$ ps -elF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD4 S 0 1 0 0 80 0 - 48361 ep_pol ? 00:02:45 systemd1 S 0 2 0 0 80 0 - 0 kthrea ? 00:00:00 kthreadd1 S 0 3 2 0 80 0 - 0 smpboo ? 00:00:48 ksoftirqd/01 S 0 4 2 0 80 0 - 0 worker ? 00:00:19 kworker/0:01 S 0 5 2 0 60 -20 - 0 worker ? 00:00:00 kworker/0:0H1 S 0 8 2 0 -40 - - 0 smpboo ? 00:00:01 migration/0&lt;/code>&lt;/pre>&lt;p>Now lets manually mount the forked process procfs to /proc directory using the mount command.&lt;/p>&lt;pre>&lt;code>$ mount t proc proc /proc$ ps -elF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD4 S 0 1 0 0 80 0 - 29176 wait pts/7 00:00:00 bash0 R 0 35 1 0 80 0 - 37226 - pts/7 00:00:00 ps&lt;/code>&lt;/pre>&lt;p>If you notice the above output carefully, youll see that the bash thinks its PID 1. Every newly created process namespace get its own numbering scheme, separate from the parent PID namespace. This is main advantage of process or PID namespace, especially when moving containers from one host to another without having to worry about conflicting with the PID associated with existing processes in the new host.&lt;/p>&lt;p>The unshare command has an option to automatically mount the newly created procfs to the /proc directory using the mount-proc option.&lt;/p>&lt;pre>&lt;code>$ ushare fp mount-proc /bin/bash$ ps -elF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD4 S 0 1 0 1 80 0 - 29185 wait pts/7 00:00:00 bash0 R 0 26 1 0 80 0 - 37226 - pts/7 00:00:00 ps$&lt;/code>&lt;/pre>&lt;p>More interestingly, you could request unshare command to run any command after creating a new PID namespace, including chroot command. So lets see an example combining the two.&lt;/p>&lt;pre>&lt;code>$ sudo unshare -fp --mount-proc=$PWD/rootfs/proc/ chroot rootfs /bin/bashbash-4.3#bash-4.3# psPID USER VSZ STAT COMMAND1 root 13724 S /bin/bash2 root 12000 R psbash-4.3# pwd/bash-4.3#&lt;/code>&lt;/pre>&lt;p>Lets exit from the new rootfs and go back to the parent rootfs.&lt;/p>&lt;pre>&lt;code>bash-4.3# exitexit$ ps -elError, do this: mount -t proc proc /proc$&lt;/code>&lt;/pre>&lt;p>There was an error because the procfs mounted in /proc was that of the forked process. We need to remount the parent process procfs at /proc.&lt;/p>&lt;pre>&lt;code>$ mount -t proc proc /proc$ ps -elF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD4 S 0 1 0 0 80 0 - 48361 ep_pol ? 00:02:45 systemd1 S 0 2 0 0 80 0 - 0 kthrea ? 00:00:00 kthreadd1 S 0 3 2 0 80 0 - 0 smpboo ? 00:00:48 ksoftirqd/01 S 0 4 2 0 80 0 - 0 worker ? 00:00:19 kworker/0:01 S 0 5 2 0 60 -20 - 0 worker ? 00:00:00 kworker/0:0H1 S 0 8 2 0 -40 - - 0 smpboo ? 00:00:01 migration/0$&lt;/code>&lt;/pre>&lt;h2 id="heading-1">&lt;/h2>&lt;h2 id="creating-network-namespace">Creating Network Namespace:&lt;/h2>&lt;p>Similar to process namespace, you could create a new network namespace using the &amp;ldquo;unshare&amp;rdquo; command with the net option. Here is an example.&lt;/p>&lt;pre>&lt;code># From the default namespace$ ifconfig -sIface MTU RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgeth0 1350 108002738 0 2567 392 85862941 0 0 0 BMRUlo 65536 1406853 0 0 0 1406853 0 0 0 LRU$ unshare --net /bin/bash$ ifconfig # inside the newly created network namespace$ brctl addbr bridge0$ ifconfig bridge0 up$ ifconfigbridge0: flags=4163&amp;lt;UP,BROADCAST,RUNNING,MULTICAST&amp;gt; mtu 1500ether b6:ad:a5:9f:c5:9c txqueuelen 0 (Ethernet)RX packets 0 bytes 0 (0.0 B)RX errors 0 dropped 0 overruns 0 frame 0TX packets 5 bytes 418 (418.0 B)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0$&lt;/code>&lt;/pre>&lt;p>Now lets exit the new network namespace and go back to the parent namespace which is the default namespace.&lt;/p>&lt;pre>&lt;code>$ exitexit$ ifconfig -sIface MTU RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgeth0 1350 108002738 0 2567 392 85862941 0 0 0 BMRUlo 65536 1406853 0 0 0 1406853 0 0 0 LRU$&lt;/code>&lt;/pre>&lt;p>We dont see the bridge interface bridge0 in the default namespace. That explains the network namespace isolation.&lt;/p>&lt;p>An alternate way to create a network namespace is using the ip netns command.&lt;/p>&lt;pre>&lt;code>$ ip netns add newnet$ ip netns listnewnet$&lt;/code>&lt;/pre>&lt;p>Now to enter into the newnet network namespace use the exec option followed by any command to run, as shown below.&lt;/p>&lt;pre>&lt;code>$ ip netns exec newnet bash$ ifconfig$ brctl addbr bridge0$ ifconfig bridge0 up$ ifconfig -sIface MTU RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgbridge0 1500 0 0 0 0 8 0 0 0 BMRU$ brctl showbridge name bridge id STP enabled interfacesbridge0 8000.000000000000 no$ exitexit$ ifconfig -sIface MTU RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgeth0 1350 108002738 0 2567 392 85862941 0 0 0 BMRUlo 65536 1406853 0 0 0 1406853 0 0 0 LRU$&lt;/code>&lt;/pre>&lt;p>For each newly created network namespace, a file with the same name would be generated under the /var/run/netns folder.&lt;/p>&lt;pre>&lt;code>$ ls /var/run/netns/newnet$&lt;/code>&lt;/pre>&lt;h3 id="heading-2">&lt;/h3>&lt;h3 id="entering-namespace-using-setns-and-nsenter">Entering namespace using &lt;em>setns()&lt;/em> and &lt;em>nsenter&lt;/em>:&lt;/h3>&lt;p>If you have a namespace created already but want to let some other process share the same namespace you could use the nsenter command. nsenter command is just a wrapper written on top of the setns() systemcall. nsenter command lets you to access any namespace not just network namespace.&lt;/p>&lt;pre>&lt;code>$ nsenter -hUsage:nsenter [options] [...]Run a program with namespaces of other processes.Options:-t, --target  target process to get namespaces from-m, --mount[=] enter mount namespace-u, --uts[=] enter UTS namespace (hostname etc)-i, --ipc[=] enter System V IPC namespace-n, --net[=] enter network namespace-p, --pid[=] enter pid namespace-U, --user[=] enter user namespace-S, --setuid  set uid in entered namespace-G, --setgid  set gid in entered namespace...$&lt;/code>&lt;/pre>&lt;p>Here is an example C program to make a process to switch from default network namespace to &amp;ldquo;newnet&amp;rdquo; namespace using &amp;ldquo;setns()&amp;rdquo; systemcall.&lt;/p>&lt;pre>&lt;code>...int main(){ /* Spawn a child process */ int id = fork(); if ( id == 0) { /* Child Process still using default netns */ system(&amp;quot;ifconfig&amp;quot;); /* Set the child to run in &amp;quot;newnet&amp;quot; */ int fd = open(&amp;quot;/var/run/netns/newnet&amp;quot;, O_RDONLY); if (fd == -1) { return -1; } if (setns(fd, 0) == -1) { return (-2); } /* child now uses the &amp;quot;newnet&amp;quot; netns */ system(&amp;quot;ifconfig&amp;quot;); ... return 0; } /* parent using the default netns */ system(&amp;quot;ifconfig&amp;quot;); ... return 0;}&lt;/code>&lt;/pre>&lt;p>In the example below, Ill show how to access a network namespace using the nsenter command.&lt;/p>&lt;pre>&lt;code>$ nsenter --net=/var/run/netns/newnet /bin/bash$ ifconfig -sIface MTU RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgbridge0 1500 0 0 0 0 8 0 0 0 BMRU$ brctl showbridge name bridge id STP enabled interfacesbridge0 8000.000000000000 no$ exit$&lt;/code>&lt;/pre>&lt;p>Now, wrapping up the network namespace section, lets combine all the three commands explained above to create a container with its own rootfs, procfs and netns.&lt;/p>&lt;pre>&lt;code>$ pwd/tmp/tmp.3nfwC5jpyB$ lsrootfs$ sudo nsenter --net=/var/run/netns/newnet unshare -fp --mount-proc=rootfs/proc chroot rootfs/ /bin/bashbash-4.3# pwd/bash-4.3# lsbin data etc lib lost+found mnt proc sbin tftpboot usrboot dev home lib64 media opt run sys tmp varbash-4.3# ifconfigbridge0 Link encap:Ethernet HWaddr 7A:E8:FA:08:9F:0AUP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1RX packets:0 errors:0 dropped:0 overruns:0 frame:0TX packets:8 errors:0 dropped:0 overruns:0 carrier:0collisions:0 txqueuelen:0RX bytes:0 (0.0 B) TX bytes:648 (648.0 B)bash-4.3# psPID USER VSZ STAT COMMAND1 root 13724 S /bin/bash4 root 12000 R psbash-4.3# exitexit$ pwd/tmp/tmp.3nfwC5jpyB$ ifconfig -sIface MTU Met RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgeth0 1500 0 294377 0 0 0 146774 0 0 0 BMRUlo 65536 0 4795 0 0 0 4795 0 0 0 LRU$&lt;/code>&lt;/pre>&lt;h2 id="cgroups">cgroups:&lt;/h2>&lt;p>cgroups is a short form for control groups. cgroups provide us the knobs to control and ration the system resources utilized by containers. Without the cgroups support in the Linux Kernel, containers could contend with other processes for system resources such as CPU, memory, network etc., and could potentially starve out other processes or containers running in the same host.&lt;/p>&lt;p>The kernel exposes the cgroups through the &lt;strong>/sys/fs/cgroup&lt;/strong> directory.&lt;/p>&lt;pre>&lt;code>$ sudo su# ls /sys/fs/cgroup/blkio/ cpuacct/ devices/ hugetlb/ net_cls/ perf_event/cpu/ cpuset/ freezer/ memory/ net_prio/ systemd/&lt;/code>&lt;/pre>&lt;p>For this example, lets restrict the CPU resource used by a process. A new cgroup can be easily created by just adding a directory under /sys/fs/cgroup/cpu/. For example, I added demo cpu cgroup under /sys/fs/cgroup/cpu/.&lt;/p>&lt;pre>&lt;code># mkdir /sys/fs/cgroup/cpu/demo&lt;/code>&lt;/pre>&lt;p>Once we create a directory under a cgroup category, the kernel automatically fills the directory with files necessary to manage the cgroup.&lt;/p>&lt;pre>&lt;code># ls l /sys/fs/cgroup/cpu/demo/total 0-rw-r--r-- 1 root root 0 Jun 9 20:58 cgroup.clone_children-rw-r--r-- 1 root root 0 Jun 9 20:58 cgroup.procs-rw-r--r-- 1 root root 0 Jun 9 21:17 cpu.cfs_period_us-rw-r--r-- 1 root root 0 Jun 9 21:18 cpu.cfs_quota_us-rw-r--r-- 1 root root 0 Jun 9 21:09 cpu.shares-r--r--r-- 1 root root 0 Jun 9 20:58 cpu.stat-rw-r--r-- 1 root root 0 Jun 9 20:58 notify_on_release-rw-r--r-- 1 root root 0 Jun 9 20:59 tasks#&lt;/code>&lt;/pre>&lt;p>Each file listed above represents a configurable parameter for the cpu cgroup. You could dump the content of these files to see the default values.&lt;/p>&lt;pre>&lt;code># cat /sys/fs/cgroup/cpu/demo/cpu.shares1024&lt;/code>&lt;/pre>&lt;p>Now lets configure the demo cpu cgroup with cpu.shares set to 25% such that any processes or containers using the cgroup would be restricted to use just 25% of the available 100% cpu cycles. This restriction would come into play only when there is a contention for the cpu. Meaning, if there are no other processes contending for the cpu, then the process assigned to the demo cpu group could potentially use upto 100% of the available cpu.&lt;/p>&lt;pre>&lt;code># echo 256 &amp;gt; /sys/fs/cgroup/cpu/demo/cpu.shares# cat /sys/fs/cgroup/cpu/demo/cpu.shares256&lt;/code>&lt;/pre>&lt;p>To make a process to join the cgroup, just write the PID of the process to &lt;strong>/sys/fs/cgroup/cpu/demo/tasks&lt;/strong> file. In the example below, I added the PID of the current shell to the tasks file.&lt;/p>&lt;pre>&lt;code># echo $$ &amp;gt; /sys/fs/cgroup/cpu/demo/tasks&lt;/code>&lt;/pre>&lt;p>Here is a simple C program with an infinite loop to use 100% cpu.&lt;/p>&lt;pre>&lt;code>// task.c#include &amp;quot;stdio.h&amp;quot;main(){ while(1);}&lt;/code>&lt;/pre>&lt;p>Compile and run the program.&lt;/p>&lt;pre>&lt;code># gcc task.c o task1# cp task1 task2&lt;/code>&lt;/pre>&lt;p>Run task1 in the current shell.&lt;/p>&lt;pre>&lt;code># ./task1&lt;/code>&lt;/pre>&lt;p>Now open another shell window and run the top command to monitor the CPU utilization by task1.&lt;/p>&lt;pre>&lt;code>$ toptop - 22:23:34 up 1 day, 5:50, 5 users, load average: 0.89, 0.52, 0.99Tasks: 235 total, 3 running, 231 sleeping, 1 stopped, 0 zombie%Cpu(s):100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem: 2032384 total, 1716488 used, 315896 free, 109260 buffersKiB Swap: 2094076 total, 5400 used, 2088676 free. 724224 cached MemPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND19420 root 20 0 4196 628 548 R 99.1 0.0 2:00.22 task1228 root 20 0 0 0 0 S 0.3 0.0 0:04.76 jbd2/sda1-81096 jenkins 20 0 1622688 228188 21648 S 0.3 11.2 3:44.05 java1 root 20 0 33900 4344 2712 S 0.0 0.2 0:02.46 init$&lt;/code>&lt;/pre>&lt;p>It shows almost 100% CPU utilization by task1. This is because no other process is contending with task1 for the cpu resource. Now lets open another shell window and run task2 in it.&lt;/p>&lt;pre>&lt;code>$ ./task2&lt;/code>&lt;/pre>&lt;p>Now look at the window where the top command is still running. It would show that as task2 cranks up on the cpu utilization, task1s cpu utilization comes down close to the configured CPU utilization limit.&lt;/p>&lt;pre>&lt;code>$ toptop - 22:26:44 up 1 day, 5:53, 5 users, load average: 1.56, 0.93, 1.06Tasks: 238 total, 3 running, 234 sleeping, 1 stopped, 0 zombie%Cpu(s):100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem: 2032384 total, 1718652 used, 313732 free, 109308 buffersKiB Swap: 2094076 total, 5400 used, 2088676 free. 724232 cached MemPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND19442 root 20 0 4196 636 556 R 79.3 0.0 0:12.64 task219420 root 20 0 4196 628 548 R 19.9 0.0 4:15.70 task13201 www-data 20 0 360480 6176 2628 S 0.7 0.3 0:45.12 apache21 root 20 0 33900 4344 2712 S 0.0 0.2 0:02.46 init2 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kthreadd&lt;/code>&lt;/pre>&lt;p>When you want to delete a cgroup, execute the rmdir command to remove the directory. Before you delete a cgroup make sure all the processes using the cpu have exited.&lt;/p>&lt;pre>&lt;code># exit$ sudo rmdir /sys/fs/cgroup/cpu/demo/&lt;/code>&lt;/pre>&lt;p>Similarly, you could manage the memory cgroup by creating a new folder under &lt;strong>/sys/fs/cgroup/memory&lt;/strong> and configure the appropriate files under it.&lt;/p>&lt;h2 id="conclusion">Conclusion:&lt;/h2>&lt;p>&lt;strong>Containers are no magic&lt;/strong>. It basically leverages some of the Linux Kernel primitives such as namespace, chroot, cgroups etc to create a controlled, isolated, pseudo-virtual environment for a group of processes to run.&lt;/p>&lt;p>Congratulations! Now that you have learnt these container secrets, you have become an expert in container technology!!&lt;/p></description></item><item><title>Linux Containers - Advanced LXC Networking - Part 01</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/advanced-lxc-networking-part-01/</link><pubDate>Sat, 19 Nov 2016 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/advanced-lxc-networking-part-01/</guid><description>&lt;p>In the &lt;a href="https://ganeshtechblog.wordpress.com/2016/11/18/linux-container-lxc-networking-fundamentals/">previous article&lt;/a> I discussed about some basic LXC networking concepts. In this article I&amp;rsquo;ll discuss about the LXC configuration files that play a key role in automated provisioning of the interfaces and bridges.&lt;/p>&lt;h2 id="who-tells-lxc-to-create-lxcbr0-">Who Tells LXC to Create lxcbr0 ?&lt;/h2>&lt;p>The answer is:&lt;em>/var/lib/lxc/test-container/config&lt;/em>. The file has several sections and the networking section at the bottom of the file is what we are interested in.&lt;/p>&lt;pre>&lt;code>$ sudo cat /var/lib/lxc/test-container/config# Template used to create this container: /usr/share/lxc/templates/lxc-ubuntu# Parameters passed to the template:# For additional config options, please look at lxc.container.conf(5)# Common configurationlxc.include = /usr/share/lxc/config/ubuntu.common.conf# Container specific configurationlxc.rootfs = /var/lib/lxc/test-container/rootfslxc.mount = /var/lib/lxc/test-container/fstablxc.utsname = test-containerlxc.arch = amd64# Network configurationlxc.network.type = vethlxc.network.flags = uplxc.network.link = lxcbr0lxc.network.hwaddr = 00:16:3e:b4:86:23&lt;/code>&lt;/pre>&lt;p>This is a container specific configuration file. There is a global or default configuration file at &amp;ldquo;*/etc/lxc/default.conf&amp;rdquo;*used by a container in the absence of a container specific configuration file.&lt;/p>&lt;pre>&lt;code>$ cat /etc/lxc/default.conf lxc.network.type = vethlxc.network.link = lxcbr0lxc.network.flags = uplxc.network.hwaddr = 00:16:3e:xx:xx:xx&lt;/code>&lt;/pre>&lt;p>Who configures the IP address for the &lt;em>lxcbr0&lt;/em> ? Who specifies the DHCP range for the containers connected to &lt;em>lxcbr0&lt;/em> ? For this we need to look at file: &lt;em>/etc/init/lxc-net.conf&lt;/em>&lt;/p>&lt;pre>&lt;code>$ cat /etc/init/lxc-net.confdescription &amp;quot;lxc network&amp;quot;author &amp;quot;Serge Hallyn &amp;lt;serge.hallyn@canonical.com&amp;gt;&amp;quot;start on starting lxcstop on stopped lxcenv USE_LXC_BRIDGE=&amp;quot;true&amp;quot;env LXC_BRIDGE=&amp;quot;lxcbr0&amp;quot;env LXC_ADDR=&amp;quot;10.0.3.1&amp;quot;env LXC_NETMASK=&amp;quot;255.255.255.0&amp;quot;env LXC_NETWORK=&amp;quot;10.0.3.0/24&amp;quot;env LXC_DHCP_RANGE=&amp;quot;10.0.3.2,10.0.3.254&amp;quot;env LXC_DHCP_MAX=&amp;quot;253&amp;quot;env LXC_DHCP_CONFILE=&amp;quot;&amp;quot;env varrun=&amp;quot;/run/lxc&amp;quot;env LXC_DOMAIN=&amp;quot;&amp;quot;......&lt;/code>&lt;/pre>&lt;p>In addition to the &lt;em>lxcbr0&lt;/em> settings, the above file also programs the &lt;em>iptables&lt;/em> with NAT rules(masquerade)such that the container can talk to the outside world or internet.&lt;/p>&lt;pre>&lt;code>$ sudo iptables --list -t natChain PREROUTING (policy ACCEPT)target prot opt source destination Chain INPUT (policy ACCEPT)target prot opt source destination Chain OUTPUT (policy ACCEPT)target prot opt source destination Chain POSTROUTING (policy ACCEPT)target prot opt source destination MASQUERADE all -- 10.0.3.0/24 !10.0.3.0/24 $ &lt;/code>&lt;/pre>&lt;h2 id="interface-configuration-files">Interface Configuration Files:&lt;/h2>&lt;p>The interface specific configurations are stored in &lt;em>/etc/network/interface&lt;/em>. You can edit this file to make changes to the interface configuration. In the example below, &lt;em>eth0&lt;/em>is configured to use DHCP to talk to *lxcbr0,*acting as DHCP server, to get its IP address. However, if you want to assign a static IP address to the &lt;em>test-container&lt;/em>, you may edit the file to do so. Changes to this file are carried across container reboot.&lt;/p>&lt;pre>&lt;code>root@test-container:/# cat /etc/network/interfaces# This file describes the network interfaces available on your system# and how to activate them. For more information, see interfaces(5).# The loopback network interfaceauto loiface lo inet loopbackauto eth0iface eth0 inet dhcp&lt;/code>&lt;/pre>&lt;p>To configure static IP address on &lt;em>eth0&lt;/em> change the &lt;em>eth0&lt;/em> configuration in the file to some like this:&lt;/p>&lt;pre>&lt;code>...auto eth0iface eth0 inet staticaddress 10.0.3.124gateway 10.0.3.1netmask 255.255.255.0...&lt;/code>&lt;/pre></description></item><item><title>Linux Container (LXC) Networking Fundamentals</title><link>https://gvelrajan.github.io/ethernetresearch/geekzone/linux-container-lxc-networking-fundamentals/</link><pubDate>Fri, 18 Nov 2016 00:00:00 +0000</pubDate><guid>https://gvelrajan.github.io/ethernetresearch/geekzone/linux-container-lxc-networking-fundamentals/</guid><description>&lt;p>We saw how to create an LXC container in the &lt;a href="https://ganeshtechblog.wordpress.com/2016/11/05/beginners-guide-to-linux-containers-with-lxc/">previous article&lt;/a>. In this article we&amp;rsquo;ll see how to set up a basic network for LXC and make it communicate to the outside world.&lt;/p>&lt;h2 id="lxcbasic-networking-setup">LXCBasic Networking Setup:&lt;/h2>&lt;p>An LXC Container comes up with default networking interfaces - an ethernet interface(eth0)and a loopback interface(lo). I logged into my &amp;ldquo;&lt;em>test-container&amp;rdquo;&lt;/em> created in the &lt;a href="https://ganeshtechblog.wordpress.com/2016/11/05/beginners-guide-to-linux-containers-with-lxc/">previous article&lt;/a>and listed the interface using the&lt;em>ifconfig&lt;/em> command.&lt;/p>&lt;pre>&lt;code>root@test-container:/# ifconfigeth0 Link encap:Ethernet HWaddr 00:16:3e:b4:86:23 inet addr:10.0.3.124 Bcast:10.0.3.255 Mask:255.255.255.0 inet6 addr: fe80::216:3eff:feb4:8623/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:25 errors:0 dropped:0 overruns:0 frame:0 TX packets:11 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:4801 (4.8 KB) TX bytes:1382 (1.3 KB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)root@test-container:/#&lt;/code>&lt;/pre>&lt;p>The &lt;em>eth0&lt;/em> interface has been assigned an private IP address of 10.0.3.124. This LXC subnet doesn&amp;rsquo;t overlap with the host network subnet address. The &lt;em>ifconfig&lt;/em> command executed on the host machine ( outside the container ) provides the following output.&lt;/p>&lt;pre>&lt;code>$ ifconfigeth0 Link encap:Ethernet HWaddr 00:0c:29:9c:80:88 inet addr:192.168.85.132 Bcast:192.168.85.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:fe9c:8088/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:252 errors:0 dropped:0 overruns:0 frame:0 TX packets:298 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:67077 (67.0 KB) TX bytes:31026 (31.0 KB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:276 errors:0 dropped:0 overruns:0 frame:0 TX packets:276 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:39984 (39.9 KB) TX bytes:39984 (39.9 KB)lxcbr0 Link encap:Ethernet HWaddr fe:d9:bb:5b:a6:81 inet addr:10.0.3.1 Bcast:10.0.3.255 Mask:255.255.255.0 inet6 addr: fe80::744d:3ff:fef2:5c6e/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:147 errors:0 dropped:0 overruns:0 frame:0 TX packets:183 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:12744 (12.7 KB) TX bytes:26509 (26.5 KB)veth3M3GH3 Link encap:Ethernet HWaddr fe:d9:bb:5b:a6:81 inet6 addr: fe80::fcd9:bbff:fe5b:a681/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:48 errors:0 dropped:0 overruns:0 frame:0 TX packets:71 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:4592 (4.5 KB) TX bytes:10258 (10.2 KB)$ &lt;/code>&lt;/pre>&lt;p>In the above output, we see &lt;em>eth0&lt;/em> and &lt;em>lo&lt;/em> as usual. However, the IP address configured on the physical ethernet interface &lt;em>eth0&lt;/em> is 192.168.85.132. Interestingly, the lxcbr0 has been assigned an IP address of 10.0.3.1which falls in the same subnet range as the test-container&amp;rsquo;s subnet 10.0.3.0/24. I&amp;rsquo;ll explain later in this article, why the bridge has been assigned with an IP address.&lt;/p>&lt;p>Whenever LXC is created a default LXC bridge *lxcbr0*is also created. A Linux Bridge is a software equivalent of a physical switch used to interconnect physical host machines. All containers spawned within a host would connect to this bridge by default.&lt;/p>&lt;p>The &lt;em>veth3M3GH3&lt;/em> interface is a veth ( virtual ethernet) type interface that corresponds to the &lt;em>eth0&lt;/em> interface we saw inside the test-container. The&lt;em>veth3M3GH3&lt;/em> is connected automatically to the*lxcbr0*by default when the LXC is spawned.&lt;/p>&lt;pre>&lt;code>$ brctl showbridge name bridge id STP enabled interfaceslxcbr0 8000.fed9bb5ba681 no veth3M3GH3$ &lt;/code>&lt;/pre>&lt;h2 id="how-lxc-container-talks-to-the-outside-world-">How LXC container talks to the outside world ?&lt;/h2>&lt;p>First let&amp;rsquo;s check if the connectivity between the container and the host machine is in place. Try pinging the host machine&amp;rsquo;s &lt;em>eth0&lt;/em> interface IP address from inside the test-container.&lt;/p>&lt;pre>&lt;code>root@test-container:/# ping 192.168.85.132 -c 5PING 192.168.85.132 (192.168.85.132) 56(84) bytes of data.64 bytes from 192.168.85.132: icmp_seq=1 ttl=64 time=0.200 ms64 bytes from 192.168.85.132: icmp_seq=2 ttl=64 time=0.095 ms64 bytes from 192.168.85.132: icmp_seq=3 ttl=64 time=0.106 ms64 bytes from 192.168.85.132: icmp_seq=4 ttl=64 time=0.052 ms64 bytes from 192.168.85.132: icmp_seq=5 ttl=64 time=0.076 ms--- 192.168.85.132 ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 3999msrtt min/avg/max/mdev = 0.052/0.105/0.200/0.052 msroot@test-container:/# &lt;/code>&lt;/pre>&lt;p>Yey!! It works. But how does it work because these are two different subnets? Let&amp;rsquo;s check the routing table of test-container.&lt;/p>&lt;pre>&lt;code>root@test-container:/# ip routedefault via 10.0.3.1 dev eth0 10.0.3.0/24 dev eth0 proto kernel scope link src 10.0.3.124 root@test-container:/# &lt;/code>&lt;/pre>&lt;p>So looks like the &lt;em>lxbr0&lt;/em>&amp;rsquo;s IP address(10.0.3.1) has been set as the default gateway to route any packet whose destination is other than 10.0.3.0/24 subnet. As I described earlier, the &lt;em>eth0&lt;/em> interface inside the container is the visiblein the host network space as interface*veth3M3GH3. *We know*veth3M3GH3*is connected to &lt;em>lxcbr0&lt;/em> by default. This is how, the packets sent via the&lt;em>eth0&lt;/em> interfacein the container reaches the&lt;em>lxcbr0&lt;/em>present outside the container in the host network space.&lt;/p>&lt;p>On a side note, one additional information we could glean from this output is that,the Linux Bridge &lt;em>lxbr0&lt;/em> has been configured to function as a gateway or router in-addition to its bridge functionality.&lt;/p>&lt;p>Similarly, I was able to ping the container&amp;rsquo;s &lt;em>eth0&lt;/em> IP address from the host network space.&lt;/p>&lt;pre>&lt;code>$ ping 10.0.3.124 -c 5PING 10.0.3.124 (10.0.3.124) 56(84) bytes of data.64 bytes from 10.0.3.124: icmp_seq=1 ttl=64 time=0.208 ms64 bytes from 10.0.3.124: icmp_seq=2 ttl=64 time=0.068 ms64 bytes from 10.0.3.124: icmp_seq=3 ttl=64 time=0.062 ms64 bytes from 10.0.3.124: icmp_seq=4 ttl=64 time=0.095 ms64 bytes from 10.0.3.124: icmp_seq=5 ttl=64 time=0.076 ms--- 10.0.3.124 ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 3996msrtt min/avg/max/mdev = 0.062/0.101/0.208/0.055 ms$&lt;/code>&lt;/pre>&lt;p>This is because, the host route table has been set up in such a way that any packet destined to 10.0.3.0/24 network is reachable via &lt;em>lxcbr0&lt;/em>.&lt;/p>&lt;pre>&lt;code>$ ip routedefault via 192.168.85.2 dev eth0 proto static 10.0.3.0/24 dev lxcbr0 proto kernel scope link src 10.0.3.1 192.168.85.0/24 dev eth0 proto kernel scope link src 192.168.85.132 metric 1 $&lt;/code>&lt;/pre>&lt;h2 id="inter-networkingtwo-lxc-containers">Inter-NetworkingTwo LXC Containers:&lt;/h2>&lt;p>I spawned another container named &amp;ldquo;&lt;em>my-container&lt;/em>&amp;rdquo;. It comes up with an IP of 10.0.3.8 on &lt;em>eth0&lt;/em> interface.&lt;/p>&lt;pre>&lt;code>root@my-container:/# ifconfigeth0 Link encap:Ethernet HWaddr 00:16:3e:22:5e:63 inet addr:10.0.3.8 Bcast:10.0.3.255 Mask:255.255.255.0 inet6 addr: fe80::216:3eff:fe22:5e63/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:47 errors:0 dropped:0 overruns:0 frame:0 TX packets:32 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:8155 (8.1 KB) TX bytes:3152 (3.1 KB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)&lt;/code>&lt;/pre>&lt;p>Now check the routing table in &lt;em>my-container&lt;/em>.&lt;/p>&lt;pre>&lt;code>root@my-container:/# ip routedefault via 10.0.3.1 dev eth0 10.0.3.0/24 dev eth0 proto kernel scope link src 10.0.3.8 root@my-container:/# &lt;/code>&lt;/pre>&lt;p>On the host network space check if the equivalent virtual ethernet interface has been created.&lt;/p>&lt;pre>&lt;code>$ ifconfigeth0 Link encap:Ethernet HWaddr 00:0c:29:9c:80:88 inet addr:192.168.85.132 Bcast:192.168.85.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:fe9c:8088/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:353 errors:0 dropped:0 overruns:0 frame:0 TX packets:798 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:81156 (81.1 KB) TX bytes:79957 (79.9 KB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:301 errors:0 dropped:0 overruns:0 frame:0 TX packets:301 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:45156 (45.1 KB) TX bytes:45156 (45.1 KB)lxcbr0 Link encap:Ethernet HWaddr fe:a9:0d:a9:aa:4d inet addr:10.0.3.1 Bcast:10.0.3.255 Mask:255.255.255.0 inet6 addr: fe80::744d:3ff:fef2:5c6e/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:233 errors:0 dropped:0 overruns:0 frame:0 TX packets:266 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:19951 (19.9 KB) TX bytes:36711 (36.7 KB)veth1TV1KE Link encap:Ethernet HWaddr fe:a9:0d:a9:aa:4d inet6 addr: fe80::fca9:dff:fea9:aa4d/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:48 errors:0 dropped:0 overruns:0 frame:0 TX packets:67 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:4592 (4.5 KB) TX bytes:10023 (10.0 KB)veth3M3GH3 Link encap:Ethernet HWaddr fe:d9:bb:5b:a6:81 inet6 addr: fe80::fcd9:bbff:fe5b:a681/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:91 errors:0 dropped:0 overruns:0 frame:0 TX packets:129 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:9221 (9.2 KB) TX bytes:16408 (16.4 KB)$ &lt;/code>&lt;/pre>&lt;p>So eth0 interface inside my-container is seen in the host network space as &lt;em>veth1TV1KE&lt;/em> interface. Check if &lt;em>veth1TV1KE&lt;/em> is connected to the Linux Bridge lxcbr0 by default.&lt;/p>&lt;pre>&lt;code>$ brctl showbridge name bridge id STP enabled interfaceslxcbr0 8000.fea90da9aa4d no veth1TV1KE veth3M3GH3$&lt;/code>&lt;/pre>&lt;p>Kewl! It is indeed connected to &lt;em>lxcbr0&lt;/em>. Let&amp;rsquo;s see if we could ping between the two containers.&lt;/p>&lt;pre>&lt;code>root@my-container:/# ping 10.0.3.124 -c 5PING 10.0.3.124 (10.0.3.124) 56(84) bytes of data.64 bytes from 10.0.3.124: icmp_seq=1 ttl=64 time=0.088 ms64 bytes from 10.0.3.124: icmp_seq=2 ttl=64 time=0.046 ms64 bytes from 10.0.3.124: icmp_seq=3 ttl=64 time=0.063 ms64 bytes from 10.0.3.124: icmp_seq=4 ttl=64 time=0.075 ms64 bytes from 10.0.3.124: icmp_seq=5 ttl=64 time=0.092 ms--- 10.0.3.124 ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 3997msrtt min/avg/max/mdev = 0.046/0.072/0.092/0.020 msroot@my-container:/# &lt;/code>&lt;/pre>&lt;p>Check the ping in the reverse direction.&lt;/p>&lt;pre>&lt;code>root@test-container:/# ping 10.0.3.8 -c 5PING 10.0.3.8 (10.0.3.8) 56(84) bytes of data.64 bytes from 10.0.3.8: icmp_seq=1 ttl=64 time=0.265 ms64 bytes from 10.0.3.8: icmp_seq=2 ttl=64 time=0.076 ms64 bytes from 10.0.3.8: icmp_seq=3 ttl=64 time=0.112 ms64 bytes from 10.0.3.8: icmp_seq=4 ttl=64 time=0.086 ms64 bytes from 10.0.3.8: icmp_seq=5 ttl=64 time=0.107 ms--- 10.0.3.8 ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 3999msrtt min/avg/max/mdev = 0.076/0.129/0.265/0.069 ms&lt;/code>&lt;/pre>&lt;p>That&amp;rsquo;s all folks !&lt;/p></description></item></channel></rss>